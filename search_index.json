[["index.html", "SM-4331 Advanced Statistics About", " SM-4331 Advanced Statistics Dr Haziq Jamil 2021-11-20 About Updated for 2021/22 session. These are the course notes for SM-4331 Advanced Statistics, a fourth-year module taken by students at Universiti Brunei Darussalam (UBD). The course covers the mathematical theory behind statistical inference concepts. "],["contents.html", "Contents", " Contents Welcome to SM-4331! This is a Level 4 Major Option module weighing 4 MCs. Students typically take this module in their fourth year (final or penultimate semester). It is highly recommended for students having a strong interest in statistics and probability, especially students whose final year project involves a statistical component. SM-2205 Intermediate Statistics is a pre-requisite for this module, for which all students should have taken. SM-4331 Advanced Statistics complements SM-4337 Applied Statistics and SM-4339 Survival Analysis very nicely, so it will be beneficial to take these modules together. This class is all about deepening your understanding about statistical inference. There will be an emphasis on the mathematical aspects and theorem proving of important statistical concepts, and less on practical applications (this is left for SM-4337). Thus, it is a class about asking the question ‘why?’, rather than ‘how?’. However, most concepts introduced will be accompanied with R code for students to explore in their own time. By the end of this module, hopefully, students will appreciate and be able to understand why things work the way they do in the statistical world. Past student feedback on this module is that it is on the difficult side. Well, it is advanced statistics, after all. Taking past feedback into account, I have redesigned this module to make it so that students get to follow the content better. For more details, see the Class Philosophy section below. Goals We will first revisit in further detail the fundamental building blocks of mathematical statistics, beginning with set theory, probability theory and probability distributions. We will also learn about convergences of random variables in order to understand several very important results in statistics (e.g. the law of large numbers and the central limit theorem). The syllabus then focusses on the three important statistical inference activities: point estimation, interval estimation, and hypothesis testing. To cap things off, we will tackle linear regression, arguably the most important statistical tool at a practitioner’s disposal, from a mathematical aspect. Incidental learning outcomes Besides the core content of mathematical statistics that we will cover in this module, I really hope my students will be able to realise the following incidental learning outcomes: To be able to present your arguments in a logical and cohesive manner, at the same time gain confidence in public speaking. To train you to read more. I simply cannot emphasise enough how important reading is to your intellectual development. It will undoubtedly also improve your grammar and expand your vocabulary. Need some inspirational quotes? Here are several: A reader lives a thousand lives before he dies. The man who never reads lives only one. Reading is the gateway skill that makes all other learning possible. She read books as one would breathe air, to fill up and live. A book is a device to light the imagination. To connect statistical theory with applied computation via exploration of R code. If you’ve never learnt a programming language before, now is a good time to start. Sometimes the code is given so you can just copy and paste into an R terminal and see what happens for yourself! To be self-independent in your studies. I understand that attending weekly, physical classes gives you a form of structure, makes you comfortable. Having video lectures that you can view in your own time is the opposite of this. Moreover, there are no notes to be had. This is an opportunity to instil self-discipline in yourself if you haven’t had any to begin with, or to strengthen it if you’ve had some! If you stick to the schedule, complete all the tasks, you will do well. "],["module-information.html", "Module information", " Module information Class format Blended learning (mixture of face-to-face and online teaching). As per current university guidelines, I am instructed to conduct at least 40% of the course through online learning. Therefore, the format is as follows: Video lectures. Each topic will be presented via videos released incrementally topic by topic. You may view these at your leisure, but you should aim to complete the viewings before the next set of videos are released. You are encouraged to take notes. There will not be any face-to-face or Zoom lectures. Tutorials. We will have a 2-hour tutorial class (face-to-face) every other week. You will volunteer or be called upon to present the answers to the exercises. Another format we might do is “breakout sessions” where you split into small groups and tutor each other. Breather/Recap sessions. We will dedicate a 2-hour face-to-face class every other week to recap the completed part. During these sessions, you will have the opportunity to clarify any concept or idea that you are still unsure of. No new materials will be taught, it is only for answering questions. You may not ask about the “starred” questions in the tutorials directly. Two weekly sessions are timetabled: Tuesdays 1410-1600 and Wednesdays 1150-1340. Tuesday sessions will be for tutorials and Wednesdays for breather/recap sessions. Note that since we are doing video lectures, we will only see each other every other week. Classes will be in FOS 2.18, unless otherwise told. Assessment Formative assessment 1 \\(\\times\\) mock exam in Week 14 Exercise sheets Summative assessment 60% examination: Closed-book, real-life and invigilated with “typical” worked out solutions type questions. The scheduled date for the exam is Tuesday, 4 May 2021, 2.00-4.00pm at Chancellor Hall (more details closer to the date). Answer 4 out of 5 questions. Calculators are allowed. A minimal formulae sheet will be provided, as well as statistical tables. 20% topical tests: Open-book, multiple choice format, online submission through Canvas. There will be seven tests in total, corresponding to each topic/part of the module. In total, there are 100 equally weighted questions (0.2 points each) so each test will have 10-15 questions depending on the topic. Each test will be available as soon as a new topic is taught, and will be available until the next topic is taught. You may take the test at any time in between. You will have 12 hours to complete the test once you start it. NO RETAKES. 10% tutorials: There are 7 planned tutorial sessions, corresponding to 7 exercise sheets. The majority of the questions are for practice (formative assessment), but there will 1-3 “starred” questions which you will have to hand in for grading. The total marks for all of these star questions will be 100, which then counts for 10% of the overall summative mark. 10% participation: I will give you marks based on your participation during tutorial sessions, breather/recap sessions, as well as online Canvas discussion groups. The marks will reflect your participation level for the entire semester. The rubric is found in the table below. (BONUS) 5% notes: If you take notes for this course, you may submit them to me for grading. I will grade based on aesthetics (tidiness, organisation, readability) as well as content (did you grasp the key concepts? did you do all the little “green checks” from the lectures?). Check the schedule for submission dates. Key data Past class sizes: 2016S2 = 57, 2017S2 = 70, 2018S2 = 18, 2019S2 = 9, 2020S2 = 9 (avg: 32.6) SFE grade average: 3.7 / 5.0 (74.0 %) "],["course-policy.html", "Course policy", " Course policy Here I detail several policies pertaining to my course that I have in place. Communication policy I am usually quick to respond to student e-mails, despite receiving about a gazillion of them each day. Perhaps it is partly due to my dislike of leaving things unattended and obsession to get things moving (this is my problem, not yours). In an effort to preserve my mental health, perhaps have a think first whether that e-mail is necessary. Here are some guidelines. If you are unable to come to tutorials or recap/breather class, and you’re afraid of losing participation marks, then you are responsible for giving me a note in hard copy that documents the reason for the missed class. An e-mail is unnecessary unless the impromptu absence involved missing an exam. If you are emailing for an extension (star questions) or a remake (topical tests), the answer is always “no”. The deadlines are flexible already as it is, so schedule your time accordingly. If you missed a class and wanted to know what was discussed, I sincerely think you’re missing the point. The classes are meant for you to ask the questions and for me to see your level of understanding. By not being there you miss out on both counts. If you have a question about the syllabus, perhaps read this syllabus. If you have a genuine question about the course contents, sure you could e-mail me. But why not post it up as a Canvas discussion and earn points? I do not give out my WhatsApp number to my students. I think there should be clear boundaries between students and instructors, and this, in my opinion crosses that boundary. I have e-mail on my phone (for better or worse) so it really makes no difference if you WhatsApp or if you e-mail. E-mail communication is official, which means that you are accountable for what you say to me, and more importantly for what I say to you. Note that Canvas message is equal to e-mail. Attendance policy Students I understand that sometimes unexpected things come about that delays us or prevents us altogether from class. Really, I do–I have spent 9 years in formal tertiary training learning about random events. However, I also believe that those who want to will make time, and those who don’t want to will make excuses. So unless there is an excusable reason for your absence (or tardiness) then your participation marks will be affected. Teacher Ah, the life of an early-career academic. I often get asked what exactly does a professor do? Well, we split our time between research, teaching and admin work, often not in equal parts unfortunately. What this means for you is that sometimes there are “urgent” non-teaching matters that apparently require my attention, which may happen to clash with our schedule. I don’t expect this to happen very often (in the last 4 semesters I have only had to cancel/reschedule classes due to these reasons: marshall duties during convocation, UBD open day, Wawasan 2035 meeting, research travel). But if they do, I promise that I will firstly do my best to keep our scheduled time together. Quite frankly, I’d rather talk about statistics with you than anything else. Failing that, I will keep you informed well ahead of time. I will reschedule the missed hours at a suitable time for everyone’s benefit. Conduct The medium of instruction is english. This means that I will officially lecture in english, converse in english, and write in english. I expect you to do the same. This is often misconstrued as shying away from our mother tongue (malay), but I assure you my intentions are noble. The majority of students will converse in anything other than proper english outside my class, so for the little time we spend together, at the very least you can get to practice your language skills. I intend to provide a safe and conducive environment for learning for my students, especially in tutorial classes. This means that you will not be ridiculed for making mistakes, or not knowing things, or forgetting things, etc. The purpose is to genuinely gauge your developmental level, and the only way I can do that is if you try. We will also not rudely interject or diminish each other’s ideas, or worse yet, each other’s character. Please be respectful. I expect each one of you to do the tests by yourself. This means no collaborative work, although it’s fine to “Google” stuff if you need to or consult a dictionary for some tricky words. You may think, well it is open-book after all, so why can’t we discuss among ourselve the solutions? Here’s a secret: The tests are not meant to test your knowledge or ability, they are there to force you to refer back to your notes and reading materials :) Together with the note-writing process, these tests will reinforce the mind-map you have created in your brain, allowing you better recall, so you’ll do better in the exam, which you are sitting for by yourself. Trust the process, trust me, and don’t cheat. Learning Management System I’m a believer in LM systems like Canvas. Therefore this is the main avenue for me to distribute learning materials (slides, exercise sheets, solutions, etc.) as well as post announcements. Canvas has an app for students that you may wish to download onto your smartphones (you can get notifications too). Alternatively, keep checking Canvas on your computers on a regular basis. Canvas will also be the avenue for submissions. COVID-19 has made me realise that I much prefer marking on my iPad. If you can upload a PDF copy of your work (I’m referring to the “starred” tutorial questions) that would be really great. Otherwise, you may drop a hard copy of your work into my pigeonhole. "],["resources.html", "Resources", " Resources This course My lecture slides This book Textbooks I estimate that you will probably only absorb 40% of the material through my lectures alone. Please supplement your understanding by reading the texts. These books are written by professors and course instructors who have uncountably more experience than I have, and are more able to explain the statistical concepts much better than I ever could. I used these books myself during my undergraduate years so I trust they will beneficial for you as they were for me! Miscellaneous A Twitter thread on education resources Taking Good Notes Fun with Attendance and Grades (i.e. Students Should Attend Class) Really recommended: All About that Bayes: Probability, Statistics, and the Quest to Quantify Uncertainty. Talk by Dr. Kristin Lennox on YouTube. https://www.qualitydigest.com/inside/standards-column/secret-foundation-statistical-inference-120115.html "],["what-is-statistics.html", "What is statistics?", " What is statistics? Statistics is a scientific subject focussed on collecting and analysing data. Collecting means designing experiments, designing questionnaires, designing sampling schemes, administration of data collection. Analysing means modelling, estimation, testing, forecasting. Statistics is an application-oriented mathematical subject; it is particularly useful or helpful in answering questions such as: Does a certain new drug prolong life for AIDS sufferers? Is global warming really happening? Are O-level and A-level examinations standard declining? Is the house market in Brunei oversaturated? Is the Chinese yuan undervalued? If so, by how much? Questions that can be answered with statistical analysis are wide-ranging, hence making it useful in a variety of fields and specialties, from the hard sciences (chemistry, geology, physics, etc.) to the social sciences (business, economics, psychology, etc.) and beyond1 2. Given today’s data-centric world that we live in, I posit that numerical literacy is now as important as literacy itself! https://www.significancemagazine.com/science/458-does-new-york-city-really-have-as-many-rats-as-people?highlight=WyJuZXciLCInbmV3IiwieW9yayIsIm5ldyB5b3JrIl0=↩︎ https://thoughtcatalog.com/anonymous/2015/04/what-is-the-statistical-chance-of-finding-the-love-of-my-life/↩︎ "],["learning-statistics.html", "Learning statistics", " Learning statistics There are three aspects to learning statistics: Ideas and concepts. Understanding why statistics is needed, and what you are able to do and not do with statistics. Methods. Knowing “how to do” (applied) statistics. Theory. Knowing the “why” of statistics and understanding why things are the way they are. Very mathematics focused. In this course, there is an emphasis on the theory aspect of statistics. It is my hope that you are already familiar with basic statistical concepts (covered in SM-2205 Intermediate Statistics!), and for those of you who will be around next semester, the SM-4337 Applied Statistics module is highly recommended to learn about applying statistics in real-life situations. Of course, for those who have taken SM-4337 will find connections between what we will be discussing in this module and what you have come across there. This course may (at times) feel “mathematical for the sake of mathematics”. In my defence, having a solid foundation in statistical theory will empower you greatly in your data analysis quest. Yes, there are software out there which seem to automagically generate the statistics of interest and even fit statistical models for the user blindly. Two things: There is still the matter of interpretation of these output. Will you be able to explain to your boss/stakeholder/customer/etc. the meaning of the data you helped them analyse? Will you be able to spot any assumptions that are fundamental to the model being true/useful? Remember, gargbage in garbage out. Some words of wisdom: Students who analyze data, or who aspire to develop new methods for analyzing data, should be well grounded in basic probability and mathematical statistics. Using fancy tools like neural nets, boosting, and support vector machines without understanding basic statistics is like doing brain surgery before knowing how to use a band-aid. —Larry Wasserman (in All of Statistics) In the grand scheme of things, mastery of statistical theory is just one part of the equation to be a practical data analyst. Given how popular the term ‘data science’ these days, it is worth noting the figure3 below. Data science is seen as the intersection between Mathematics and Statistics, Computer Science, and Domain Knowledge. Given the importance of the computer science (read: coding and programming skills) as the enabler of data science activities, I try as much as I can to encourage you to these skills up yourself. Wherever appropriate you might see R code embedded within the text. As a side note, R programming is not examinable. https://towardsdatascience.com/introduction-to-statistics-e9d72d818745↩︎ "],["population-sample-and-parametric-models.html", "Population, sample and parametric models", " Population, sample and parametric models To motivate the use of statistics in everyday life, let us consider two practical situations where you might employ statistical methods: BMW M Division has proudly unveiled the successor to their current “king of sedans”, the new BMW M3 Competition (G80), sporting a 503 bhp twin-turbo 3.0 litre inline-six S58 engine with a claimed acceleration rate of 0-100 km/h in 3.9 seconds. The Authority for Info-communications Technology Industry of Brunei Darussalam (AITI) conducted the Household ICT Survey in 2018 and reported that 95% percent of individuals personally use the internet on a daily basis, a slight decrease from 97% in the year 2016. Estimates are accurate within 2% margin of error with 95% confidence. Your immediate thought should be “how can I trust these figures?”. In general, it’s always good to approach life with a healthy dose of skepticism. We certainly don’t want to be duped by people claiming to present a version of the truth, when in reality it is a skewed version of the truth (or worse yet, false). In data we trust! But only if we are mathematically-savvy… Population vs sample In both cases, the conclusion is drawn on a population (i.e. all of the subjects concerned) based on the information from a sample (i.e. a subset of the population). For BMW M Division, it is impossible to measure the entire population (obtain the acceleration rates), constituting all BMW M3 (G80) cars that have been made and are yet to be made in the future. Often this is referred to as an infinite population model. For AITI, while possible, it is (economically) unfeasible to measure the entire population, i.e. to ask everyone in Brunei whether or not they use the internet on a daily basis. It would be very difficult to knock on everybody’s door and obtain responses in a timely manner (what if they’re out of the country? what if they’re sick? what if they just don’t want to respond?). Anyone who has done any form of survey work will understand the intricate problems that might arise. In any case, it is important to make the distinction between population and sample. The population is defined to be the entire set of the objects concerned, and those objects are typically represented by some numbers. We do not know the entire population in practice. A sample is a (randomly selected) subset of a population, and is a set of known data in practice. When people claim to have data (about some phenomenon), they typically imply that they have a sample of the population, and not the population data itself. There are exceptions of course, for instance, a country’s census captures population data every 10 years. Another example is when the population itself is small such that all data can be collected easily. A question that you might be thinking to yourself is the following: Is doing analysis on the sample good enough? Will it reveal the same insights as if we’re analysing the population data? The answer to this is that it depends on how you perform the sample! Things to look out for is definitely bias in data collection methods. Here are some examples: Asking the question “Can you live without the internet?” via an online poll of adults. Think about it. How exactly can you ask people without internet whether they can live without the internet? Clearly, the sample that you collect is biased towards those who are privileged enough to have access. And if you were wondering, yes this really happened. Asking people to volunteer their responses typically lead to bias. It is suggested that those who has something to complain about will voice their opinions more than those who do not. Other errors can crop up during data collection process which may skew the representation of the data (e.g. duplicate data, missing data, substituted data, etc.) It is certainly important that data be collected in a methodological manner, in order for valid inferences to be drawn. Sampling methodology is beyond the scope of this course, however! Parametric models For a given problem, it is quite advantagous to assume that a population obeys some probability distribution law. To be a little bit more concrete, suppose we assign the variable \\(x\\) to be the quantity of interest. Then we assume that the quantity of interest has a distribution function \\(f(x|\\theta)\\) (we will recap the concept of distribution functions in the next chapter!). Furthermore, The form of the distribution i.e. \\(f(\\cdot|\\theta)\\) is known (e.g. normal, Poisson, exponential, etc.). The “specifics” of the distribution is (assumed to be) not known, but potentially knowable if data were available. The unknown characteristics of the distribution are traditionally represented by \\(\\theta\\) (such as the mean, variance, rate, etc.–any quantity that characterises how the distribution behaves). We call \\(\\theta\\) the parameter(s) of the model. Such an assumed distribution is called a parametric model. For the two earlier examples, Let \\(X =\\) acceleration of BMW M3 G80 vehicles. Assume \\(X\\sim\\mathop{\\mathrm{N}}(\\mu,\\sigma^2)\\). Here \\({\\boldsymbol\\theta}= (\\mu,\\sigma^2)^\\top\\), where \\(\\mu\\) is the ‘true’ acceleration rate. In this example, the parameter is 2-dimensional instead of unidimensional. Let \\(\\{0,1\\} \\ni X =\\) someone in Brunei uses the internet daily. Assume \\(X\\sim\\mathop{\\mathrm{Bern}}(p)\\). Here \\(\\theta = p\\), the ‘true’ proportion of daily internet users in Brunei. There is no god-given right for your quantity of interest \\(X\\) to follow a particular distribution! These are simply assumptions, in order to make it easy to model reality. Parametric assumptions may be correct, or they may not. In fact, strictly speaking, all models are wrong, but some are useful (quote attributed to the British statistician George Box). A sample: a set of data or random variables?–A duality A sample of size \\(n\\), \\(\\{X_1,\\dots,X_n\\}\\), is also called a random sample. It consists of \\(n\\) concrete numbers in a practical problem. When I say ‘concrete’ here, it means numbers that you can play around with–you can add them up, subtract them, plot them, take averages, and so on. These would be numbers that you collect into a spreadsheet, say. The more contentious part of the term ‘random sample’ is the word ‘random’ itself. The word ‘random’ encapsulates the possibility of the concrete numbers collected in the samples being different, by virtue of: The sample may be taken by different people or entities. The sample may be obtained at a different time or location. The sample may be measured using different instruments (albeit measuring the same thing). etc. Essentially, different samples may well be different subsets of a population. With this, a sample may also be viewed as \\(n\\) (independent and identically distributed) random variables, simply because their values are conceptually not fixed (at least not until you observe them–but even then it is simply one possible realisation of potentially many others). Now, if a sample is not random then perhaps there would be no need for statistics. But hardly ever would you find this to be the case. Rigorous mathematical methods exist to deal with this randomness, and thus viewing samples as random variables allows us to assess the performance of a statistical method. Variability of estimates Suppose you set out to answer questions 1 and 2 above by collecting some data. This is what you find: BMW M example A sample of \\(n=38\\) gave the sample mean \\[\\bar X_n = \\frac{1}{n}\\sum_{i=1}^n X_i = 3.9\\] In words, the average acceleration (0-100 km h-1) of the sample of cars yielded the value 3.9 seconds. But realise that a different sample may well give a different sample mean. For instance, 29 YouTubers and other social media “influencers” were given access to the new M3 on a race track, and their sample mean yielded \\(\\bar X_n = 3.4\\). What do we make of this discrepancy? Whose figure do we trust? It doesn’t seem so satisfying to know that different samples will give different results. For instance, what do you tell the public or media about the acceleration figure of the new M3s? Evidently relying on a singular measure (in this case the mean) is not enough. Moreoever, it keeps on changing in value! The key is to be able to employ probabilistic statements about our results. For instance, “the acceleration figure is 3.9 plus or minus 0.01 about 95% of the time when we sample” is a much more confident statement to make, rather than providing a figure that keeps on changing. By treating the data \\(X_1,\\dots,X_n\\) as random variables, it is implied that \\(\\bar X_n\\) is also a random variable. Everything that is random should have a distribution. If the distribution of \\(\\bar X_n\\) concentrates closely around the unknown \\(\\mu\\), then it is a good estimator! AITI example For the AITI example, there is that statement ‘…accurate to within 2% margin of error with 95% confidence’. This statement alludes to the variability of the estimate, if another random sample was obtained. The estimate in this case was also the sample mean (proportion of people who use the internet on a daily basis), \\[ \\hat p = \\bar X_n = \\frac{1}{n}\\sum_{i=1}^n X_i. \\] Mathematically, the confidence statement reads \\[ \\Pr(|\\hat p - p| \\leq 0.02 ) = \\Pr\\big(p\\in [\\hat p-0.02, \\hat p + 0.02]\\big) = 0.95 \\] that is, the true value is covered 95% of the time inside an interval of width 0.02 under repeated sampling. This statement is made possible due to the randomness of the estimator \\(\\hat p\\). "],["probability-and-statistics.html", "Probability and statistics", " Probability and statistics Thus far, we have thrown the term ‘inference’ around and maybe we’ve all taken it for granted. Cambridge’s dictionary defines infer as ‘to reach an opinion from available information or facts’. And indeed that is what we are ultimately interested in when we analyse data. Sure, there might be that random element to the data that we have to contend with, but it is about reaching some form of conclusion one way or another using data. In the previous section, we’ve just implicitly described the three main activities concerning statistical inference. Point estimation “What is \\(\\mu\\)?” Hypothesis testing “Is \\(\\mu=3.4\\) and not \\(\\mu=3.9\\)?” Interval estimation “What’s an upper and lower bound estimate for \\(\\mu\\)?” These three activities will be the main focus of this course, and we will formalise the notion of each one in turn. Hopefully you can now appreciate how statistics is an inherently applied subject, making use of mathematics (probability in particular) to answer problems across a variety of fields. What is the difference between probability and statistics? The following figure might be helpful. Probability is a highly mathematical subject (although maybe the name doesn’t seem to suggest it, it really has its origin in abstract measure theory). In probability, we ask questions like What is \\(\\mathop{\\mathrm{E}}(X)\\)? (expectations) What is \\(\\Pr(X &gt; a)\\)? (probability calculations) for some given value of \\(\\theta\\) in an assumed family of parametric distributions. Whereas in statistics, we are more interested in questions like What is \\(\\theta\\)? Is \\(\\theta\\) larger than \\(\\theta_0\\)? How confident am I that \\(\\theta \\in (\\theta_l,\\theta_u)\\)? given some observed data. I like to think of the two as reverse processes. Statistics definitely needs probability for sure, so we will have to master probability theory before doing any statistical inference. On to the next chapter! "],["probability-theory-primer.html", "Chapter 1 Probability theory primer", " Chapter 1 Probability theory primer As I was looking for a rationale as to why we begin statistical inference with learning about probability theory, I notice that C&amp;B couldn’t have put it better: The subject of probability theory is the foundation upon which all statistics is built, providing a means for modeling populations, experiments, or almost anouthing else that could be considered a random phenomenon. Through these models, statisticians are able to draw inferences about populations, inferences based on examination of only a part of the whole. — George Casella &amp; Roger L. Berger in Statistical Inference By right, probability theory and the mathematics of random events deserves one dedicate module of its own. For our purposes, it suffices to ‘skim through the surface’ as it were, and cover the basic and necessary ideas to move forward with statistical inference. Learning objectives By the end of this chapter, you will be able to: Compute probabilities of events by simple counting and application of various known probability results Understand the notion of conditional and independent events, leading up to the application of Bayes’ Theorem Formalise mathematically the notion of random variables and make calculations using its distribution function Compute expectations (and variances) including via the method of moment generating functions Readings Casella and Berger (2002) All of Chapter 1 (skip sections 1.2.3 and 1.2.4). Chapter 2, section 2.2 and 2.3 only. Chapter 4, section 4.1, 4.2 and 4.5 only. Wasserman (2004) All of Chapter 1. Chapter 2, sections 2.1–2.2, 2.5–2.8. Chapter 3, sections 3.1–3.5. Topics not covered: Counting and enumerating outcomes, moment generating functions (to be covered in the next topic), transformations of r.v., multivariate distributions (bivariate only). YouTube video: The medical test paradox YouTube video: Bayes theorem "],["elementary-set-theory.html", "1.1 Elementary set theory", " 1.1 Elementary set theory A discussion of probability theory is almost always begun by talking about the concept of ‘sets’. As the term implies, sets are a collection of things. You’re sure to come across sets before, such as the set of family members in your household, or the set of all natural numbers, or something even more funky like the set of all sets (the universal set4). When statisticians talk about sets it is usually in the context of conducting an “experiment”5. The important bits are: The sample space \\(\\Omega\\) is the set of possible outcomes of an experiment. Elements \\(\\omega \\in \\Omega\\) are called sample outcomes or realisations. Subsets of \\(E \\subseteq \\Omega\\) are called events. So whatever the context of the random process might be, we should be comfortable identifying what the sample space is, what its elements are, and what possible events might occur. Example 1.1 In tossing a two-sided coin \\(n \\geq 2\\) times, let \\(H\\) denote ‘heads’, while \\(T\\) denote ‘tails’. Let \\({\\boldsymbol\\omega}= (\\omega_1,\\dots,\\omega_n)\\) be the results of these coin tosses. Then the sample space is \\[ \\Omega = \\Big\\{ {\\boldsymbol\\omega}\\, \\big|\\, \\omega_i \\in \\{H,T\\} \\Big\\}. \\] Let \\(E\\) be the event that the first head appears on the second toss. Then this event can be mathematically described as \\[ E = \\Big\\{ {\\boldsymbol\\omega}\\, \\big|\\, \\omega_1 = T, \\omega_2 = H, \\omega_i \\in \\{H,T\\} \\text{ for } i &gt; 2 \\Big\\}. \\] 1.1.1 Set operations There are several things that we can do to sets much like we can do to numbers in arithmetic. Here is an abridged version of set operations. The complement of an event \\(A\\), written \\(A^c\\), is the set of all elements that are not in \\(A\\): \\(A^c = \\{\\omega | \\omega\\not\\in A \\}\\). The complement of \\(\\Omega\\) is the empty set \\(\\emptyset = \\{\\}\\). The union of events \\(A\\) and \\(B\\) (thought of as “A or B or both”) is defined \\[ A \\cup B = \\{\\omega \\in \\Omega \\,|\\, \\omega \\in A \\text{ or } \\omega \\in B \\}. \\] The intersection of events \\(A\\) and \\(B\\) (thought of as “A and B”) is defined \\[ A \\cap B = \\{\\omega \\in \\Omega \\,|\\, \\omega \\in A \\text{ and } \\omega \\in B \\}. \\] Unions and intersections on sets are commutative, associative, and distributive6. Commutativity: \\(A \\cup B = B \\cup A\\) and \\(A \\cap B = B \\cap A\\). Associativity: \\(A \\cup (B \\cup C) = (A \\cup B) \\cup C\\) and \\(A \\cap (B \\cap C) = (A \\cap B) \\cap C\\). Disributive laws: \\(A \\cap (B \\cup C) = (A \\cap B) \\cup (A \\cap C)\\) and \\(A \\cup (B \\cap C) = (A \\cup B) \\cap (A \\cup C)\\). DeMorgan’s Laws: \\((A \\cup B)^c = A^c \\cap B^c\\) and \\((A \\cap B)^c = A^c \\cup B^c\\) It’s pretty straightforward to prove the commutative, associative, distributive and DeMorgan’s properties using the rules that precede them in the list. You may like to try this out yourself. Or you may have come across a kind of “sketch proof” involving Venn diagrams. The operations of unions and intersections can be extended to infinite collections of sets as well. If \\(A_1,A_2,A_3,\\dots\\) is collection of sets, all defined on a sample space \\(\\Omega\\), then \\[\\begin{gather*} \\bigcup_{i=1}^\\infty A_i = \\{x \\in \\Omega \\,|\\, x\\in A_i \\text{ for some } i \\}, \\\\ \\bigcap_{i=1}^\\infty A_i = \\{x \\in \\Omega \\,|\\, x\\in A_i \\text{ for all } i \\}. \\end{gather*}\\] Example 1.2 Let \\(\\Omega=(0,1]\\) and define \\(A_i=[1/i, 1]\\). Then, \\[\\begin{align*} \\bigcup_{i=1}^\\infty A_i &amp;= \\{1\\} \\cup [1/2, 1] \\cup [1/3, 1] \\cup \\cdots = (0,1], \\\\ \\bigcap_{i=1}^\\infty A_i &amp;= \\{1\\} \\cap [1/2, 1] \\cap [1/3, 1] \\cap \\cdots = \\{1\\}. \\end{align*}\\] 1.1.2 Partitions We say that two events \\(A\\) and \\(B\\) are disjoint or mutually exclusive if \\(A \\cap B = \\{ \\}\\). Disjoint sets have no points in common. Suppose that \\(A_1,A_2,\\dots\\) are events defined on \\(\\Omega\\) such that they are (pairwise) disjoint, i.e.  \\[ A_i \\cap A_j = \\{ \\}, \\text{ for } i \\neq j. \\] Then the collection \\(A_1,A_2,\\dots\\) forms a partition of \\(\\Omega\\). Partitions divide the sample space into non-overlapping pieces. Example 1.3 A deck of playing cards has four suits: \\(\\clubsuit, \\diamondsuit, \\spadesuit, \\heartsuit\\). Let \\(A=\\{\\clubsuit,\\diamondsuit\\}\\) and \\(B=\\{\\spadesuit, \\heartsuit\\}\\). Then \\(A\\) and \\(B\\) form a partition of the sample space. Example 1.4 The set \\(\\{\\mathbb{R}_+, \\mathbb{R}_{-}, \\{0\\}\\}\\) (ie the set of positive reals, negative reals, and zero respectively) is a partition of the real numbers \\(\\mathbb{R}\\) since \\(\\mathbb{R}_{+} \\cup \\mathbb{R}_{-} \\cup \\{0\\}=\\mathbb{R}\\); \\(\\mathbb{R}_{+} \\cap \\mathbb{R}_{-} = \\mathbb{R}_{+} \\cap \\{0\\} = \\mathbb{R}_{-} \\cap \\{0\\} = \\{ \\}\\); and \\(\\{\\mathbb{R}_{+}\\), \\(\\mathbb{R}_{-}\\), and \\(\\{0\\}\\}\\) are all non empty. Note that \\(\\{0\\}\\) is not an empty set. It contains exactly one element, the number zero. Which, as it turns out, is a paradox: https://en.wikipedia.org/wiki/Russell%27s_paradox↩︎ Although, we don’t really mean it like how scientists mean experiments to be (we’re not fiddling around with buttons or chemicals). I suppose it’s more about the process of the (random) data generating mechanism itself.↩︎ See C&amp;B Thm 1.14↩︎ "],["axiomatic-probability.html", "1.2 Axiomatic probability", " 1.2 Axiomatic probability In principle, we can understand and easily grasp the notion of probability as the “frequency of an event occurring”. But how do we operationalise this concept? That is, by what rules and mechanisms are we allowed to assign probabilities to events? If we can overcome this task and are able to assign probabilities to (random) events in an experiment, then we can start to analyse them statistically! 1.2.1 Probability as a measure Let us take a measure-theoretic approach to defining probabilities. We will dive straight into the rigors of definitions before providing a somewhat apologetic rationale as to why such mathematical difficulties are required for probability theory. As the name implies, measure theory is the theory about how we measure things (duh!). Measure itself is a fundamental concept in mathematics, and it would be useful to come up with a mathematical framework for how we deal with everyday concepts like length, mass, area, volume, and so on. Importantly, such a framework allow us to reliably measure in even higher dimensions or onto more abstract constructs not yet imaginable. Intuitively, a measure is simply a function whose input is the thing we want to measure (let’s call it a set), and whose output is a non-negative number. Don’t worry, a formal definition will follow, but for now, call this function \\(\\mu\\). It would be fair to expect a measure \\(\\mu\\) to satisfy \\(A \\subseteq B \\Rightarrow \\mu(A) \\leq \\mu(B)\\) \\(A \\subseteq B \\Rightarrow \\mu(B-A)= \\mu(B) - \\mu(A)\\) If \\(\\{A_1,A_2,\\dots\\}\\) are mutually exclusive sets (disjoint), then \\(\\mu\\left(\\cup_{i=1}^\\infty A_i \\right) = \\sum_{i=1}^\\infty \\mu(A_i)\\) The first property simply says that if \\(A\\) is a subset of \\(B\\), then the measure of \\(A\\) is at most the measure of \\(B\\). The second property follows this up by saying that the measure of the set \\(B-A\\), that is, the set that is obtained by starting with \\(B\\) and taking away the parts that is contained in \\(A\\), then the measure of this created set is the difference between the measures of \\(B\\) and \\(A\\). Finally, the third property, also known as countable additivity, simply states that the measure of the whole is equal to the sum of the parts. It turns out that the first and second properties follow from the third (and the fact that a measure cannot be negative)–see Definition 1.2. So we have this intuition about what the measure should be, but what about the stuff we want to measure? For our purposes, we are interested in measuring subsets of \\(\\Omega\\). We ask, are we able to measure all possible subsets of \\(\\Omega\\)? At a glance, perhaps if \\(\\Omega\\) is countable (e.g. \\(\\Omega=\\{1,2,3\\}\\)), it is easy to describe the subsets of \\(\\Omega\\) through the power set7 \\({\\mathcal P}(\\Omega)\\), which is the set of all possible subsets of \\(\\Omega\\), but what about when \\(\\Omega\\) is uncountable (e.g. an interval \\(\\Omega=[0,1]\\in\\mathbb{R}\\)). Given a sample space \\(\\Omega\\), we need to define the largest possible collection of subsets of \\(\\Omega\\) that can be observed and on which we can assign valid measure. Definition 1.1 (\\(\\sigma\\)-algebra) A collection \\({\\mathcal F}\\) of subsets of a set \\(\\Omega\\) is called a \\(\\sigma\\)-algebra if it satisfies the following conditions: If \\(A \\in {\\mathcal F}\\), then \\(A^c \\in cF\\) [closed under complementation]. If \\(A_1,A_2,\\cdots \\in {\\mathcal F}\\), then \\(\\cup_{i=1}^\\infty A_i \\in {\\mathcal F}\\) [closed under countable unions]. \\(\\{\\} \\in {\\mathcal F}\\) [contains the empty set]. As a remark, condition iii. can be replaced with \\(\\Omega\\in{\\mathcal F}\\) by virtue of condition i.. The \\(\\sigma\\)-algebra is a collection of events or subsets of the sample space \\(\\Omega\\), including \\(\\Omega\\) itself and the empty set \\(\\{\\}\\), which is closed under countable applications of set operations. This is because DeMorgan’s Law allows us to write the countable union property in iii. also as countable intersections: If \\(A_1,A_2,\\cdots \\in {\\mathcal F}\\), then by i. \\(A_1^c,A_2^c,\\cdots \\in {\\mathcal F}\\), and hence \\(\\cup_{i=1}^\\infty A_i\\in{\\mathcal F}\\) and also its complement. By DeMorgan’s Law, \\[ \\left( \\cup_{i=1}^\\infty A_i^c \\right)^c = \\cap_{i=1}^\\infty A_i. \\] Sets contained in \\({\\mathcal F}\\) are called measurable sets. The \\(\\sigma\\)-algebra is an important condition for measure to not breakdown, because it helps draw a line as to which subsets of the sample space is measurable, and which is not. Out of interest, condition iii. in Definition 1.1 is the condition that makes \\({\\mathcal F}\\) a \\(\\sigma\\)-algebra (the \\(\\sigma\\) stands for countable sum). Without this condition, one ends up with just an algebra of sets, one that is most likely too small, failing to contain sets that we would like assign a measure. Let’s take a look at some examples of \\(\\sigma\\)-algebras. Example 1.5 The trivial \\(\\sigma\\)-algebra: \\[\\big\\{ \\{\\}, \\Omega \\big\\}.\\] This corresponds the case of no information. The power set of the sample space \\(\\Omega\\): \\[\\big\\{ A | A \\subseteq \\Omega \\big\\}.\\] This corresponds the case of full information. The collection \\(\\big\\{ \\{\\}, A, A^c, \\Omega \\big\\}\\) is a \\(\\sigma\\)-algebra, for any \\(A\\subseteq \\Omega\\). Let \\(\\Omega = \\{a,b,c,d\\}\\). A possible8 \\(\\sigma\\)-algebra is \\[\\big\\{\\{\\}, \\{a,b,c,d\\}, \\{a,b\\}, \\{c,d\\} \\big\\}.\\] Define \\(B(s)\\) to be a square of side length \\(s\\). Let \\(\\Omega\\) be the collection of points in \\((0,1)\\times(0,1)\\subset \\mathbb{R}^2\\) contained within the a unit square \\(B(1)\\). Then \\[{\\mathcal F}=\\{ \\text{Collection of points contained in the square } B(s) \\text{ with } s \\in (0,1) \\}.\\] It should be clear there are uncountably many such squares that can be fit within the unit square. Just as a remark, most introduction to probability measure will deal with finite or countable sets when introducing \\(\\sigma\\)-algebras, giving readers an impression that it’s only possible to define \\(\\sigma\\)-algebras on such sets. The fifth example above gives an example of a \\(\\sigma\\)-algebra which is uncountable. The twin \\((\\Omega,{\\mathcal F})\\) is called a measurable space. This sort of defines the “parts” of our problem which are measurable, as per Definition 1.1. What’s missing is a measure, i.e. the thing that actually tells us ‘how long a piece of string is’, so to speak9. We now define a measure as follows. Definition 1.2 (Measure) A measure \\(\\mu\\) is a non-negative real valued function defined on a \\(\\sigma\\)-algebra, i.e. \\(\\mu:{\\mathcal F}\\to\\mathbb{R}_{\\geq 0}\\), where \\(\\mathbb{R}_{\\geq 0}\\) are the non-negative real numbers and \\({\\mathcal F}\\) a \\(\\sigma\\)-algebra of subsets of \\(\\Omega\\). The measure \\(\\mu\\) satisfies the following properties: \\(\\mu(\\{\\})=0\\). \\(\\mu\\) is countably additive, i.e. if \\(A_1,A_2,\\dots\\) are disjoint events, then \\[\\mu\\left( \\cup_{i=1}^\\infty A_i \\right) = \\sum_{i=1}^\\infty \\mu(A_i).\\] If, in addition the measure of the entire sample space is normalised (i.e. \\(\\mu(\\Omega)=1\\)), then \\(\\mu\\) is called a probability measure. We will see this in the next section. The triplet \\((\\Omega,{\\mathcal F},\\mu)\\) is called a measure space (note that without the measure it is called a measurable space). This space simply tells us the parts needed for well-defined measure to take place on the subsets of \\(\\Omega\\). Example 1.6 The counting measure. Let \\(\\Omega\\) be a countable set [You may be creative as you like here to make this less abstract, e.g. the books on your shelf or the members of your family, although the set need not be finite]. Let \\({\\mathcal F}={\\mathcal P}(\\Omega)\\) be the power set of \\(\\Omega\\). For all sets \\(A\\in{\\mathcal A}\\), define \\[\\mu(A) = \\begin{cases} |A| &amp; A \\text{ has finitely many elements}\\\\ \\infty &amp;\\text{otherwise} \\end{cases}\\] where the operator \\(|\\cdot|\\) represents the cardinality of the set, i.e. the number of elements it contains (its size). The Lebesgue measure in one dimension. Let \\(\\Omega=\\mathbb{R}\\), and define \\({\\mathcal F}\\) to contain all sets of the form [a,b], i.e. closed intervals, (a,b), i.e. open intervals, (a,b], i.e. open-closed intervals; and [a,b), i.e. closed-open intervals. for all real numbers \\(a\\) and \\(b\\). We can deduce that the \\(\\sigma\\)-algebra \\({\\mathcal F}\\) contains all possible “nice” intervals of the real line, including unbounded intervals and even singletons, which means any continuous partition of the real line can be measured (including a point, which should have measure zero). To see this, using the properties of \\(\\sigma\\)-algebras, unbounded intervals are in \\({\\mathcal F}\\), since, for instance \\[(x,+\\infty)=\\cup_{i=1}^\\infty(x,x+i).\\] singletons are in \\({\\mathcal F}\\), since \\[\\{x\\}=\\cap_{i=1}^\\infty (x-1/i,x+1/i).\\] This set \\({\\mathcal F}\\) has a special name, called the Borel \\(\\sigma\\)-algebra. All that’s left is to define the measure. The Lebesgue measure \\(\\mu\\) assigns the usual concept of length to any continuous interval on \\(\\mathbb{R}\\) (to be precise, the Borel \\(\\sigma\\)-algebra on \\(\\mathbb{R}\\)): \\[\\mu\\left(A\\right)=b-a\\] where \\(A\\) is any interval of \\(\\mathbb{R}\\) of the above forms (closed, open, open-closed, closed-open). This measure works even for singleton sets or unbounded intervals. 1.2.2 Axioms of probability \\(\\mathbb{P}:{\\mathcal B}\\to[0,1]\\) is a probability measure on \\((\\Omega, {\\mathcal B})\\) if it satisfies the the following three axioms: Axiom 1: \\(\\mathbb{P}(A) \\geq 0, \\forall A \\in \\Omega\\). Axiom 2: \\(\\mathbb{P}(\\Omega) = 1\\). Axiom 3: For pairwise disjoint events \\(A_1,A_2,\\dots\\), \\[ \\mathbb{P}\\bigg( \\bigcup_{i=1}^\\infty A_i \\bigg) = \\sum_{i=1}^\\infty A_i. \\] Remark. There are two main interpretation of probabilities. The frequentist interpretation is that if we flip the coin many times, then the proportion of heads that is observed will be 50% in the long run. The subjectivist interpretation is that the probability measures an observer’s strength of belief that the event is true. In either interpretation, the three axioms must be satisfied. 1.2.3 Derived probability results Let \\(A\\) and \\(B\\) be measurable events from the sample space \\(\\Omega\\). The following results can be derived using only the three axioms: \\(\\mathbb{P}(\\{ \\}) = 0\\) \\(0\\leq\\mathbb{P}(A)\\leq 1\\) \\(\\mathbb{P}(A^c)=1-\\mathbb{P}(A)\\) \\(\\mathbb{P}(A \\cup B) = \\mathbb{P}(A) + \\mathbb{P}(B) - \\mathbb{P}(A \\cap B)\\) If \\(A \\subseteq B\\), then \\(\\mathbb{P}(A) \\leq \\mathbb{P}(B)\\) \\(\\mathbb{P}(A) = \\sum_{i=1}^\\infty \\mathbb{P}(A \\cap C_i)\\) for any partition \\(C_1,C_2,\\dots\\) of \\(\\Omega\\) (Law of Total Probability) 1.2.4 Why measure theory? Consider the uniform distribution on a random variable \\(X\\) on the unit interval, denoted \\(X\\sim\\mathop{\\mathrm{Unif}}(0,1)\\). You may have come across this before, and know that the probability that \\(X\\) lies in any interval contained in \\([0,1]\\) is simply the length of the interval, i.e. \\[\\begin{equation} \\mathbb{P}\\big([a,b]\\big) = \\mathbb{P}\\big([a,b)\\big) = \\mathbb{P}\\big((a,b]\\big) = \\mathbb{P}\\big((a,b)\\big) = b-a, \\tag{1.1} \\end{equation}\\] for \\(0 \\leq a \\leq b \\leq 1\\). This definition works fine for the degenerate case \\(\\mathbb{P}(\\{a\\})=0\\) for the singleton set \\(\\{a|a\\in(0,1)\\}\\). In general, if \\(A\\) and \\(B\\) are disjoint subsets of \\([0,1]\\) then \\[\\begin{equation} \\mathbb{P}(A \\cup B) = \\mathbb{P}(A) + \\mathbb{P}(B), \\end{equation}\\] and we can even extend this notion to that of countable additivity \\[\\begin{equation} \\mathbb{P}\\left( \\cup_{i=1}^\\infty A_i \\right) = \\sum_{i=1}^\\infty \\mathbb{P}(A_i), \\tag{1.2} \\end{equation}\\] for disjoint sets \\(\\{A_1,A_2,\\dots\\}\\)10. For a uniform measure on \\([0,1]\\), one expects that the measure of some subset \\(A \\subseteq [0,1]\\) to be unaffected by “shifting” (with wrap-around) of that subset by some fixed amount \\(r\\in[0,1]\\). Define the \\(r\\)-shift of \\(A\\subseteq [0,1]\\) by \\[ A \\oplus r := \\left\\{ a + r \\mid a \\in A, a+r \\leq 1 \\right\\} \\cup \\left\\{ a + r - 1 \\mid a \\in A, a+r &gt; 1 \\right\\}. \\] Then we should have \\[\\begin{equation} \\mathbb{P}(A \\oplus r) = \\mathbb{P}(A). \\tag{1.3} \\end{equation}\\] Figure 1.1: An interval in [0,1] shifted by some fixed amount, with wrap-around, should have consistent length. At this point you might notice that all of this resonates with the previous example on the Lebesgue measure, except perhaps the shifting part, and indeed that is the case. Suppose that we dispense with measure theory and do not define things like the \\(\\sigma\\)-algebra on the \\([0,1]\\) or the triplet \\((\\Omega,{\\mathcal F},\\mathbb{P})\\), and only use the above probability definitions given in (1.1), (1.2), and (1.3). How far can we push the boundaries of such probability definitions before things start to breakdown? Consider these questions: What is the probability that \\(X\\) is rational? What is the probability that \\(X^n\\) is rational for some positive integer \\(n\\)? What is the probability that \\(X\\) is algebraic11? All seemingly fair and interesting questions, but are they well defined? Can we actually measure them and assign probabilities to such events? Taking a step back further, we ask: Are all possible subsets \\(A\\subseteq [0,1]\\) measurable? Does \\(\\mathbb{P}(A)\\) even make sense for any event \\(A\\) we can think of? It turns out the answer is no, and can be proven by contradiction with the help of equivalence relations. You may skip this part if you wish, but I’ll try to keep the technicalities to a minimum (or at least provide some helpful intuition where needed). This shows the need for the heavy machinery that is measure theory for assigning probabilities to events12. Proposition 1.1 There does not exist a definition of \\(\\mathbb{P}(A)\\), defined for all subsets \\(A\\subseteq[0,1]\\), satisfying (1.1), (1.2), and (1.3). Proof. All we need to show is the existence of one such subset of \\([0,1]\\) whose measure is undefined. The set we are about to construct is called the Vitali set13, after Giuseppe Vitali who described it in 1905. Define an equivalence relation on \\([0,1]\\) by the following: \\[x\\sim y \\Rightarrow x-y \\in \\mathbb{Q}\\] That is, two real numbers \\(x\\) and \\(y\\) are deemed to be the same if their difference is a rational number. We would like to separate all the real numbers between 0 and 1 inclusive by this equivalence relation, and collect them into groups called equivalence classes. For instance, the numbers 0 and 0.5 will be in the same category, since their difference is rational, but the numbers 0.5 and \\(\\pi^{-1}\\) would be in separate categories. There will be infinitely many such equivalence classes, but we’re not particularly concerned so much about how many there are. Construct the Vitali set \\(V\\) as follows: Take precisely one element from each equivalent class, and put it in \\(V\\). As a remark, such a \\(V\\) must surely exist by the Axiom of Choice14. Consider now the union of shifted Vitali sets by some value rational value \\(r\\in[0,1]\\), \\[ \\bigcup_{r} (V \\oplus r) \\] As a reminder, the set of rational numbers is countably infinite15. We make a few observations: The equivalence relation partitions the interval \\([0,1]\\) into a disjoint union of equivalence classes. In other words, the sets \\((V \\oplus r)\\) and \\((V \\oplus s)\\) are disjoint for any rationals \\(r\\neq s\\), such that \\(r,s\\in[0,1]\\). If they were not disjoint, this would mean that there exists some \\(x,y\\in[0,1]\\) with \\(x+r\\in(V \\oplus r)\\) and \\(y+s\\in (V \\oplus s)\\) such that \\(x+r=y+s\\). But then this means that \\(x-y=s-r\\in\\mathbb{Q}\\) so \\(x\\) and \\(y\\) are in the same equivalent class, and this is a contradiction. Every point in \\([0,1]\\) is contained in the union \\(\\bigcup_{r} (V \\oplus r)\\). To see this, fix a point \\(x\\) in \\([0,1]\\). Note that this point belongs to some equivalent class of \\(x\\), and in this equivalence class there exists some point \\(\\alpha\\) which belongs to \\(V\\) as well by construction. Hence, \\(\\alpha \\sim x\\), and thus \\(x-\\alpha=r\\in\\mathbb{Q}\\), implying that \\(x\\) is a point in the Vitali set \\(V\\) shifted by \\(r\\). Therefore, \\[[0,1] \\subseteq \\bigcup_{r} (V \\oplus r).\\] and we may write \\[1 = \\mathbb{P}([0,1]) \\leq \\mathbb{P}\\left(\\bigcup_{r} (V \\oplus r)\\right),\\] since the measure of any set contained in another must have smaller or equal measure. This relation is in fact implied by (1.2). Let \\(A\\) and \\(B\\) be such that \\(A \\subseteq B\\). Then we may write \\(B = A \\cup (B-A)\\) where the sets \\(A\\) and \\(B-A\\) are disjoint. Hence, \\(\\mathbb{P}(B)=\\mathbb{P}(A)+\\mathbb{P}(B-A)\\), and since measures are non-negative and in particular \\(\\mathbb{P}(B-A)\\in[0,1]\\), we have that \\(\\mathbb{P}(B)\\geq \\mathbb{P}(A)\\). However since the probability measure cannot be greater than 1, it must be equal to 1. The disjoint union \\(\\bigcup_{r} (V \\oplus r)\\) has probability measure (according to our definitions in (1.1), (1.2), and (1.3)) \\[\\begin{align*} \\mathbb{P}\\left(\\bigcup_{r} (V \\oplus r)\\right) &amp;= \\sum_r \\mathbb{P}(V \\oplus r) \\\\ &amp;= \\sum_r \\mathbb{P}(V) \\end{align*}\\] Putting these three observations together gives us \\[ 1 = \\mathbb{P}\\left(\\bigcup_{r} (V \\oplus r)\\right) = \\sum_r \\mathbb{P}(V). \\] This leads to the desired contradiction: A countably infinite sum of the same quantity repeated can only equal 0, \\(+\\infty\\), or \\(-\\infty\\), but it can never equal 1. In summary, Not all subsets of uncountable sets are measurable. Admitting all subsets of uncountable sets will break mathematics. \\(\\sigma\\)-algebras are the patch that fixes mathematics. It gatekeeps the subsets of uncountable sets and disregards those which are not measurable. Actually, if you have been following along, you might realise that we are at risk of breaking mathematics when dealing with uncountable sets. Strictly speaking, we only need \\(\\sigma\\)-algebras when working in a set with uncountable cardinality. Finally, what on earth is an “unmeasurable” set? Wouldn’t it be (even arbitrarily) possible to just define a measure for whatever set we can think of? If the above example hasn’t convinced you enough, some other mathematicians have tried to resolve this but it seems it is not possible to do so without encountering paradoxes, such as the one below. The Banach–Tarski paradox states that a ball in the ordinary Euclidean space can be doubled using only the operations of partitioning into subsets, replacing a set with a congruent set, and reassembly. To be clear, no rule of mathematics are broken in the Banach-Tarski paradox, but the result defies intuition. Another statement of this paradox is that we can chop up a pea into finitely many pieces and reassemble it into the sun (pea-sun paradox). If we don’t lay out the foundations for measuring probabilities rigorously, we can end up with nonsensical answers! This section was highly inspired by the following references: Rosenthal, J. (2006). A first look at rigorous probability. The discussion here: https://stats.stackexchange.com/q/199280 This YouTube video on Vitali Sets: https://youtu.be/ameugr-wjeI For the example at hand, the power set is \\({\\mathcal P}(\\Omega)=\\{ \\{\\}, \\{1\\}, \\{2\\}, \\{3\\}, \\{1,2\\}, \\{1,3\\}, \\{2,3\\}, \\{1,2,3\\} \\}\\)↩︎ You may notice that other \\(\\sigma\\)-algebras are indeed possible, e.g. the power set of \\(\\Omega\\) in this case. There is a notion of the smallest \\(\\sigma\\)-algebra containing the collection of “basic events”. Luckily for us, the event space that we will usually be working with will be the smallest \\(\\sigma\\)-algebra without much technicalities, so we shall not explore this concept any further.↩︎ https://idioms.thefreedictionary.com/How+long+is+a+piece+of+string%3F↩︎ A concrete example of this is for the sets \\(A_1=(0,1/2)\\), \\(A_2=(1/2, 3/4)\\), \\(A_3=(3/4,7/8)\\), and so on (adding half the interval at each iteration). One finds that the measure of the countable union is \\(\\sum_{i=1}^\\infty (1/2)^i=1\\).↩︎ An algebraic number is a number that is a root of a non-zero polynomial in one variable with integer coefficients.↩︎ Or at least, for cases where “not so nice” events need to be measured.↩︎ https://en.wikipedia.org/wiki/Vitali_set↩︎ Given a collection of non-empty sets, it is always possible to construct a new set by taking one element from each set in the original collection. See https://brilliant.org/wiki/axiom-of-choice/↩︎ https://www.homeschoolmath.net/teaching/rational-numbers-countable.php↩︎ "],["conditional-probabilities.html", "1.3 Conditional probabilities", " 1.3 Conditional probabilities 1.3.1 Conditional probability Update the sample space based on new information, and thus update probability calculations. Definition 1.3 (Conditional probabilities) If \\(A\\) and \\(B\\) are events in \\(\\Omega\\), and \\(\\mathbb{P}(B)&gt;0\\), then the conditional probability of \\(A\\) given \\(B\\) is \\[ \\mathbb{P}(A | B) = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)}. \\] The given information is now the “new” sample space: \\(\\mathbb{P}(B | B) = 1\\). All further occurrences are calibrated with respect to their relation to \\(B\\). Think of \\(\\mathbb{P}(A | B)\\) as the fraction of times \\(A\\) occurs among those in which \\(B\\) occurs. For mutually exclusive events \\(A\\) and \\(B\\), \\(\\mathbb{P}(A | B)=\\mathbb{P}(B | A)=0\\). In general, \\(\\mathbb{P}(A | B) \\neq \\mathbb{P}(A)\\) and \\(\\mathbb{P}(A | B) \\neq \\mathbb{P}(B | A)\\). Example 1.7 A medical test for a disease \\(D\\) has outcomes ‘\\(+\\)’ and ‘\\(-\\)’. The probabilities are: \\(D\\) \\(D^c\\) \\(+\\) 0.009 0.099 \\(-\\) 0.001 0.891 From the definition of conditional probability, \\[ \\mathbb{P}(+|D) = \\frac{\\mathbb{P}(+ \\cap D)}{\\mathbb{P}(D)} = \\frac{0.009}{0.009 + 0.001} = 0.90 \\] and \\[ \\mathbb{P}(-|D^c) = \\frac{\\mathbb{P}(- \\cap D^c)}{\\mathbb{P}(D^c)} = \\frac{0.891}{0.099 + 0.891} \\approx 0.90. \\] Suppose you go for a test and get a positive result. What is the probability you have the disease? Most will answer 0.90. Actually, \\[ \\mathbb{P}(D|+) = \\frac{\\mathbb{P}(D \\cap +)}{\\mathbb{P}(+)} = \\frac{0.009}{0.009 + 0.099} = 0.08. \\] Notice that \\(\\mathbb{P}(D \\cap +) = \\mathbb{P}(+|D)\\mathbb{P}(D)\\) after some rearranging; and \\(\\mathbb{P}(+) = \\mathbb{P}(+ \\cap D) + \\mathbb{P}(+ \\cap D^c)\\) since \\(D\\) and \\(D^c\\) are disjoint. We can write \\[ \\mathbb{P}(D|+) = \\frac{\\mathbb{P}(+|D)\\mathbb{P}(D)}{\\mathbb{P}(+|D)\\mathbb{P}(D) + \\mathbb{P}(+|D^c)\\mathbb{P}(D^c)}. \\] For \\(\\mathbb{P}(D|+)\\) to be large, it seems \\(\\mathbb{P}(D)\\) needs to be large in addition to \\(\\mathbb{P}(+|D)\\), i.e. disease is prevalent. 1.3.2 Bayes Theorem Theorem 1.1 (Bayes' Rule) Let \\(A_1,A_2,\\dots\\) be a partition of the sample space, and let \\(B\\) be any set. Then, for each \\(i=1,2,\\dots\\), \\[ \\mathbb{P}(A_i|B) = \\frac{\\mathbb{P}(B|A_i)\\mathbb{P}(A_i)}{\\sum_{j=1}^\\infty \\mathbb{P}(B|A_j)\\mathbb{P}(A_j)}. \\] This is easily proven using definitions of conditional probabilities, as well as the law of total probability. Remark. Some will call \\(\\mathbb{P}(A_i)\\) the prior probability, and the \\(\\mathbb{P}(A_i|B)\\) posterior probability. "],["independent-events.html", "1.4 Independent events", " 1.4 Independent events 1.4.1 Independence In some cases, the occurrence of a particular event \\(B\\) has no effect on the probability of another event \\(A\\): \\[ \\mathbb{P}(A | B) = \\mathbb{P}(A). \\] If this is true, we can use the relationship \\(\\mathbb{P}(A \\cap B) = \\mathbb{P}(A | B)\\mathbb{P}(B)\\) to derive the following definition. Definition 1.4 Two events \\(A\\) and \\(B\\) are statistically independent if and only if \\[ \\mathbb{P}(A \\cap B) = \\mathbb{P}(A)\\mathbb{P}(B). \\] If \\(A\\) and \\(B\\) are independent then so too are \\(A\\) and \\(B^c\\); \\(A^c\\) and \\(B\\); and \\(A^c\\) and \\(B^c\\). (Probably not) Rev. Thomas Bayes c. 1701 – 7 April 1761 Here’s an experiment we can do to examine the concept of independent events. Consider tossing a fair die. Let \\(A = \\{2, 4, 6\\}\\) and \\(B = \\{1,2,3,4\\}\\). You should be able to work out, using the above probability results and the definition of conditional probabilities, that \\(\\mathbb{P}(A)=1/2\\), \\(\\mathbb{P}(B)=2/3\\), and \\(\\mathbb{P}(A \\cap B)=1/3\\). Hence, we deduce that \\(A\\) and \\(B\\) are independent, since the product of each probability event is the probability of their intersection. If you were feeling bored and had a lot of time to spare, you could verify this empirically using an actual die. While this would be an afternoon well spent, let’s use R to simulate some draws from the sample space \\(\\Omega = \\{1,2,3,4,5,6\\}\\), and count the number of times each events \\(A\\), \\(B\\) and \\(A \\cap B\\) occurs. # Throw a dice 10 times sample(1:6, size = 10, replace = TRUE) ## [1] 3 6 3 2 2 6 3 5 4 6 From the above, \\(n(A) = 6\\), \\(n(B)=6\\), and \\(n(A \\cap B)=3\\). Do this 1,000 times, and count events automatically. x &lt;- sample(1:6, size = 1000, replace = TRUE) head(x, 100) ## [1] 6 1 2 3 5 3 3 1 4 1 1 5 3 2 2 1 6 3 4 6 1 3 5 4 2 5 1 1 2 3 4 5 5 3 6 1 2 ## [38] 5 5 4 5 2 1 1 3 1 6 5 1 2 4 4 6 6 3 6 6 1 6 2 1 2 4 5 5 6 3 1 4 6 1 6 1 3 ## [75] 6 4 1 6 6 3 6 5 3 6 2 5 5 3 2 2 2 4 2 2 6 4 4 6 1 6 nA &lt;- sum(x %in% c(2, 4, 6)) # counts the frequency of 2, 4, 6 nB &lt;- sum(x %in% c(1, 2, 3, 4)) # counts the frequency of 1, 2, 3, 4 nAB &lt;- sum(x %in% c(2, 4)) # counts the frequency of 2, 4 # Results c(A = nA, B = nB, AnB = nAB) / 1000 ## A B AnB ## 0.495 0.674 0.333 Empirically, we have \\(\\hat{\\mathbb{P}}(A)\\hat{\\mathbb{P}}(B) =\\) 0.495 \\(\\times\\) 0.674\\(=\\) 0.33363. This matches with the value of \\(\\hat{\\mathbb{P}}(A \\cap B)\\) in the table, as well as the theoretical value of 1/3. "],["random-variables.html", "1.5 Random variables", " 1.5 Random variables Ask (randomly) 50 people whether they like (“1”) or dislike (“0”) learning statistics. What is the sample space for this experiment? This would be all 1/0 combinations such as \\[\\begin{align*} \\overbrace{1000101\\cdots 10001}^{50} \\end{align*}\\] Specifically, \\(\\Omega = \\big\\{(X_1,X_2,\\dots,X_{50}) \\,|\\, X_i \\in \\{0,1\\} \\big\\}\\). Realise that \\(|\\Omega| = 2^{50}\\). This is huge! 2 ^ {50} ## [1] 1.1259e+15 For context, the average American, working full-time, would have to work 25 billion years to earn 1 quadrillion dollars. What if we defined \\(Y = \\sum_{i=1}^{50} X_i\\)? \\(Y\\) is the count of the number of people who like learning statistics from this sample of 50. The minimum value for \\(Y\\) is 0, and the maximum is 50. So the new sample space for \\(Y\\) is \\(S = \\{0,1,2,\\dots,50\\}\\). Much easier to deal with! \\(Y\\) was defined by mapping a function from the original sample space \\(\\Omega\\) to the new space \\(S\\) (usually a set of real numbers). \\(Y\\) is called a random variable. Remark. Random variables (r.v.) are conventionally denoted with uppercase letters, and the realised values of the variable will be denoted by the corresponding lowercase letters. Thus, the r.v. \\(X\\) can take the value \\(x\\). Example 1.8 Flip a coin twice and let \\(X\\) be the number of heads. The sample space of the coin flips is \\(\\Omega = \\{\\text{HH}, \\text{HT}, \\text{TH}, \\text{TT} \\}\\). The sample space of \\(X\\) is \\(S = \\{0,1,2 \\}\\). The mapping of the r.v. is illustrated as follows: We can see that a r.v. \\(X\\) is a mapping16 \\(X:\\Omega \\to \\mathbb{R}\\) that assigns a real number \\(X(\\omega)\\) to each outcome \\(\\omega\\). Then, \\[ \\mathbb{P}(X=x) = \\mathbb{P}\\big(X^{-1}(x)\\big) = \\mathbb{P}\\big(\\{ \\omega \\in \\Omega \\,|\\, X(\\omega) = x\\} \\big) \\] Example 1.9 The r.v. \\(X\\) can be summarised as follows: \\(\\omega\\) \\(\\mathbb{P}(\\{\\omega\\})\\) \\(X(\\omega)\\) TT 1/4 0 TH 1/4 1 HT 1/4 1 HH 1/4 2 \\(x\\) \\(\\mathbb{P}(X = x)\\) \\(X^{-1}(x)\\) 0 1/4 TT 1 1/2 TH, HT 2 1/4 HH Technically, a measurable function. See Wasserman (2004, Appendix 2.13).↩︎ "],["distribution-functions.html", "1.6 Distribution functions", " 1.6 Distribution functions With every random variable \\(X\\), we associate a function called the cumulative distribution function of \\(X\\). Definition 1.5 The cumulative distribution function (cdf) of a r.v. \\(X\\), is the function \\(F_X:\\mathbb{R}\\to[0,1]\\) defined by \\[ F_X(x) = \\mathbb{P}(X \\leq x), \\text{ for all } x. \\] Example 1.10 From Example 1.9, we have that \\[ F_X(x)= \\begin{cases} 0 &amp;x &lt; 0 \\\\ 0.25 &amp;0 \\leq x &lt; 1 \\\\ 0.75 &amp;1 \\leq x &lt; 2 \\\\ 1 &amp;x \\geq 2 \\\\ \\end{cases} \\] This can be sketched as follows: Figure 1.2: Distribution function of the random variable \\(X\\) 1.6.1 Properties of cdfs \\(\\lim_{x\\to-\\infty} F(x) = 0\\) and \\(\\lim_{x\\to+\\infty} F(x) = 1\\). \\(F(x)\\) is non-decreasing: \\(x_1 &lt; x_2 \\Rightarrow F(x_1) \\leq F(x_2)\\). Drawing the function from left to right, it must either increase or stay the same value, but not decrease in value. \\(F(x)\\) is right-continuous: for every \\(x_0\\), \\(\\lim_{x \\downarrow x_0} F(x) = F(x_0)\\). This means “the solid dots will be on the left of the step function”. In fact, any function satisfying the above properties is a cdf. For proofs of these facts, see the reference textbooks. Clearly, \\(F\\) itself can be discontinuous (we saw this in the previous example). This is associated with whether the r.v. \\(X\\) is continuous or not. That is, \\(F_X(x)\\) is a continuous function \\(\\Rightarrow\\) \\(X\\) is continuous. \\(F_X(x)\\) is a step function \\(\\Rightarrow\\) \\(X\\) is discrete. 1.6.2 Identically distributed r.v. Let \\(X\\) have cdf \\(F\\) and let \\(Y\\) have cdf \\(G\\). If \\(F(x)=G(x)\\) for all \\(x\\), then \\(\\mathbb{P}(X\\in A) = \\mathbb{P}(Y \\in A)\\) for all (measurable) sets. \\(X\\) and \\(Y\\) are said to be identically distributed. Remark. Note that two identically distributed r.v. are not necessarily equal in value, only the probabilities of observing the same values are identical. Think about two independent coin flips. The probability of H/T in each flip is the same, but the outcome may not be. "],["probability-functions.html", "1.7 Probability functions", " 1.7 Probability functions 1.7.1 Probability mass function Associated with a r.v. \\(X\\) and its cdf \\(F_X\\) is another function, called either the probability density function (pdf) if it is continuous, or the probability mass function (pmf) if it is discrete. Definition 1.6 (Probability mass function) A discrete r.v. \\(X\\) only take countably many values \\({\\mathcal X}= \\{x_1, x_2,\\dots \\}\\). Its probability mass function (pmf) is \\[ f_X(x) = \\mathbb{P}(X=x), \\text{ for all } x \\in {\\mathcal X}. \\] Example 1.11 The pmf from Example 1.9 is given by \\[ f_X(x) = \\begin{cases} 1/4&amp;x=0 \\\\ 1/2&amp;x=1 \\\\ 1/4&amp; x=2\\\\ 0 &amp;\\text{otherwise}\\\\ \\end{cases} \\] Pmfs measure “point probabilities”. Since outcomes of discrete r.v.s are countable, we can add up probabilities over all the points in the event. For any \\(a\\), \\(b\\) both in \\({\\mathcal X}\\) such that \\(a \\leq b\\), we have that \\[ \\mathbb{P}(a \\leq X \\leq b) = \\sum_{x=a}^b f_X(x). \\] As a special case we get \\[ \\mathbb{P}(X \\leq b) = \\sum_{x\\leq b} f_X(x) = F_X(b). \\] Consequently, Each \\(f_x(x) \\geq 0\\) for all \\(x\\); and \\(\\sum_{x} f_x(x) = 1\\). 1.7.2 Probability density functions We want to translate the same idea of “point probabilities” over to the continuous case, but must be more careful here. Let \\(X\\) be a continuous r.v. (i.e., its cdf is continuous). The analogous procedure would be to consider \\[ \\mathbb{P}(X \\leq x) = F_X(x) = \\int_{-\\infty}^x f_X(x) \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!x, \\] and using the Fundamental Theorem of Calculus, we have that \\[ f_X(x) = \\frac{\\text{d}}{\\text{d}x}F_X(x). \\] We can see that the cdf is like “adding up” the “point probabilities” \\(f_X(x)\\) to obtain interval probabilities. Definition 1.7 (Probability density function) A continuous r.v. \\(X\\) takes any numerical value in an interval or collection of intervals (having an uncountable range). Its probability density function (pdf) is the function \\(f_X(x)\\) that satisfies \\[ F_X(x) = \\int_{-\\infty}^x f_X(\\tilde x) \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!\\tilde x, \\text{ for all } x. \\] Note that \\(f_X(x) \\geq 0\\) for all \\(x\\); \\(\\int_{-\\infty}^\\infty f_X(x) \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!x = 1\\); and \\(\\mathbb{P}(X=x) = \\int_x^x f(\\tilde x) \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!\\tilde x = 0\\). Don’t think of \\(f(x)\\) as probability functions–this only holds for discrete r.v.. Read Wasserman (Warning after Example 2.13 on p.24). Example 1.12 Suppose that \\(X\\) is uniformly distributed on the interval \\((a,b) \\subset \\mathbb{R}\\). Its pdf is given by \\[ f_X(x) = \\begin{cases} \\frac{1}{b-a} &amp; a &lt; x &lt; b \\\\ 0 &amp; \\text{otherwise} \\end{cases} \\] When \\(a &lt; x &lt; b\\), the cdf is \\[\\begin{align*} F_X(x) = \\int_{-\\infty}^x f_X(\\tilde x) \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!\\tilde x &amp;=\\cancelto{0}{\\int_{-\\infty}^a f_X(\\tilde x) \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!\\tilde x} + \\int_{a}^x \\frac{1}{b-a} \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!\\tilde x \\\\ &amp;= \\left[ \\frac{\\tilde x}{b-a} \\right]_a^x = \\frac{x-a}{b-a}, \\end{align*}\\] while \\(F_X(x) = 0\\) for \\(x&lt;a\\), and \\(F_X(x)=1\\) for \\(x&gt;b\\). Figure 1.3: Plot of pdf Figure 1.4: Plot of cdf Continuous r.v. miscellanea On notation: we write \\(X \\sim F_X(x)\\) to mean that “\\(X\\) has a distribution given by \\(F_X(x)\\)”. The symbol ‘\\(\\sim\\)’ is read “is distributed as”. Sometimes we write \\(X \\sim f_X(x)\\). Or by their specially given name, e.g. \\(X\\sim \\mathop{\\mathrm{Unif}}(a,b)\\). If \\(X\\) and \\(Y\\) are identically distributed, then \\(X\\sim Y\\). Note that since \\(\\mathbb{P}(X=0)\\) if \\(X\\) is continuous, then \\(\\mathbb{P}(X \\leq b) = \\mathbb{P}(X &lt;b) + \\cancelto{0}{\\mathbb{P}(X=b)}\\); and so \\(\\mathbb{P}(a \\leq X \\leq b) = \\mathbb{P}(a \\leq X &lt; b) = \\mathbb{P}(a &lt; X \\leq b) = \\mathbb{P}(a &lt; X &lt; b)\\). Sometimes we just write \\(\\int f(x) \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!x\\) to mean \\(\\int_{-\\infty}^{\\infty} f(x) \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!x\\). The following are true: \\(\\mathbb{P}(a&lt;X&lt;b) = F(b) - F(a)\\) (be careful, this is not true for discrete r.v.) \\(\\mathbb{P}(X &gt; a) = 1 - F(a)\\) As a side note, mixed discrete and continuous distributed r.v. do exist, but we won’t be covering them in this course. The Riemann integral is defined as the limit of the of the areas of these bars, as the number of bars gets larger and larger (and hence the width of the bars get smaller and smaller). "],["multiple-random-variables.html", "1.8 Multiple random variables", " 1.8 Multiple random variables 1.8.1 Bivariate distributions Probability models may involve more than one random variable, known as multivariate models. Consider the simplest kind, dealing with only two r.v.s in each discrete and continuous case. Definition 1.8 (Joint mass function) Given a pair of discrete r.v. \\(X\\) and \\(Y\\), the joint mass function or joint pmf is defined by \\[ f_{X,Y}(x,y) = \\mathbb{P}(X=x,Y=y). \\] Definition 1.9 (Joint density function) A function \\(f_{X,Y}:\\mathbb{R}^2\\to\\mathbb{R}\\) is called a joint probability density function (pdf) of the continuous random vector \\((X,Y)\\) if for any set \\(A\\subseteq\\mathbb{R}^2\\), \\[ \\mathbb{P}((X,Y) \\in A) = \\iint_{A} f_{X,Y}(x,y)\\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!x \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!y. \\] All the univariate properties carry over to the bivariate (and multivariate) case: \\(f_{X,Y}(x,y) \\geq 0\\) for all \\((x,y) \\in \\mathbb{R}^2\\) \\(\\sum_x\\sum_y f(x,y) = 1\\) if discrete, \\(\\iint f(x,y)\\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!x \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!y = 1\\) if continuous The joint cdf is defined \\(F_{X,Y}(x,y) = \\mathbb{P}(X\\leq x, Y\\leq y)\\) Example 1.13 A bivariate distribution for two discrete r.v. \\(X\\) and \\(Y\\) each taking values 0 or 1: \\(Y=0\\) \\(Y=1\\) \\(X=0\\) 1/9 2/9 \\(X=1\\) 2/9 4/9 For e.g., \\(\\mathbb{P}(X=1,Y=1) = f(1,1) = 4/9\\). Example 1.14 Consider a uniform distribution on the unit square \\([0,1] \\times [0,1]\\). It has pdf given by \\[ f(x,y) = \\begin{cases} 1 &amp;0\\leq x \\leq 1, 0\\leq y \\leq 1 \\\\ 0 &amp;\\text{otherwise} \\end{cases} \\] This is a well-defined pdf, as \\(f\\geq 0\\) and \\(\\int\\int f(x,y)\\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!x \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!y = 1\\). Suppose we want to find \\(\\Pr(X&lt;1/2, Y&lt;1/2)\\) and \\(\\Pr(X + Y &lt; 1)\\). \\[\\begin{align*} \\mathbb{P}(X&lt;1/2, Y&lt;1/2) &amp;= \\int_0^{1/2} \\int_0^{1/2} \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!x \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!y \\\\ &amp;= \\left[ \\left[ xy \\right]_{0}^{1/2} \\right]_{0}^{1/2} = 1/4. \\end{align*}\\] For the second probability, note that the set \\(\\{x+y&lt;1\\}\\) corresponds to \\(\\{0&lt;y&lt;1, 0&lt;x &lt; 1-y\\}\\). \\[\\begin{align*} \\mathbb{P}(X+Y&lt;1) &amp;= \\int_0^{1} \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!y \\int_0^{1-y} \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!x \\\\ &amp;= \\int_0^{1} \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!y [x]_0^{1-y} \\\\ &amp;= \\int_0^{1} (1-y) \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!y = \\big[y - y^2/2\\big]_0^1 = 1/2. \\end{align*}\\] This is the view from above. What is the volume of this wedge? It is the area of the shaded region multiplied by height 1. 1.8.2 Marginal distributions We can recover the distribution for one of the r.v. in a bivariate (or multivariate) model by summing/integrating over the remaining probibility distribution. Definition 1.10 (Marginal distribution) For a bivariate r.v. \\((X,Y)\\), the marginal distributions may be obtained as \\[ f_X(x) = \\begin{cases} \\sum_y f_{X,Y}(x,y) &amp;\\text{if $Y$ is discrete} \\\\ \\int_y f_{X,Y}(x,y) \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!y &amp;\\text{if $Y$ is continuous} \\\\ \\end{cases} \\] \\[ f_Y(y) = \\begin{cases} \\sum_x f_{X,Y}(x,y) &amp;\\text{if $X$ is discrete} \\\\ \\int_x f_{X,Y}(x,y) \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!x &amp;\\text{if $X$ is continuous} \\\\ \\end{cases} \\] See the textbooks for some examples. A note to say that since the joint cdf is defined to be \\[ F(x,y) = \\mathbb{P}(X \\leq x, Y\\leq y), \\] the marginal cdfs can be obtained from the joint cdf for \\(X\\) as \\[\\begin{align*} F_X(x) &amp;= \\sum_{k\\leq x} \\left( \\sum_y f_{X,Y}(k,y) \\right) \\\\ &amp;= \\mathbb{P}(X \\leq x, Y \\leq \\infty) \\\\ &amp;=F_{X,Y}(x,\\infty) \\end{align*}\\] and similarly \\(F_Y(y)=F_{X,Y}(\\infty,y)\\) for \\(Y\\). Note that for continuous random variables, \\(\\int_{-\\infty}^x \\left(\\int f_{X,Y}(\\tilde x,y) \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!y \\right) \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!\\tilde x\\). 1.8.3 Conditional distributions Oftentimes when two r.v. \\((X,Y)\\) are observed, the values of the two variables are related. Some examples: Height (\\(X\\)) and weight (\\(Y\\)) of a person; A level points score (\\(X\\)) and socio-economic status (\\(Y\\)); Heart rate (\\(X\\)) and oxygen saturation levels (\\(Y\\)). Knowledge about the value of \\(Y\\) gives us some information about the value of \\(Y\\). This should sound familiar. Definition 1.11 (Conditional distributions, discrete) If \\(X\\) and \\(Y\\) are discrete, the conditional pmf of \\(X\\) given \\(Y=y\\) is \\[ f_{X|Y}(x|y) = \\mathbb{P}(X=x|Y=y) = \\frac{\\mathbb{P}(X=x, Y=y)}{\\mathbb{P}(Y=y)} = \\frac{f_{X,Y}(x, y)}{f_Y(y)}, \\] Definition 1.12 (Conditional distributions, continuous) If \\(X\\) and \\(Y\\) are discrete, the conditional of of \\(X\\) given \\(Y=y\\) is \\[ f_{X|Y}(x|y) = \\frac{f_{X,Y}(x, y)}{f_Y(y)}. \\] As a function of \\(x\\), \\(f_{X|Y}(x|y)\\) is indeed a pdf, since \\[ \\int f_{X|Y}(x|y) \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!x = \\int \\frac{f_{X,Y}(x, y)}{f_Y(y)} \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!x = \\frac{f_Y(y)}{f_Y(y)} = 1. \\] The probability of \\(X\\) given \\(Y=y\\) is computed as \\[ \\mathbb{P}(X \\in A|Y=y) = \\int_A f_{X|Y}(x|y) \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!x. \\] We can rearrange the equations to yield \\[f_{X|Y}(x|y)f_Y(y)=f_{Y|X}(y|x)f_X(x).\\] Example 1.15 Let \\(X\\) and \\(Y\\) have the joint pdf \\(f(x,y)=x+y\\) for \\(0\\leq x,y\\leq 1\\). Suppose \\(Y=a\\) has been observed, where \\(a\\in[0,1]\\). Firstly, the the pdf of \\(Y\\) is \\[ f_Y(y) = \\int_0^1 (x+y) \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!x = \\left[xy +y^2/2\\right]_0^1 = y + 1/2. \\] The conditional pdf for \\(X\\) is \\[ f_{X|Y}(x|Y=a) = \\frac{f_{X,Y}(x,Y=a)}{f_Y(a)} = \\frac{x+a}{a + 1/2}. \\] We can compute \\(\\mathbb{P}(X&lt;1/4|Y=1/3)\\) by \\[\\begin{align*} \\mathbb{P}(X&lt;1/4|Y=1/3) = (1/3 + 1/2)^{-1} \\int_{0}^{1/4} (x+1/3) \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!x = 11/80. \\end{align*}\\] 1.8.4 Independent random variables Previously we came across the concept of independence of probabilistic events. We can extend this notion to random variables. Definition 1.13 (Independece of r.v.) Two r.v. \\(X\\) and \\(Y\\) are independent if and only if for every \\(x\\in\\mathbb{R}\\) and \\(y\\in\\mathbb{R}\\), \\[ f_{X,Y}(x,y) = f_X(x)f_Y(y). \\] We write \\(X \\perp Y\\). Apparently, if there exists functions \\(g(x)\\) and \\(h(y)\\) (not necessarily pdfs) such that \\(f(x,y)=g(x)h(y)\\) for all \\(x,y\\), then \\(X\\) and \\(Y\\) are independent. The assumption of independence is used very often in statistical inference as it simplifies calculations quite a lot. Example 1.16 Recall the bivariate distribution on the unit square (c.f. Example 1.14). Note that the pdf of \\(X\\) is \\(f_X(x) = \\int_0^1 \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!y = 1\\), and similarly \\(f_Y(y)=1\\). It is easy to see that \\(X\\) and \\(Y\\) are independent, since \\[ f_{X,Y}(x,y) = 1 = f_X(x)f_Y(y). \\] As a consequence, to generate a random sample from \\((X,Y)\\), one can randomly sample values \\(X\\sim\\mathop{\\mathrm{Unif}}(0,1)\\), and independently sample \\(Y\\sim\\mathop{\\mathrm{Unif}}(0,1)\\). "],["expectations.html", "1.9 Expectations", " 1.9 Expectations 1.9.1 Expected values The expected value, or expectation, of a random variable is merely its average value weighted according to the probability distribution. It signifies the typical value of an observation of a random variable. The symbol ‘\\(\\mu\\)’ is often used to denote the expected value. Other symbols used are \\(\\mathop{\\mathrm{E}}X\\) and \\(\\mathop{\\mathrm{E}}[X]\\). The expectation is not to be confused with the sample mean of a set of observations \\(\\{x_1,\\dots,x_n \\}\\), i.e. \\(\\bar x_n = \\frac{1}{n} \\sum_{i=1}^n x_i\\). Example 1.17 Let \\(X \\in \\{0,1\\}\\) take value 1 with probability \\(p\\), and 0 with probability \\(1-p\\). \\(X\\) is a Bernoulli r.v., and we write \\(X \\sim \\mathop{\\mathrm{Bern}}(p)\\). Then, \\[ \\mathop{\\mathrm{E}}(X) = \\sum_x x\\mathbb{P}(X = x) = 1\\cdot p + 0 \\cdot (1-p) = p. \\] Example 1.18 Let \\(X\\) be a cts. r.v. such that \\(X\\sim\\mathop{\\mathrm{Unif}}(a,b)\\). Then \\[ \\mathop{\\mathrm{E}}(X) = \\int_a^b \\frac{x}{b-a} = \\frac{a+b}{2}. \\] Example 1.19 Let \\(X\\) be a cts. r.v. with pdf \\(f(x)=\\{ \\pi(1+x^2) \\}^{-1}\\) with support over \\(\\mathbb{R}\\). This is the Cauchy distribution with location and scale parameter 0 and 1 respectively. Using the substitution \\(u = x^2 + 1\\) and \\(\\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!u/2 = x \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!x\\), we find that \\[\\begin{align*} \\mathop{\\mathrm{E}}(X) &amp;= \\int_{-\\infty}^\\infty \\frac{x \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!x}{\\pi(1+x^2)} \\\\ &amp;= \\int_{-\\infty}^0 \\frac{x \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!x}{\\pi(1+x^2)} + \\int_{0}^\\infty \\frac{x \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!x}{\\pi(1+x^2)} \\\\ &amp;= \\frac{1}{2\\pi} \\int_{u=\\infty}^{u=1} \\frac{\\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!u}{u} + \\frac{1}{2\\pi} \\int_{u=1}^{u=\\infty} \\frac{\\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!u}{u} \\\\ &amp;= \\frac{1}{2\\pi} \\left[\\log u \\right]_{\\infty}^{1} + \\frac{1}{2\\pi} \\left[\\log u \\right]^{\\infty}_{1} \\\\ &amp;= \\frac{1}{2\\pi} (\\infty - \\infty) = \\ ??? \\end{align*}\\] so the mean is undefined. 1.9.2 Expectations of functions of r.v. Realise that if \\(X\\) is a r.v., then any function of \\(X\\), \\(g(X)\\), is also a random variable17. Often time we will want to know the mean of \\(g(X)\\). Theorem 1.2 Let \\(X\\) be a r.v. with pdf \\(f_X(x)\\), and let \\(Y=g(X)\\). Then \\[ \\mathop{\\mathrm{E}}(Y) = \\int g(x)f_X(x)\\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!x. \\] In particular, the \\(k\\)th moment of \\(X\\) for \\(k\\in\\mathbb{Z}\\) is defined to be \\[ \\mathop{\\mathrm{E}}(X^k) = \\int x^kf_X(x)\\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!x. \\] The \\(k\\)th central moment is defined as \\(\\mathop{\\mathrm{E}}((X-\\mu)^k)\\), where \\(\\mu:=\\mathop{\\mathrm{E}}(X)\\). 1.9.3 Properties of expectations Let \\(X\\) be a r.v., and \\(a,b,c\\in\\mathbb{R}\\) be constants. Here are some important properties of expectations [you should really know these!]. They also work for \\(g(X)\\) too. \\(\\mathop{\\mathrm{E}}(aX +bX +c) = a\\mathop{\\mathrm{E}}(X) + b\\mathop{\\mathrm{E}}(X) + c\\) (linearity of expectations) If \\(Y\\) is a r.v. s.t. \\(X\\perp Y\\), then \\(\\mathop{\\mathrm{E}}(XY) = \\mathop{\\mathrm{E}}(X)\\mathop{\\mathrm{E}}(Y)\\) If \\(X\\geq 0\\) for all \\(x\\), then \\(\\mathop{\\mathrm{E}}(X)\\geq 0\\) If \\(a \\leq X \\leq b\\) for all \\(x\\), then \\(a \\leq \\mathop{\\mathrm{E}}(X) \\leq b\\) \\(\\mathop{\\mathrm{E}}(X) = \\min_b \\mathop{\\mathrm{E}}((X-b)^2)\\) (see Example 2.2.6 C&amp;B) As a corollary, if \\(X_1,\\dots,X_n\\) are r.v. and \\(a_1,\\dots,a_n\\) are constants, then \\[ \\mathop{\\mathrm{E}}\\left(\\sum_{i=1}^n a_iX_i \\right) = \\sum_{i=1}^n a_i\\mathop{\\mathrm{E}}(X_i). \\] Additionally, if \\(X_1,\\dots,X_n\\) are independent, \\[ \\mathop{\\mathrm{E}}\\left(\\prod_{i=1}^n a_iX_i \\right) = \\prod_{i=1}^n \\mathop{\\mathrm{E}}(X_i). \\] 1.9.4 Variance Aside from the mean of a r.v., perhaps the most important moment is the second central moment, more commonly known as the variance. Definition 1.14 (Variance) Let \\(X\\) be a r.v. with mean \\(\\mu\\). The variance of \\(X\\) is defined \\[ \\mathop{\\mathrm{Var}}(X) = \\mathop{\\mathrm{E}}\\big[(X-\\mu)^2\\big], \\] assuming this expectation exists. The standard deviation is \\(\\text{sd}(X) = \\sqrt{\\mathop{\\mathrm{Var}}(X)}\\). The symbol \\(\\sigma^2\\) is often used to denote the variance, and \\(\\sigma\\) the standard deviation. An alternative formula is \\(\\sigma^2 = \\mathop{\\mathrm{E}}(X^2) - \\{\\mathop{\\mathrm{E}}(X)\\}^2\\). This variance is not to be confused with the sample variance of a set of observations \\(\\{x_1,\\dots,x_n\\}\\), i.e. \\(s^2 = \\frac{1}{n}\\sum_{i=1}^n (x_i-\\bar x_n)^2\\) (although, inspect the two formulae for similarities!). The variance measures the spread of a distribution. That is, how far apart or close together the “mass” of a distribution are. To illustrate this, have a look at the following \\(\\mathop{\\mathrm{N}}(0,\\sigma^2)\\) pdfs for different values of \\(\\sigma^2\\). 1.9.5 Covariance and correlation The covariance and correlation between \\(X\\) and \\(Y\\) measure how strong the linear relationship is between \\(X\\) and \\(Y\\). Definition 1.15 (Covariance and correlation) For two r.v. \\(X\\) and \\(Y\\) with finite means \\(\\mu_X\\) and \\(\\mu_Y\\) resp., and variances \\(\\sigma^2_X\\) and \\(\\sigma^2_Y\\) resp., the covariance between \\(X\\) and \\(Y\\) is \\[ \\mathop{\\mathrm{Cov}}(X,Y) = \\mathop{\\mathrm{E}}\\big[(X-\\mu_X)(Y-\\mu_Y) \\big]. \\] Their correlation is the number defined by \\[ \\rho_{XY} := \\frac{\\mathop{\\mathrm{Cov}}(X,Y)}{\\sigma_X\\sigma_Y} \\] An alternative formula is \\(\\mathop{\\mathrm{E}}(XY) -\\mathop{\\mathrm{E}}(X)\\mathop{\\mathrm{E}}(Y)\\). The covariance of \\(X\\) with itself is \\(\\sigma^2\\), while the correlation of \\(X\\) with itself is 1. Try and work this out yourself! The magnitude of the covariance by itself does not reflect how strong the relationship between \\(X\\) and \\(Y\\) is, so this is where the correlation comes in. \\(\\rho_{XY}\\) takes values between -1 and 1. \\(\\rho_{XY}=0\\) implies that there is no linear relationship at all between \\(X\\) and \\(Y\\). On the other hand, \\(\\rho_{XY}=1\\) (\\(\\rho_{XY}=-1\\)) implies a perfect positive (negative) linear relationship. In fact, \\(|\\rho_{XY}=1|\\) iff \\(\\exists a\\neq 0,b\\in\\mathbb{R}\\) s.t. \\(\\mathbb{P}(Y=aX+b)=1\\). If \\(a&gt;0\\) then \\(\\rho_{XY}=1\\), and if \\(a&lt;0\\) then \\(\\rho_{XY}=-1\\). If \\(X\\) and \\(Y\\) are independent, then \\(\\mathop{\\mathrm{Cov}}(X,Y) =\\rho_{XY}=0\\). Ttry and prove this! Remark. If \\(\\mathop{\\mathrm{Cov}}(X,Y)=\\rho_{XY}=0\\), then \\(X\\) and \\(Y\\) are independent. Let \\(X,Y\\,\\overset{\\text{iid}}{\\sim}\\,\\mathop{\\mathrm{N}}(0,1)\\). We can draw some random values in R, and produce a scatterplot to see the relationship between them. X &lt;- rnorm(n = 100, mean = 0, sd = 1) Y &lt;- rnorm(n = 100, mean = 0, sd = 1) qplot(X, Y, geom = &quot;point&quot;) Now suppose \\(Y=2X + Z\\), where \\(Z\\sim\\mathop{\\mathrm{N}}(0,1)\\). Now, \\(\\mathop{\\mathrm{Cov}}(X,Y)= 2\\), and \\(\\mathop{\\mathrm{Var}}(Y)=2\\). Theoretically, \\(\\rho_{XY}=2/\\sqrt{1\\cdot 2}\\approx 0.71\\). Z &lt;- rnorm(n = 100, mean = 0, sd = 1) Y &lt;- 2 * X + Z qplot(X, Y, geom = &quot;point&quot;) 1.9.6 Properties of variances and covariances Let \\(X\\) and \\(Y\\) be random variables, and \\(a\\neq0,b\\in\\mathbb{R}\\) be constants. \\(\\mathop{\\mathrm{Var}}(aX + b) = a^2\\mathop{\\mathrm{Var}}(X)\\) \\(\\mathop{\\mathrm{Var}}(X \\pm Y) = \\mathop{\\mathrm{Var}}(X) + \\mathop{\\mathrm{Var}}(Y) \\pm 2\\mathop{\\mathrm{Cov}}(X,Y)\\) If \\(X\\) and \\(Y\\) are independent, then \\(\\mathop{\\mathrm{Var}}(X \\pm Y) = \\mathop{\\mathrm{Var}}(X) + \\mathop{\\mathrm{Var}}(Y)\\) As a corollary, let \\(X_1,\\dots,X_n\\) be r.v. Then, \\[ \\mathop{\\mathrm{Var}}\\left(\\sum_{i=1}^nX_i \\right) = \\sum_{i=1}^n \\mathop{\\mathrm{Var}}(X_i) + \\sum_{i\\neq j}\\mathop{\\mathrm{Cov}}(X_i,X_j) \\] Let \\(X,Y,W,V\\) be r.v., and \\(a,b,c,d\\in\\mathbb{R}\\). Then \\(\\mathop{\\mathrm{Cov}}(X,Y) = \\mathop{\\mathrm{Cov}}(Y,X)\\) \\(\\mathop{\\mathrm{Cov}}(X,b) = 0\\) \\(\\mathop{\\mathrm{Cov}}(aX,Y) = a\\mathop{\\mathrm{Cov}}(X,Y)\\) \\(\\mathop{\\mathrm{Cov}}(aX+b,cY+d)=ac\\mathop{\\mathrm{Cov}}(X,Y)\\) \\(\\mathop{\\mathrm{Cov}}(X+Y,W+V)=\\mathop{\\mathrm{Cov}}(X,Y) + \\mathop{\\mathrm{Cov}}(X, V) + \\mathop{\\mathrm{Cov}}(Y,W) + \\mathop{\\mathrm{Cov}}(Y,V)\\) Example 1.20 Let \\(X\\sim\\mathop{\\mathrm{N}}(0,1)\\), and \\(Y=2X+1\\). Then \\[\\mathop{\\mathrm{Var}}(Y)=\\mathop{\\mathrm{Var}}(2X+1)=4\\mathop{\\mathrm{Var}}(X) = 4\\]. Further, \\[\\mathop{\\mathrm{Cov}}(X,Y)=\\mathop{\\mathrm{Cov}}(X,2X+1)=2\\mathop{\\mathrm{Cov}}(X,X)=2\\mathop{\\mathrm{Var}}(X)=2\\]. 1.9.7 Variance-covariance matrix Consider a random vector \\((X_1,\\dots,X_n)^\\top\\) whose mean is \\((\\mu_1,\\dots,\\mu_n)^\\top\\). The variance-covariance matrix, usually denoted \\({\\boldsymbol\\Sigma}\\in\\mathbb{R}^{n\\times n}\\), is defined to be \\[ {\\boldsymbol\\Sigma}= \\begin{pmatrix} \\mathop{\\mathrm{Var}}(X_1) &amp;\\mathop{\\mathrm{Cov}}(X_1,X_2) &amp;\\cdots &amp;\\mathop{\\mathrm{Cov}}(X_1,X_n) \\\\ \\mathop{\\mathrm{Cov}}(X_2,X_1) &amp;\\mathop{\\mathrm{Var}}(X_2) &amp;\\cdots &amp;\\mathop{\\mathrm{Cov}}(X_2,X_n) \\\\ \\vdots &amp;\\vdots &amp;\\ddots&amp;\\vdots \\\\ \\mathop{\\mathrm{Cov}}(X_n,X_1) &amp;\\mathop{\\mathrm{Cov}}(X_n,X_2) &amp;\\cdots &amp;\\mathop{\\mathrm{Var}}(X_n) \\\\ \\end{pmatrix} \\] The correlation matrix is similar in structure to the above, except the off-diagonals are filled with \\(\\rho_{X_iX_j}\\) and the diagonals are all 1. Can you figure out why this is? 1.9.8 Conditional expectations Conditional pmfs/pdfs are also useful for calculating conditional expectations, i.e. the average value of a random variable \\(X\\) given some information about another r.v. \\(Y\\) which might affect it. Definition 1.16 (Conditional expectation) The conditional expectation of a function of a r.v. \\(X\\), \\(g(X)\\) say, given a value of a nother r.v. \\(Y=y\\), is \\[ \\mathop{\\mathrm{E}}\\left[g(X)|Y=y\\right] = \\begin{cases} \\sum_x g(x)\\overbrace{\\mathbb{P}(X=x|Y=y)}^{f_{X|Y}(x|y)} &amp;\\text{if $X$ is discrete}\\\\ \\int g(x)f_{X|Y}(x|y)\\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!x &amp;\\text{if $X$ is continuous}\\\\ \\end{cases} \\] All of the properties of the usual expectations are applicable. However, whereas \\(\\mathop{\\mathrm{E}}(X)\\) is a number (non-random), \\(\\mathop{\\mathrm{E}}(X|Y=y)\\) is a function of \\(y\\). If we have not observed \\(Y\\), then \\(\\mathop{\\mathrm{E}}(X|Y)\\) is a random variable. Example 1.21 Suppose we draw \\(Y\\sim\\mathop{\\mathrm{Unif}}(0,1)\\). After we observe \\(Y=y\\in[0,1]\\), we draw \\(X|(Y=y) \\sim \\mathop{\\mathrm{Unif}}(y,1)\\). Intuitively, we expect that \\(\\mathop{\\mathrm{E}}(X|Y=y)\\) to be half-way between \\(y\\) and 1, i.e. \\((1+y)/2\\). In fact, \\(f_{X|Y}(x|y) = (1-y)^{-1}\\), so \\[\\begin{align*} \\mathop{\\mathrm{E}}(X|Y=y) &amp;= \\int_y^1 xf_{X|Y}(x|y) \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!x \\\\ &amp;= \\frac{1}{1-y} \\int_y^1 x \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!x =\\frac{1-y^2}{2(1-y)} = \\frac{(1-y)(1+y)}{2(1-y)} =\\frac{1+y}{2}. \\end{align*}\\] However, if \\(Y\\) has not been observed yet, then \\(\\mathop{\\mathrm{E}}(X|Y)=(1+Y)/2\\) is a r.v. whose value is \\(\\mathop{\\mathrm{E}}(X|Y=y)=(1+y)/2\\) once observed. If \\(\\mathop{\\mathrm{E}}(X|Y)\\) is a r.v., what is its mean? Theorem 1.3 (Rule of iterated expectations/Law of total expectations) If \\(X\\) and \\(Y\\) are two r.v., then \\[ {\\mathop{\\mathrm{E}}}_Y\\left[\\mathop{\\mathrm{E}}(X|Y)\\right] = \\mathop{\\mathrm{E}}(X), \\] provided the expectation exists. More generally, \\(\\mathop{\\mathrm{E}}(g(X)) = \\mathop{\\mathrm{E}}\\left[\\mathop{\\mathrm{E}}(g(X)|Y)\\right]\\) for any function \\(g\\). The total average \\(\\mathop{\\mathrm{E}}(X)\\) is the average \\(\\mathop{\\mathrm{E}}_Y(\\cdot)\\) of the case-by-case averages \\(\\mathop{\\mathrm{E}}(X|Y)\\) over \\(Y\\). Proof. \\[\\begin{align*} {\\mathop{\\mathrm{E}}}_Y\\left[\\mathop{\\mathrm{E}}(X|Y)\\right] &amp;= \\int \\left( \\int x f_{X|Y}(x|y)\\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!x \\right) f_Y(y)\\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!y \\\\ &amp;= \\int \\int x \\cdot \\overbrace{f_{X|Y}(x|y) f_Y(y)}^{f_{X,Y}(x,y)} \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!y \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!x \\\\ &amp;= \\int x \\cdot \\overbrace{\\int f_{X,Y}(x,y) \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!y}^{f_X(x)} \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!x \\\\ &amp;= \\mathop{\\mathrm{E}}(X) \\end{align*}\\] 1.9.9 Conditional variance Definition 1.17 (Conditional variance) The conditional variance of a r.v. \\(X\\) given \\(Y=y\\) is \\[ \\mathop{\\mathrm{Var}}(X|Y=y) = \\mathop{\\mathrm{E}}\\left[ \\left(X - \\mathop{\\mathrm{E}}(X|Y=y)\\right)^2 \\,\\Big|\\, Y=y\\right]. \\] An alternative formula: \\[ \\mathop{\\mathrm{Var}}(X|Y=y) = \\mathop{\\mathrm{E}}\\left(X^2 | Y=y\\right) - \\left\\{ \\mathop{\\mathrm{E}}(X|Y=y) \\right\\}^2. \\] The law of total variance states that \\[ \\mathop{\\mathrm{Var}}(X) = {\\mathop{\\mathrm{E}}}_Y\\left[\\mathop{\\mathrm{Var}}(X|Y) \\right] + {\\mathop{\\mathrm{Var}}}_Y\\left[\\mathop{\\mathrm{E}}(X|Y) \\right]. \\] Note that, in this context, both \\(\\mathop{\\mathrm{Var}}(X|Y)\\) and \\(\\mathop{\\mathrm{E}}(X|Y)\\) are random variables. The variance of \\(X\\) is the sum of two parts: The average of the variance of \\(X\\) over all possible values of the r.v. \\(Y\\). This is called the average within-sample variance. The variance of the conditional expectation of \\(X\\) given \\(Y\\). This is called the between-sample variance (of the conditional averages). See also: https://math.stackexchange.com/a/3377007 We can even describe the distribution for any transformation of \\(X\\), see C&amp;B Sec 2.1.↩︎ "],["moment-generating-functions.html", "1.10 Moment generating functions", " 1.10 Moment generating functions 1.10.1 Moment generating functions As the name implies, the moment generating function (mgf) is used for finding moments of a r.v.. Other uses: Characterising a distribution Finding distributions of sums of r.v. As a tool in statistical proofs 1.10.2 Generating moments Consider the following: \\[\\begin{align*} \\frac{\\text{d}}{\\text{d}t}M_X(t) \\bigg|_{t=0} &amp;= \\frac{\\text{d}}{\\text{d}t} \\mathop{\\mathrm{E}}(e^{tX}) \\bigg|_{t=0} \\\\ &amp;= \\mathop{\\mathrm{E}}\\left[ \\frac{\\text{d}}{\\text{d}t}e^{tX} \\right]\\bigg|_{t=0} \\\\ &amp;= \\mathop{\\mathrm{E}}[Xe^{tX}] \\Big|_{t=0} \\\\ &amp;= \\mathop{\\mathrm{E}}(X). \\end{align*}\\] We can iterate the steps again to generate the \\(k\\)-th moment of \\(X\\) by taking \\(k\\) derivatives and setting \\(t=0\\). Note that this relies on being able to interchange the order of differentiation and integration. See §2.4 C&amp;B. For “nice distributions” generally there are no problems. Theorem 1.4 If \\(X\\) has mgf \\(M_X(t)\\), then \\[ \\mathop{\\mathrm{E}}(X^k) = M_X^{(k)}(0) = \\frac{\\text{d}^k}{\\text{d}t^k}M_X(t) \\bigg|_{t=0}. \\] That is, the \\(k\\)-th moment is equal to the \\(k\\)-th derivative of \\(M_X(t)\\) evaluated at \\(t=0\\). Example 1.22 Let \\(X\\sim\\mathop{\\mathrm{Exp}}(1/r)\\) with \\(f_X(x)=r e^{-r x}\\) for \\(x\\in[0,\\infty)\\). Then for \\(t&lt;r\\), \\[ M_X(t) = \\int_0^\\infty e^{tx}\\cdot r e^{- r x} \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!x = r \\int_0^\\infty e^{(t- r)x} \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!x = \\frac{ r}{ r- t}. \\] \\(\\mathop{\\mathrm{E}}(X)=M_X&#39;(t)\\big|_{t=0}=\\frac{r}{(r-t)^2}\\Big|_{t=0}=1/r\\). 1.10.3 Properties of mgf If \\(Y=aX+b\\), then \\(M_Y(t)=e^{bt}M_X(at)\\). If \\(X_1,\\dots,X_n\\) are independent and \\(Y=\\sum_{i=1}^n X_i\\), then \\(M_Y(t)=\\prod_{i=1}^n M_{X_i}(t)\\). If \\(X\\) and \\(Y\\) are r.v. s.t. \\(M_X(t)=M_Y(t)\\) for all \\(t\\) in an open interval around 0, then \\(F_X(x)=F_Y(x)\\) for all \\(x\\). The mgf has the property that it uniquely defined a distribution. That is, if two distributions have identical mgfs then they have the same distribution. "],["exercises.html", "1.11 Exercises", " 1.11 Exercises Using only the three axioms of probability, prove the following statements: \\(\\Pr(\\{\\})=0\\) If \\(A \\subseteq B\\) then \\(\\Pr(A) \\leq \\Pr(B)\\) Hint: Write \\(B = A \\cup (B \\cap A^c)\\) \\(0 \\leq \\Pr(A) \\leq 1\\) \\(\\Pr(A^c)=1-\\Pr(A)\\) If \\(A \\cap B = \\{\\}\\), then \\(\\Pr(A\\cup B) = \\Pr(A) + \\Pr(B)\\) This is called the ``Monty Hall Problem’’. A prize is placed at random behind one of three doors. You pick a door. To be concrete, let’s suppose you always pick door 1. Now Monty Hall chooses one of the other two doors, opens it and shows you that it is empty. He then gives you the opportunity to keep your door or switch to the other unopened door. Should you stay or switch? Intuition suggests it doesn’t matter. The correct answer is that you should switch. Prove it. It will help to specify the sample space and the relevant events carefully. Thus write \\(\\Omega = \\big\\{(\\omega_1,\\omega_2) \\,|\\, \\omega_i \\in \\{1,2,3\\}\\big\\}\\) where \\(\\omega_1\\) is where the prize is and \\(\\omega_2\\) is the door Monty opens. There are three cards. The first is green on both sides, the second is red on both sides and the third is green on one side and red on the other. We choose a card at random and we see one side (also chosen at random). If the side we see is green, what is the probability that the other side is also green? Many people intuitively answer 1/2. Show that the correct answer is 2/3. For independent events \\(A_1,\\dots,A_n\\), show that \\[\\Pr(A_1 \\cup \\cdots \\cup A_n) = 1 - \\prod_{i=1}^n \\big(1 - \\Pr(A_i)\\big).\\] A pair of dice is rolled \\(n\\) times. How large must \\(n\\) be so that the probability of rolling at least one double six is more than 1/2? The probability that a child has blue eyes is 1/4. Assume independence between children. Consider a family with 3 children. If it is known that at least one child has blue eyes, what is the probability that at least two children have blue eyes? If it is known that the youngest child has blue eyes, what is the probability that at least two children have blue eyes? Prove the following statements. If \\(A \\perp B\\), then \\(A^c \\perp B^c\\). \\(\\Pr(A \\cap B \\cap C) = \\Pr(A| B \\cap C)\\Pr(B|C)\\Pr(C)\\). Let \\(X\\) be distributed according to \\[ f_X(x) = \\begin{cases} 1/4&amp;0&lt;x&lt;1\\\\ 3/8&amp;3&lt;x&lt;5 \\\\ 0&amp;\\text{otherwise} \\end{cases} \\] Show that \\(f_X\\) is indeed a probability density function. Find the cumulative distribution function of \\(X\\). Suppose we toss a coin once and let \\(p\\) be the probability of heads. Let \\(X\\) denote the number of heads and let \\(Y\\) denote the number of tails. Prove that \\(X\\) and \\(Y\\) are independent. Let \\[ f_{X,Y}(x,y) = \\begin{cases} c(x+y^2) &amp;0\\leq x \\leq 1, 0\\leq y \\leq 1 \\\\ 0&amp;\\text{otherwise} \\end{cases} \\] Find the value of \\(c\\). Find \\(\\Pr(X&lt;1/2 | Y=1/2)\\). Consider a sequence of independent coin flips, each of which has probability \\(p\\) of being heads. Define a random variable \\(X\\) as the length of the run (of either heads or tails) started by the first trial. For example, \\(X=3\\) if either \\(TTTH\\) or \\(HHHT\\) is observed. Find the distribution of \\(X\\) and find \\(\\mathop{\\mathrm{E}}(X)\\). For a random variable \\(X\\) with mean \\(\\mu\\) and variance \\(\\mathop{\\mathrm{Var}}(X)\\) and any given constant \\(c\\in\\mathbb{R}\\), prove that \\(\\mathop{\\mathrm{Var}}(X) = \\mathop{\\mathrm{E}}(X^2) - \\mu^2\\). \\(\\mathop{\\mathrm{Var}}(X) = \\mathop{\\mathrm{E}}\\big(X(X-1)\\big) +\\mu -\\mu^2\\). \\(\\mathop{\\mathrm{E}}\\big((X-c)^2\\big) = \\mathop{\\mathrm{Var}}(X) + (\\mu-c)^2\\) so that the minimum mean squared deviation occurs when \\(c=\\mu\\). Suppose we play a game where we start with \\(c\\) dollars. On each play of the game you either double or halve your money, with equal probability. What is your expected fortune after \\(n\\) trials? Let \\(X_1,\\dots,X_n\\,\\overset{\\text{iid}}{\\sim}\\,\\mathop{\\mathrm{Unif}}(0,1)\\) and let \\(Y_n=\\max\\{X_1,\\dots,X_n\\}\\). Find \\(\\mathop{\\mathrm{E}}(Y_n)\\). Hint: Find out the distribution of \\(Y_n\\) by looking at the cdf of \\(Y_n\\). Let \\(X\\mathop{\\mathrm{Unif}}(0,1)\\). Let \\(0&lt;a&lt;b&lt;1\\). Let \\[ Y = \\begin{cases} 1 &amp; 0&lt;x&lt;b \\\\ 0 &amp;\\text{otherwise} \\end{cases} \\] and let \\[ Z = \\begin{cases} 1 &amp; a&lt;x&lt;1 \\\\ 0 &amp;\\text{otherwise} \\end{cases} \\] Are \\(Y\\) and \\(Z\\) independent? Why/why not? Find \\(\\mathop{\\mathrm{E}}(Y|Z)\\). Hint: What values \\(z\\) can \\(Z\\) take? Find first \\(\\mathop{\\mathrm{E}}(Y|Z=z)\\). Hand-in questions A certain river floors every year. Suppose that the low-water mark is set at 1 and the high-water mark \\(Y\\) has distribution function \\[ F_Y(y) = \\Pr(Y\\leq y) = 1 - \\frac{1}{y^2}, \\hspace{2em} 1\\leq y&lt;\\infty \\] Verify that \\(F_Y(y)\\) is a cdf. [1 mark] Find \\(f_Y(y)\\), the pdf of \\(Y\\). [2 marks] If the low-water mark is reset at 0 and we use a unit of measurement that is 1/10 of that given previously, the height water mark becomes \\(Z=10(Y-1)\\). What is the expected value of \\(Z\\)? [2 marks] A pdf is defined by \\[ f_{X,Y}(x,y) = \\begin{cases} c(x+2y) &amp;0\\leq x \\leq 2, 0\\leq y \\leq 1 \\\\ 0&amp;\\text{otherwise} \\end{cases} \\] Find the value of \\(c\\). [1 mark] Find the marginal distribution of \\(X\\). [2 marks] Find the joint cdf of \\(X\\) and \\(Y\\). [2 marks] Suppose we generate a random variable \\(X\\) in the following way. First we flip a fair coin. If the coin is heads, take \\(X\\) to have a \\(\\mathop{\\mathrm{Unif}}(0,1)\\) distribution. If the coin is tails, take \\(X\\) to have a \\(\\mathop{\\mathrm{Unif}}(3,4)\\) distribution. Find the mean of \\(X\\). [12 marks] Find the standard deviation of \\(X\\). [3 marks] "],["commonly-used-probability-models.html", "Chapter 2 Commonly-used probability models", " Chapter 2 Commonly-used probability models Learning objectives By the end of this chapter, you will be able to: do this do that and this Readings Casella and Berger (2002) Chapter 3, sections 3.1 3.2 3.3 Wasserman (2004) Chapter 2, sections 2.3 and 2.4. Chapter 3, section 3.6. Topics not covered: Cauchy, lognormal and double exponential (Laplace) distributions, exponential families, location and scale families "],["introduction.html", "2.1 Introduction", " 2.1 Introduction Distributions in statistics serve two main purposes: To describe the assumed behaviour of the observations made in an experiment, survey or other study; To calibrate the values of derived statistics used in constructing confidence regions, hypothesis tests, etc. Some distributions are much used for both purposes (the normal distribution being the prime example). In this Part we will focus on some distributions used for the first purpose. Distributions used mainly for the second purpose (these include the \\(\\chi^2\\), \\(t\\) and \\(F\\) distributions) will be described later, in Part 3. We deal with a family of distributions. This family is indexed by one or more parameters (c.f. parametric family), which allow us to vary certain characteristics of the distribution while staying with one functional form. For example, consider r.v.s \\(X_k\\sim\\mathop{\\mathrm{N}}(k,1)\\). These are distinct distributions yet have similar characteristics. "],["discrete-models.html", "2.2 Discrete models", " 2.2 Discrete models 2.2.1 Point mass distribution \\(X\\) has a point mass distribution at \\(a\\), written \\(X\\sim \\delta_a\\), if \\(\\Pr(X=a) = 1\\), in which case \\[ F(x) = \\begin{cases} 0 &amp;x&lt;a \\\\ 1 &amp;x\\geq 1. \\end{cases} \\] The probability mass function is \\(f(x)=1\\) for \\(x=a\\), and 0 otherwise. 2.2.2 Uniform distribution Let \\(k&gt;1\\) be a given integer. The discrete uniform distribution on \\(\\{1,\\dots,k\\}\\) has pmf \\[ f(x) = \\frac{1}{k}, \\hspace{2em} x=1,\\dots,k. \\] We write \\(X\\sim\\mathop{\\mathrm{Unif}}\\{1,\\dots,k\\}\\). \\(\\mathop{\\mathrm{E}}(X)=\\frac{k+1}{2}\\). \\(\\mathop{\\mathrm{Var}}(X)=\\frac{k^2-1}{12}\\). If \\(k=1\\), then it is the point mass distribution. The discrete uniform (and the point mass) is appeallingly simple but has relatively few “real” statistical applications. 2.2.3 Bernoulli distribution Suppose we are interested in the outcome of a (single) random trial, which can either be “success” or “failure” only. Examples include A coin flip can land either Heads or Tails. The colour of the suit of a randomly drawn card from a pack of playing cards can be either Red or Black A dice roll outcome can either be an Even or an Odd number. Babies being born being Girl or Boy. Typically we assign the value ‘1’ to denote success, and ‘0’ to denote failure. This has no qualitative meaning whatsoever, the important thing is that there are only two distinct possible outcomes. Let \\(X\\) be the r.v. denoting the outcome of success (\\(X=1\\)) or failure (\\(X=0\\)) of a binary trial. Further let the pmf for \\(X\\) be \\[ f(x|p) = \\begin{cases} p &amp; x=1\\text{ (success)}\\\\ 1-p &amp;x=0 \\text{ (failure)} \\end{cases} \\] We say that \\(X\\) has a Bernoulli distribution written \\(X\\sim\\mathop{\\mathrm{Bern}}(p)\\). The pmf can also be written \\(f(x)=p^x(1-p)^{1-x}\\). The expectation is \\[ \\mathop{\\mathrm{E}}(X) = \\sum_x xf(x) = 1\\cdot p + 0 \\cdot (1-p) = p. \\] The variance is \\[ \\mathop{\\mathrm{Var}}(X) = \\sum_x (x-\\mu)^2f(x) = (1-p)^2\\cdot p + (0-p)^2 \\cdot (1-p) = p(1-p). \\] Consider the pmf for the Bernoulli distribution above. What do you get when you plug in \\(x=1\\) and \\(x=0\\)? 2.2.4 Binomial distribution The binomial describes the distribution of the number of “successes” in \\(n\\) independent and identical binary “trials”. That is, suppose we have a situation such that A finite number \\(n\\) trials are carried out. Each trial is independent of each other. The outcome of each trial is either success or failure (binary trials). The probability \\(0 \\leq p\\leq 1\\) of a successful outcome is the same for each trial. Let \\(X\\) be the number of success outcomes in \\(n\\) trials. Then \\(X\\) has a binomial distribution, written \\(X\\sim\\mathop{\\mathrm{Bin}}(n,p)\\). The pmf of \\(X\\) is \\[ f(x|n,p) = {n \\choose x}p^x (1-p)^{n-x}. \\] \\(X\\) has support (possible values it can take) over \\(\\{0,1,2,\\dots,n\\}\\). The mean and variance are \\(\\mathop{\\mathrm{E}}(X)=np\\) and \\(\\mathop{\\mathrm{Var}}(X)=np(1-p)\\). Proof. Here’s the proof for the mean. \\[\\begin{align*} \\mathop{\\mathrm{E}}(X) = \\sum_{x=0}^n x \\frac{n!}{x!(n-x)!} p^x (1-p)^{n-x} &amp;= \\sum_{x=1}^n x \\frac{n!}{x!(n-x)!} p^x (1-p)^{n-x} \\\\ &amp;=\\sum_{x=1}^n n \\cdot \\overbrace{\\frac{(n-1)!}{(x-1)!(n-x)!}}^{{n-1 \\choose x-1}} \\cdot p^{x-1+1} (1-p)^{(n-1)-(x-1)} \\\\ &amp;= np \\overbrace{\\sum_{x-1=0}^n {n-1 \\choose x-1} p^{x-1} (1-p)^{n-x}}^{=1} \\\\ &amp;= np. \\end{align*}\\] Obtaining the variance follows similar steps. Try to replicate the proof above and obtain \\(\\mathop{\\mathrm{E}}(X^2)\\) for the binomial distribution. After that, you may obtain \\(\\mathop{\\mathrm{Var}}(X)\\) using the usual formula. Other properties and results If \\(n=1\\) then \\(X\\) is a Bernoulli r.v. \\(\\Pr(X=0)=(1-p)^n\\); \\(\\Pr(X=1)=p^n\\) Let \\(X_1,\\dots,X_n\\,\\overset{\\text{iid}}{\\sim}\\,\\mathop{\\mathrm{Bern}}(p)\\), then \\[ X = \\sum_{i=1}^n X_i \\sim \\mathop{\\mathrm{Bin}}(n,p) \\] One way to prove the above statement is by using mgfs. From this we can more easily derive \\[ \\mathop{\\mathrm{E}}(X) = \\mathop{\\mathrm{E}}\\left(\\sum_{i=1}^n X_i \\right) = \\sum_{i=1}^n \\mathop{\\mathrm{E}}(X_i) = np \\] and \\[ \\mathop{\\mathrm{Var}}(X) = \\mathop{\\mathrm{Var}}\\left(\\sum_{i=1}^n X_i \\right) = \\sum_{i=1}^n \\mathop{\\mathrm{Var}}(X_i) = np(1-p) \\] 2.2.5 Geometric distribution The geometric distribution is a type of ‘waiting time’ distribution. We count the number of Bernoulli trials to get the first success. Let \\(X\\) be distributed geometrically, \\(X\\sim\\mathop{\\mathrm{Geom}}(p)\\), where \\(p\\) is the probability of success. Clearly, \\[ f(x|p)=(1-p)^{x-1}p. \\] The support of \\(X\\) is \\(\\{1,2,3,\\dots\\}\\); it is countably infinite. This is a valid pmf since \\[ \\sum_{x=1}^\\infty f(x|p) = \\sum_{x=1}^\\infty (1-p)^{x-1}p = \\frac{p}{1-(1-p)} = 1. \\] \\(\\mathop{\\mathrm{E}}(X)=\\frac{1}{p}\\). The smaller the \\(p\\), the longer we have to wait for a success. \\(\\mathop{\\mathrm{Var}}(X)=\\frac{1-p}{p^2}\\). There is another formulation for the geometric distribution: Let \\(Y\\) be the number of failures before the first success occurs. Then \\[ f(y|p) = (1-p)^yp. \\] \\(Y\\) has support \\(\\{0,1,2,\\dots\\}\\). \\(X\\) and \\(Y\\) are related through \\(Y=X-1\\). Thus it is easy to check that \\[ \\mathop{\\mathrm{E}}(Y) = \\frac{1-p}{p} \\text{ and } \\mathop{\\mathrm{Var}}(Y)=\\frac{1-p}{p^2}. \\] We shall mainly use the first version of the geometric distribution in this course, but be aware of the alternative version as well. 2.2.6 Negative binomial Suppose we count the number of Bernoulli trials required to get a fixed number of successes, \\(r\\), each with probability of success \\(p\\). This leads to the negative binomial distribution. Denote this by \\(X\\sim\\mathop{\\mathrm{NBin}}(r,p)\\). The pmf is \\[ f(x|r,p)= {x-1 \\choose r-1} p^r (1-p)^{x-r}. \\] The pmf is easy to justify: In order to get \\(X=x\\), a total of \\(r-1\\) successes must have occurred in the previous \\(x-1\\) number of trials. Then, the pmf follows directly from the binomial pmf. Clearly, the support of \\(X\\) is \\(\\{r, r+1, r+2, \\dots \\}\\). \\(\\mathop{\\mathrm{E}}(X)=\\frac{r}{p}\\). \\(\\mathop{\\mathrm{Var}}(X)=\\frac{r(1-p)}{p^2}\\). If \\(r=1\\), then \\(X\\) is the geometric distribution. The name ‘negative binomial’ comes from noting that \\(Y=X-r\\), the number of failures seen before the \\(r\\)th success, has pmf \\[ f(y|r,p) = (-1)^y{-r \\choose y} p^r(1-p)^{r-y}, \\] which looks suspiciously close to the binomial pmf18. 2.2.7 Poisson distribution The Poisson is the most standard assumption for the distribution of a count of events that occur (separately and independently, by assumption) in time or space. Some examples: Amount of e-mails received in 24-hour period. Number of calls received by a call centre per hour. The number of photons hitting a detector in a particular time interval. The number of patients arriving in an emergency room between 10pm and 11pm. Let \\(X\\) be the number of occurrences in this interval, such that the mean number of occurrences \\(\\lambda\\) in the given interval (sometimes called the rate or intensity) is known and is finite. Then \\(X\\sim\\mathop{\\mathrm{Poi}}(\\lambda)\\), and \\[ f(x|\\lambda) = \\frac{e^{-\\lambda}\\lambda^x}{x!}, \\] for \\(x=0,1,2,\\dots\\) To work out the mean, we make use of the Taylor series expansion. Recall that \\(e^x=\\sum_{k=0}^\\infty \\frac{x^k}{k!}\\). Using this fact we can derive the moments. \\[\\begin{align*} M_X(t) &amp;= \\sum_{x=0}^\\infty \\frac{e^{tx} e^{-\\lambda}\\lambda^x}{x!} \\\\ &amp;= e^{-\\lambda} \\sum_{x=0}^\\infty \\frac{(\\lambda e^t)^x}{x!} \\\\ &amp;= e^{-\\lambda}e^{\\lambda e^t} = \\exp\\{\\lambda(e^t - 1) \\}. \\end{align*}\\] Hence \\(\\mathop{\\mathrm{E}}(X)=M_X&#39;(0)=\\lambda\\) and \\(\\mathop{\\mathrm{E}}(X^2)=M_X&#39;&#39;(0)=\\lambda^2+\\lambda\\), so \\(\\mathop{\\mathrm{Var}}(X)=\\mathop{\\mathrm{E}}(X^2)-\\mathop{\\mathrm{E}}(X)=\\lambda\\). The Poisson family is closed under addition. If \\(X\\) and \\(Y\\) are independent Poisson r.v. with means \\(\\lambda\\) and \\(\\mu\\), then \\[ X+Y \\sim \\mathop{\\mathrm{Poi}}(\\lambda + \\mu) \\] The proof uses mgf and the characterizing property of the mgf. Have a go at the proof using properties of the mgf. Details in C&amp;B, p.95↩︎ "],["continuous-models.html", "2.3 Continuous models", " 2.3 Continuous models 2.3.1 Continuous uniform distribution The continuous uniform distribution is usually taken to have support on an interval, say \\(a \\leq x \\leq b\\). Let \\(X\\sim\\mathop{\\mathrm{Unif}}(a,b)\\). The pdf is \\[ f_X(x) = \\frac{1}{b-a} \\] for \\(x\\in[a,b]\\) and 0 otherwise. \\(\\mathop{\\mathrm{E}}(X)=\\frac{a+b}{2}\\). \\(\\mathop{\\mathrm{Var}}(X)=\\frac{(a-b)^2}{12}\\). The plot of the pdf gives a “rectangular” shape, so probabilities can also be found geometrically, as we previously saw in Chapter 1. 2.3.2 Exponential distribution The exponential distribution is often used to describe the distribution of measured time intervals ‘duration data’ or ‘waiting-time data’. E.g. the amount of time until an earthquake occurs. the time between two lightbulbs failing. the length (in minutes) of faculty staff meetings at UBD. the average waiting time at a hospital’s A&amp;E. Let \\(X\\sim\\mathop{\\mathrm{Exp}}(\\lambda)\\). The pdf is \\[ f_X(x) = \\frac{1}{\\lambda} e^{-x/\\lambda}. \\] \\(X\\) has support over \\([0,\\infty]\\). \\(\\lambda &gt;0\\) is known as the “scale” parameter. The value \\(1/\\lambda\\) is known as the “rate”. \\(\\mathop{\\mathrm{E}}(X)=\\lambda\\). \\(\\mathop{\\mathrm{Var}}(X)=\\lambda^2\\). \\(aX\\sim\\mathop{\\mathrm{Exp}}(a\\lambda)\\) for \\(a&gt;0\\). The pdf experiences “exponential decay”–long wait times between two events occurring becomes more and more unlikely. The exponential distribution has a very special property: it is memoryless, in the sense that for all \\(t&gt;s&gt;0\\), \\[ \\Pr(X &gt; t+s|X&gt;s) = \\Pr(X &gt; t) \\] Given that we have been waiting for an event to occur for \\(s\\) units of time, the probability that we wait a further \\(t\\) units of time is independent to the first fact! For example, assume that bus waiting times are exponentially distributed. A memoryless wait for a bus would mean that the probability that a bus arrived in the next minute is the same whether you just got to the station or if you’ve been sitting there for twenty minutes already. We can show that \\(X\\) is a positive r.v. and memoryless if and only if it is exponentially distributed. You will prove the memoryless fact in one of the exercises for this chapter. 2.3.3 Gamma distribution The gamma distribution generalises the exponential. It is also used for modelling durations (lengths of time intervals). Let \\(X\\sim\\Gamma(\\alpha,\\beta)\\). The pdf is \\[ f_X(x) = \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} x^{\\alpha-1} e^{- x/\\beta} \\] \\(X\\) has support over \\([0,\\infty]\\). \\(\\alpha&gt;0\\) is the “shape” parameter, and \\(\\beta &gt;0\\) is the “scale” parameter. \\(\\mathop{\\mathrm{E}}(X)=\\alpha\\beta\\). \\(\\mathop{\\mathrm{Var}}(X)=\\alpha\\beta^2\\). \\(\\Gamma(\\alpha)=\\int_0^\\infty y^{\\alpha-1}e^{-y}\\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!y\\) is called the Gamma function. \\(\\Gamma(1,\\lambda) \\equiv \\mathop{\\mathrm{Exp}}(\\lambda)\\). \\(aX\\sim\\Gamma(\\alpha,a\\beta)\\) for \\(a&gt;0\\). If \\(X_i\\sim\\Gamma(\\alpha_i,\\beta)\\), then \\(\\sum_{i=1}^n X_i \\sim \\Gamma(\\sum_{i=1}^n \\alpha_i,\\beta)\\). Be aware that there is an alternative parameterisation of the exponential and gamma distribution using “scale” parameters: \\(Y \\sim \\mathop{\\mathrm{Exp}}(\\lambda)\\), where \\(f_Y(y)=\\frac{1}{\\lambda}e^{-y/\\lambda}\\). Here \\(\\lambda\\) is the … \\(Y \\sim \\Gamma(\\alpha,s)\\), where \\(f_Y(y)=\\frac{1}{\\Gamma(\\alpha)s^\\alpha} y^{\\alpha-1} e^{-y/s}\\). Here \\(s\\) is the scale parameter. The shape parameter is obtained via \\(\\beta=1/s\\). Looks similar to the exponential pdf, but more generic. Effect of changing the shape parameter: 2.3.4 Beta distribution The beta distributions are distributions on the unit interval \\([0,1]\\), or on any other interval \\([a,b]\\) by transformation \\(X \\mapsto aX + b\\). It is used to model the behaviour of random variables limited to intervals of finite length in a wide variety of disciplines. Let \\(X\\sim \\mathop{\\mathrm{Beta}}(\\alpha,\\beta)\\). The pdf is \\[ f_X(x) = \\frac{1}{B(\\alpha,\\beta)} x^{\\alpha-1} (1-x)^{\\beta-1}. \\] \\(X\\) has support over \\([0,1]\\). \\(\\alpha&gt;0\\) and \\(\\beta&gt;0\\) are known as the “shape” parameters. \\(\\mathop{\\mathrm{E}}(X)=\\frac{\\alpha}{\\alpha+\\beta}\\). \\(\\mathop{\\mathrm{Var}}(X)=\\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}\\). \\(B(\\alpha,\\beta)\\) is the beta function \\(B(\\alpha,\\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}\\). \\(\\mathop{\\mathrm{Beta}}(1,1)\\equiv \\mathop{\\mathrm{Unif}}(0,1)\\). Pdf of beta distribution "],["normal-distribution.html", "2.4 Normal distribution", " 2.4 Normal distribution The normal distribution19 is the most important distribution in statistics. Many naturally occurring phenomena can be modelled as following a normal distribution. The central limit theorem (CLT): The distribution of the mean of a sample tends to converge to a normal distribution, as more and more samples are collected. Often, the normal distribution is used for the error term in standard statistical models (e.g. linear regression). Let \\(X\\) be distributed according to a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\). We write \\(X\\sim\\mathop{\\mathrm{N}}(\\mu,\\sigma^2)\\). The pdf of \\(X\\) is \\[ f_X(x|\\mu,\\sigma^2)= \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left[-\\frac{(x-\\mu)^2}{2\\sigma^2} \\right] \\] \\(X\\) has support over \\(\\mathbb{R}\\). \\(\\mathop{\\mathrm{E}}(X)=\\mu\\). \\(\\mathop{\\mathrm{Var}}(X)=\\sigma^2\\). The normal distribution is symmetric about \\(\\mu\\). The mode and median of \\(X\\) is also \\(\\mu\\). 2.4.1 Location parameter The \\(\\mu\\) parameter is also called the “location” parameter, since it determines where the bell curve is placed. 2.4.2 Scale parameter The \\(\\sigma^2\\) parameter is also called the “scale” parameter, since it determines how spread out the curve is. 2.4.3 Linear transformations of normal random variables For any constants \\(c, d \\in \\mathbb{R}\\), the r.v. \\(Y=cX + d\\) also has a normal distribution. \\(\\mathop{\\mathrm{E}}(Y)=\\mathop{\\mathrm{E}}(cX+d)=c\\mu + d\\). \\(\\mathop{\\mathrm{Var}}(Y) = \\mathop{\\mathrm{Var}}(cX+d) = c^2 \\sigma^2\\). The facts above are proven using mgf. See the exercises at the end of this chapter. In particular, a very important transformation is the standardisation \\[ Z = \\frac{X-\\mu}{\\sigma} \\] resulting in the standard normal distribution \\(Z\\sim\\mathop{\\mathrm{N}}(0,1)\\). It has pdf \\[ \\phi(z) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{z^2}{2}}, \\] specially denoted by the greek letter ‘\\(\\phi\\)’. Figure 2.1: Carl Friedrich Gauß. 30 April 1777 – 23 February 1855. Figure 2.2: 10 Deustche Mark banknote. 2.4.4 The normal cdf The cdf of the normal distribution \\(X\\sim\\mathop{\\mathrm{N}}(\\mu,\\sigma^2)\\) is \\[ F_X(x) = \\int_{-\\infty}^x \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(\\tilde x-\\mu)^2}{2\\sigma^2}} \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!\\tilde x =: \\Phi\\left(\\frac{x-\\mu}{\\sigma} \\right), \\] where \\(\\Phi(z)=\\int_{-\\infty}^z \\phi(\\tilde z) \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!\\tilde z\\) is the cdf of \\(Z=\\frac{X-\\mu}{\\sigma}\\). Values of \\(\\Phi(\\cdot)\\) must be read from a table, as the integrals above are intractable (no closed form solution). Download the statistical tables. Some results worth noting: \\(\\Pr(Z \\leq -a)=\\Phi(-a)=1-\\Phi(a)\\) \\(\\Pr(a \\leq Z \\leq b) = \\Phi(b) - \\Phi(a)\\) \\(P(-a \\leq Z \\leq b) = \\Phi(a) + \\Phi(b) - 1\\) \\(P(-a \\leq Z \\leq -b) = \\Phi(b) - \\Phi(a)\\) \\(P(|Z| \\leq a) = P(-a \\leq Z \\leq a) = 2\\Phi(a) - 1\\) \\(P(|Z| \\geq a) = P(\\{Z &lt; -a\\} \\cup \\{Z &gt;a\\}) = 2\\big(1-\\Phi(a)\\big)\\) We can use R to calculate probabilities: pnorm(1.96, mean = 0, sd = 1) ## [1] 0.9750021 Some values of \\(\\Phi(\\cdot)\\) worth remembering: \\(\\Phi(0) = 0.5\\) \\(\\Phi(1.64) \\approx 0.95\\) \\(\\Phi(1.96) \\approx 0.975\\) The last one, for example, says that \\[\\begin{align*} \\Pr(\\mu- 1.96\\sigma \\leq X \\leq \\mu+ 1.96\\sigma) &amp;= \\Pr\\left( \\left| \\frac{X-\\mu}{\\sigma} \\right| \\leq 1.96 \\right)\\\\ &amp;= 2\\Phi(1.96)-1\\approx 0.95 \\end{align*}\\] 2.4.5 68–95–99.7 Rule Incidentally, there is a shorthand to remember the percentage of values that lie within a band around the mean in a normal distribution. Here’s a nice short exploration of the normal distribution: https://bookdown.org/cquirk/LetsExploreStatistics/lets-explore-the-normal-distribution.html↩︎ "],["some-relationships.html", "2.5 Some relationships", " 2.5 Some relationships 2.5.1 Poisson-Binomial relationship The Poisson distribution plays a useful approximation role for some of the other main discrete distributions. Let \\(X\\sim\\mathop{\\mathrm{Bin}}(n,p)\\). Then \\[ X \\approx \\mathop{\\mathrm{Poi}}(np) \\] when \\(n\\) is large and \\(p\\) is small. Typically the rule of thumb is \\(n&gt;20\\) and \\(np&lt;5\\) or \\(n(1-p)&lt;5\\). Let \\(\\lambda=np\\). Consider the limit of as \\(n\\to\\infty\\) of the binomial pmf: \\[\\begin{align*} \\lim_{n\\to\\infty} \\Pr(X=x) &amp;= \\lim_{n\\to\\infty} \\frac{n!}{x!(n-x)!}\\left(\\frac{\\lambda}{n} \\right)^x \\left(1 - \\frac{\\lambda}{n} \\right)^{n-x} \\\\ &amp;= \\frac{\\lambda^x}{x!} \\lim_{n\\to\\infty} \\underbrace{\\frac{n!}{n^x(n-x)!}}_{\\to 1} \\, \\underbrace{\\left(1 - \\frac{\\lambda}{n} \\right)^n}_{\\to e^{-\\lambda}} \\, \\underbrace{\\left(1 - \\frac{\\lambda}{n} \\right)^{-x}}_{\\to 1} \\\\ &amp;= \\frac{e^{-\\lambda}\\lambda^x}{x!} \\\\ &amp;= \\Pr(Y=x), Y\\sim\\mathop{\\mathrm{Poi}}(\\lambda). \\end{align*}\\] Some details… \\[\\begin{align*} \\frac{n!}{n^x(n-x)!} &amp;= \\frac{n(n-1)(n-2)\\cdots 3\\cdot 2 \\cdot 1}{n \\cdot n \\cdots n \\cdot (n-x)(n-x-1)\\cdots 3\\cdot 2 \\cdot 1} \\\\ &amp;= \\frac{n}{n}\\frac{n-1}{n} \\cdots \\frac{n-x+1}{n} \\end{align*}\\] and each term converges to 1 as \\(n\\to\\infty\\). Also by definition, \\[ e^a = \\lim_{n\\to\\infty }\\left(1 + \\frac{a}{n} \\right)^n. \\] 2.5.2 Poisson-Exponential The exponential distribution is the probability distribution of the time between events in a Poisson point process, i.e., a process in which events occur continuously and independently at a constant average rate. Figure 2.3: Poisson-exponential process. Let \\(N_t\\) be the number of phone calls during time period \\(t\\); and \\(X_t\\) be the waiting time until the next phone call from one at \\(t\\). By definition, the two events are equivalent: \\(\\{X_t &gt; x \\} \\equiv \\{N_t = N_{t+x}\\}\\). Then \\(\\Pr(X_t \\leq x) = 1 - \\Pr(N_t - N_{t+x}=0)\\). \\(\\Pr(N_t - N_{t+x}=0)\\) is the probability of no calls between time period \\(t+x\\) and \\(t\\), which is also the same as saying that there are no calls in \\(x\\) amount of time, \\(\\Pr(N_x=0)\\). Assume that \\(N_t\\) is a Poisson process with rate \\(\\lambda\\) per unit time \\(t\\). So \\(N_x \\sim \\mathop{\\mathrm{Poi}}(\\lambda x)\\) and \\(\\Pr(N_x=0) = e^{-\\lambda x}\\). Substituting this into the above, we get \\[ \\Pr(X_t \\leq x) = 1 - e^{-\\lambda x} \\] which is the cdf of an \\(\\mathop{\\mathrm{Exp}}(\\lambda)\\) distribution. 2.5.3 Poisson-Gamma More generally, the Poisson and gamma (which includes exponential) are closely related when the gamma . Specifically, if \\(X\\sim\\Gamma(\\alpha,\\beta)\\), then for any \\(x&gt;0\\), \\[ \\Pr(X&gt;x) = \\Pr(Y&lt;\\alpha), \\] where \\(Y\\sim\\mathop{\\mathrm{Poi}}(x/\\beta)\\). The special case for the exponential distribution is easily seen: Set \\(\\alpha=1\\), then \\[ \\Pr(X&gt;x) = \\Pr(Y&lt;1) = \\Pr(Y=0) = e^{-x/\\beta}. \\] 2.5.4 Normal approximations The normal family can be used–largely on account of the Central Limit Theorem–to approximate various other distributions. \\(\\mathop{\\mathrm{Poi}}(\\lambda) \\approx \\mathop{\\mathrm{N}}(\\lambda,\\lambda)\\), for large values of \\(\\lambda\\). \\(\\mathop{\\mathrm{Bin}}(np) \\approx \\mathop{\\mathrm{N}}(np,np(1-p))\\), for large \\(n\\) (and \\(p\\) not too close to 0 or 1). \\(\\Gamma(\\alpha,\\beta) \\approx \\mathop{\\mathrm{N}}(\\alpha\\beta, \\alpha\\beta^2)\\) for large values of \\(\\alpha\\). We will officially cover the central limit theorem in detail in the next chapter. For now, you may think of it as follows. Suppose that we’re interested in the distribution of the sample mean (which, by now, you will agree is a random variable and hence has a distribution). The central limit theorem tells us precisely what the distribution of the sample mean will be when the number of samples we collect increases. It turns out to be the normal distribution! When approximating a discrete distribution, the normal approximation is much improved by use of a ‘continuity correction’. Example 2.1 Let \\(X\\sim\\mathop{\\mathrm{Bin}}(25, 0.6)\\). So \\(\\mathop{\\mathrm{E}}(X)=25\\times 0.6=15\\) and \\(\\mathop{\\mathrm{Var}}(X)\\) \\(=25\\times0.6\\times 0.4=6\\). The normal approximation is \\(X \\approx \\mathop{\\mathrm{N}}(15, 6)\\). A binomial probability such as \\[ \\Pr(X\\leq 13)=\\sum_{x=0}^{13} {25 \\choose x} 0.6^x0.4^{25-x}=0.267 \\] can be approximated as \\[ \\Pr(X\\leq 13)\\approx \\Pr\\left(Z \\leq \\frac{13-15}{\\sqrt 6}\\right)=0.207, \\hspace{1em} Z\\sim\\mathop{\\mathrm{N}}(0,1) \\] Evidently this is not a very good approximation. However, for discrete \\(X\\), \\(\\Pr(X\\leq 13)\\) and \\(\\Pr(X\\leq 13.5)\\) are identical, and approximating the latter gives a better result: \\[ \\Pr(X\\leq 13.5)\\approx \\Pr\\left(Z \\leq \\frac{13.5-15}{\\sqrt 6}\\right)=0.270, \\hspace{1em} Z\\sim\\mathop{\\mathrm{N}}(0,1). \\] pbinom(13, size = 25, prob = 0.6) ## [1] 0.2677178 pnorm(13.5, mean = 25 * 0.6, sd = sqrt(25 * 0.6 * 0.4)) ## [1] 0.2701457 Apply these continuity corrections in your calculations! Discrete Continous \\(X=c\\) \\(c-0.5 &lt; X &lt; c + 0.5\\) \\(X&lt;c\\) \\(X &lt; c + 0.5\\) \\(X\\leq c\\) \\(X &lt; c + 0.5\\) \\(X&gt;c\\) \\(X&gt;c-0.5\\) \\(X\\geq c\\) \\(X&gt;c-0.5\\) "],["exercises-1.html", "2.6 Exercises", " 2.6 Exercises The flow of traffic at certain street corners can sometimes be modelled as a sequence of Bernoulli trials by assuming that the probability of a car passing during any given second is a constant \\(p\\) and that there is no interaction between the passing of cars at different seconds. If we treat seconds as indivisible time units (trials), the Bernoulli model applies. Suppose a pedestrian can cross the street only if no car is to pass during the next 3 seconds. Find the probability that the pedestrian has to wait for exactly 4 seconds before starting to cross. A standard drug is known to be effective in 80% of cases. A new drug is tested on 100 patients and found to be effective in 85 cases. Evaluate this apparent evidence that the new drug is superior. Hint: calculate, using a normal approximation, the probability of getting 85 or more successes if in fact the new and old drugs are equally effective. Suppose that the number of chocolate chips in a certain type of cookie follows a Poisson distribution, and that we want the proportion of cookies containing at least two chocolate chips to be greater than 0.99. Find the smallest value of the mean of the distribution that ensures this probability. A truncated discrete distribution is one in which a particular outcome or set of outcomes cannot be observed and is eliminated from the sample space. In particular, if \\(X\\) has sample space \\(\\{0,1,2,\\dots\\}\\) but \\(0\\) cannot be observed (e.g, \\(X\\) might be the size of a group making a booking for the theatre, or the number of vehicles involved in a road accident) the zero-truncated random variable \\(X_T\\) has pmf \\[ \\Pr(X_T=x) = \\frac{\\Pr(X=x)}{\\Pr(X&gt;0)} \\] for \\(x=1,2,\\dots\\). If \\(X\\sim\\mathop{\\mathrm{Poi}}(\\lambda)\\), find the pmf, mean and variance of \\(X_T\\). Show that \\[ \\int_x^\\infty {\\frac{1}{\\Gamma(\\alpha)}} z^{\\alpha-1}e^{-z} \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!z = \\sum_{y=0}^{\\alpha-1} \\frac{x^ye^{-x}}{y!}, \\hspace{2em} \\alpha=1,2,3,\\dots \\] Hint: Use integration by parts, and the fact that \\(\\Gamma(n)=(n-1)!\\) for positive integers \\(n\\). Express this formula as a probabilistic relationship between Poisson and gamma random variables. The Pareto distribution, with parameters \\(\\alpha\\) and \\(\\beta\\), has pdf \\[ f(x) = \\frac{\\beta\\alpha^\\beta}{x^{\\beta+1}}, \\hspace{2em} 0&lt;\\alpha &lt; x &lt; \\infty, \\ \\ \\ \\beta &gt;0. \\] Verify that \\(f(x)\\) is a pdf. Derive the mean and variance of this distribution. Prove that the variance does not exist if \\(\\beta \\leq 2\\). Show, using the mgf \\(M_X(t)=\\exp[\\lambda(e^t-1)]\\), that if \\(X\\sim\\mathop{\\mathrm{Poi}}(\\lambda)\\) then \\(\\mathop{\\mathrm{E}}(X)=\\mathop{\\mathrm{Var}}(X)=\\lambda\\). Let \\(N\\sim\\mathop{\\mathrm{Poi}}(\\lambda)\\) and suppose we toss a coin \\(N\\) times, where \\(p\\) is the probability that the coin lands heads up. Let \\(X\\) and \\(Y\\) be the number of heads and tails respectively. Show that \\(X\\) and \\(Y\\) are independent. Let \\(X\\sim\\mathop{\\mathrm{Poi}}(\\lambda)\\) and \\(Y\\sim\\mathop{\\mathrm{Poi}}(\\mu)\\) and assume that \\(X\\) and \\(Y\\) are independent. Show that the distribution of \\(X\\) given that \\(X+Y=n\\) is \\(\\mathop{\\mathrm{Bin}}(n,\\pi)\\), where \\(\\pi=\\frac{\\lambda}{\\lambda + \\mu}\\). Use the following hints: If \\(X\\sim\\mathop{\\mathrm{Poi}}(\\lambda)\\) and \\(Y\\sim\\mathop{\\mathrm{Poi}}(\\mu)\\), and \\(X\\) and \\(Y\\) are independent, then \\(X+Y\\sim\\mathop{\\mathrm{Poi}}(\\lambda + \\mu)\\). \\(\\{X=x,X+Y=n \\}=\\{X=x,Y=n-x \\}\\). Let \\(X_1,\\dots,X_n\\,\\overset{\\text{iid}}{\\sim}\\,\\mathop{\\mathrm{Exp}}(\\beta)\\). Let \\(Y=\\max\\{X_1,\\dots,X_n\\}\\). Find the pdf of \\(Y\\). Hint: \\(Y\\leq y\\) iff \\(X_i\\leq y\\) for \\(i=1,\\dots,n\\). In each of the following cases verify the expression given for the mgf, and in each case, use the mgf to calculate \\(\\mathop{\\mathrm{E}}(X)\\) and \\(\\mathop{\\mathrm{Var}}(X)\\). \\(\\Pr(X=x)=\\frac{e^{-\\lambda}\\lambda^x}{x!}\\), \\(M_X(t)=e^{\\lambda(e^t - 1)}\\), \\(x=0,1,2,\\dots\\), \\(\\lambda&gt;0\\). \\(\\Pr(X=x)=p(1-p)^x\\), \\(M_X(t)=\\frac{p}{1-e^t(1-p)}\\), \\(x=0,1,2,\\dots\\), \\(0&lt;p&lt;1\\). \\(f_X(x)=\\frac{e^{-(x-\\mu)^2/2\\sigma^2}}{\\sigma\\sqrt{2\\pi}}\\), \\(M_X(t)=e^{\\mu t+ \\sigma^2t^2/2}\\), \\(x\\in\\mathbb{R}\\), \\(\\mu\\in\\mathbb{R}\\), \\(\\sigma\\in\\mathbb{R}_{&gt; 0}\\). Suppose the random variable \\(T\\) is the length of life of an object (possibly the lifetime of an electrical component or of a subject given a particular treatment). The hazard function \\(h_T(t)\\) associated with the random variable \\(T\\) is defined by \\[ h_T(t) = \\lim_{\\delta \\to 0} \\frac{\\Pr(t \\leq T &lt; t+\\delta | T \\geq t)}{\\delta}. \\] It is meant to denote the “event rate at time \\(t\\), conditional on survival until time \\(t\\) or later”. Thus, we can interpret \\(h_T(t)\\) as the rate of chance of the probability that the object survives a little past time \\(t\\), given that the object survives to time \\(t\\). Show that if \\(T\\) is a continuous random variable, then \\[ h_T(t) = \\frac{f_T(t)}{1-F_T(t)}=-\\frac{\\text{d}}{\\text{d}t} \\log(1-F_T(t)). \\] Hints: Use the definition of conditional probability. The derivative of a function \\(g\\) at \\(x\\) is defined as \\[ g&#39;(x) = \\lim_{\\delta \\to 0} \\frac{g(x+\\delta) - g(x)}{\\delta }. \\] The derivative of the cdf is the pdf. Evidence concerning the guilt or innocence of a defendant in a criminal investigation can be summarized by the value of an exponential random variable \\(X\\) whose mean \\(\\mu\\) depends on whether the defendant is guilty. If innocent, \\(\\mu=1\\); if guilty, \\(\\mu=2\\). The deciding judge will rule the defendant guilty if \\(X &gt; c\\) for some suitably chosen value of \\(c\\). If the judge wants to be 95 percent certain that an innocent man will not be convicted, what should be the value of \\(c\\)? Using the value of \\(c\\) found in part a., what is the probability that a guilty defendant will be convicted? An image is partitioned into two regions, one white and the other black. A reading taken from a randomly chosen point in the white section will give a reading that is normally distributed with \\(\\mu=4\\) and \\(\\sigma^2=4\\), whereas one taken from a randomly chosen point in the black region will have a normally distributed reading with parameters \\((6, 9)\\). A point is randomly chosen on the image and has a reading of 5. If the fraction of the image that is black is \\(\\alpha\\), for what value of \\(\\alpha\\) would the probability of making an error be the same, regardless of whether one concluded that the point was in the black region or in the white region? Hint: For \\(X\\sim\\mathop{\\mathrm{N}}(\\mu,\\sigma^2)\\), express \\(\\Pr(X=c)\\) as \\(\\lim_{\\epsilon\\to 0} \\Pr(c&lt;X&lt;X+\\epsilon)\\). Then use the definition of derivatives and the fact that \\(F&#39;(x)=f(x)\\). Hand-in questions Let \\(M_X(t)\\) be the mgf of \\(X\\), and define \\(S(t) = \\log M_X(t)\\). Show that \\(S&#39;(0)= \\mathop{\\mathrm{E}}(X)\\) and \\(S&#39;&#39;(0)= \\mathop{\\mathrm{Var}}(X)\\). Remark: \\(S(t)\\) is called the cumulant-generating function of \\(X\\). [3 marks] A model for the movement of a stock supposes that if the present price of the stock is \\(s\\), then, after one period, it will be either \\(us\\) with probability \\(p\\) or \\(ds\\) with probability \\(1-p\\). Assuming that successive movements are independent, approximate the probability that the stock’s price will be up at least 30 percent after the next 1000 periods if \\(u = 1.012\\), \\(d = 0.990\\), and \\(p = 0.52\\). [5 marks] Abu goes fishing every Sunday. The number of fish he catches follows a Poisson distribution. On a proportion \\(\\pi\\) of the days he goes fishing, he does not catch anything. He makes it a rule to take home the first and then every other fish that he catches (i.e. the first, third, fifth, and so on). Using a Poisson distribution, find the mean number of fish he catches. [3 marks] Show that the probability that he takes home the last fish he catches is \\((1-\\pi^2)/2\\). Hint: Use the fact that \\(\\sum_{k=0}^\\infty\\frac{e^{-\\lambda}\\lambda^{2k+1}}{(2k+1)!}=(1-e^{-2\\lambda})/2\\). [3 marks] "],["inequalities-convergences-and-normal-random-samples.html", "Chapter 3 Inequalities, convergences, and normal random samples", " Chapter 3 Inequalities, convergences, and normal random samples Learning objectives By the end of this chapter, you will be able to: do this do that and this Readings Casella and Berger (2002) Chapter 5, sections 5.1–5.3 and 5.5. Wasserman (2004) All of chapter 4. All of chapter 5. Topics not covered here: Order statistics, almost-sure convergence, consistency (will be covered in Part 4), strong LLN, multivariate delta method, Hoeffding’s inequality, Mill’s inequality, "],["introduction-1.html", "3.1 Introduction", " 3.1 Introduction 3.1.1 Random sampling Collection of data \\(X_1,\\dots,X_n\\) in an experiment consists of several observations on a variable of interest \\(X\\). Time to failure for \\(n\\) identical circuit boards. Yield (in tonnes) of \\(n\\) seasonal harvest for Laila variety paddy. Voter preferences for \\(n\\) individuals in the US. We can model this mathematically by declaring \\(X_1,\\dots,X_n\\) to be random variables sampled from a population whose pdf or pmf is \\(f_{{\\boldsymbol X}}({\\boldsymbol x})\\). We typically write \\[ X_1,\\dots,X_n \\,\\overset{\\text{iid}}{\\sim}\\,f_{{\\boldsymbol X}}. \\] We impose the distribution \\(f_{{\\boldsymbol X}}\\) onto the data as an assumption! As the British statistician George Box once famously said, “all models are wrong, but some are useful”. 3.1.2 Independent and identical r.v. Usually, the samples are taken in such a way that the value of one observation has no effect on or relationship with any of the other observations (i.e. \\(X_1,\\dots,X_n\\) are independent); and the pdf/pmf of each observation is \\(f(x)\\) (i.e. identical). In this case, \\[ f_{{\\boldsymbol X}}(x_1,\\dots,x_n) = \\prod_{i=1}^n f(x_i) \\] In particular, if the population pdf/pmf is a member of a parametric family, say one of those introduced in Part 2, then we can write \\[ f_{{\\boldsymbol X}}(x_1,\\dots,x_n|\\theta) = \\prod_{i=1}^n f(x_i|\\theta) \\] We could then use the random samples to infer about the (unknown) parameter \\(\\theta\\). More on this in the coming parts. Side note: Finite population sampling We have just defined sampling from an infinite population. Sometimes, sampling is done from a finite population, that is, the population consists only of possible observations \\(\\{x_1,\\dots,x_N \\}\\). There are several approaches to this which may or may not yield independent samples: sampling with vs without replacement simple random sampling vs complex random sampling single-stage sampling vs multi-stage sampling etc. Very important topic in survey methodology. For more details see C&amp;B §5.1, as well as 2019 lecture slides (Chapter 2). In this course, we deal only with the infinite population model. 3.1.3 Statistic Definition 3.1 (Statistic) A statistic is any function \\(T_n = T(X_1,\\dots,X_n)\\). It cannot depend on unknown parameters, only on observables. Here are two very commonly used statistics: The sample mean \\[ \\bar X = \\frac{1}{n}\\sum_{i=1}^n X_i \\] The (unbiased) sample variance \\[ S^2 = \\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2 \\] Lemma 3.1 Let \\(X_1,\\dots,X_n\\) be a r.v. with mean \\(\\mu\\) and variance \\(\\sigma^2&lt;\\infty\\). Then \\(\\mathop{\\mathrm{E}}(\\bar X) = \\mu\\). \\(\\mathop{\\mathrm{Var}}(\\bar X) = \\frac{\\sigma^2}{n}\\). \\(\\mathop{\\mathrm{E}}(S^2)=\\sigma^2\\). The proof of this can be found in C&amp;B, Theorem 5.2.6. 3.1.4 Sampling distribution Realise that A statistic \\(T_n\\) is itself a r.v.. If it is random, it has a distribution. Along with having a distribution, all of concepts and properties we discussed in Parts 1 &amp; 2 apply. Think about the statement above, “a statistic \\(T_n\\) is itself a r.v.”–can you rationalise why this is? Suppose you collect some data and plug these values into a statistics function (e.g. the sample mean). Will the value of the sample mean be the same each time, or will it depend on the (random) values of the data? A very common theme in inferential statistics is to figure out what the distribution of \\(T_n\\) is in repeated sampling. In parametric statistics for example, the statistic \\(T_n\\) may serve as an estimator for the true unknown value \\(\\theta\\). How do we know that \\(T_n\\) is a good estimator? It takes different values with repeated sampling, is it typically close to \\(\\theta\\)? How close? 3.1.5 Large-sample approximation Some statistics have easily-derived sampling distribution; others do not. For instance, suppose each \\(X_i\\sim\\mathop{\\mathrm{N}}(\\mu,\\sigma^2)\\). Then \\[\\begin{equation} \\bar X \\sim \\mathop{\\mathrm{N}}(\\mu, \\sigma^2/n). \\tag{3.1} \\end{equation}\\] The above fact (3.1) is very important and pops up all the time in statistics. Have a go at proving the distribution of the sample mean. Generally speaking statistics derived from normal random samples have ‘easy’ distributions (we’ll see this later). But what is the distribution of \\[ n^{-1}\\sum_{i=1}^n \\tan^{-1}(X_i)? \\] We use approximate distributions, which can be found by using asymptotic arguments. That is, we consider the behaviour of the distribution of the complicated statistics \\(T_n\\) as \\(n\\to\\infty\\). For this, we first need to study inequalities and convergences. "],["inequalities.html", "3.2 Inequalities", " 3.2 Inequalities Inequalities are useful tools in establishing various properties of statistical inference methods. They may also provide estimates for probabilities with little assumption on probability distributions. There are four main inequalities that we will learn: Markov’s inequality Chebyshev’s inequality Cauchy-Schwarz inequality Jensen’s inequality 3.2.1 Markov’s inequality In probability theory, Markov’s inequality gives an upper bound for the probability that a non-negative r.v. exceeds some positive constant. Lemma 3.2 (Markov's inequality) Let \\(X\\geq 0\\) be a non-negative r.v. and \\(\\mathop{\\mathrm{E}}(X) &lt; \\infty\\). Then, for any \\(t&gt;0\\), \\[\\Pr(X\\geq t) \\leq \\frac{\\mathop{\\mathrm{E}}(X)}{t}.\\] Markov’s inequality relate probabilities to expectations, and provides bounds for the cumulative distribution function of a r.v.. Proof. Let \\(f(x)\\) be the pdf of \\(X\\). Since \\(X\\geq 0\\), \\[\\begin{aligned} \\mathop{\\mathrm{E}}(X) = \\int_{-\\infty}^{\\infty} x\\,f(x) \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!x &amp;= \\int_0^{\\infty} x\\,f(x) \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!x \\\\ &amp;= \\int_0^t x\\,f(x) \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!x + \\int_t^\\infty x\\,f(x) \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!x \\\\ &amp;\\geq \\int_t^\\infty x\\,f(x) \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!x \\\\ &amp;\\geq t\\int_t^\\infty f(x) \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!x \\\\ &amp;= t\\Pr(X\\geq t) \\end{aligned}\\] Corollary 3.1 For any r.v. \\(X\\) and any constant \\(t&gt;0\\), \\[\\begin{aligned} \\Pr(|X|\\geq t) &amp;\\leq \\frac{E|X|}{t} \\ \\ \\ \\text{ provided } E|X|&lt;\\infty \\\\ \\Pr(|X|\\geq t) &amp;\\leq \\frac{E\\big(|X|^k\\big)}{t^k} \\ \\ \\ \\text{ provided } E\\big(|X|^k\\big)&lt;\\infty \\end{aligned}\\] The tail probability \\(\\Pr(|X|\\geq t)\\) is a useful measure in insurance and risk management in finance. The more moments \\(X\\) has, the smaller the tail probabilities are. 3.2.2 Chebyshev’s inequality In probability theory, Chebyshev’s inequality guarantees that no more than a certain fraction of values can be more than a certain distance from the mean. Lemma 3.3 (Chebyshev's inequality) Suppose a r.v. \\(X\\) has mean \\(\\mu\\) and variance \\(\\sigma^2\\). Then, for any \\(t&gt;0\\), \\[\\Pr(|X-\\mu| \\geq t\\sigma) \\leq \\frac{1}{t^2}.\\] The proof of Chebyshev’s inequality follows directly from Markov’s inequality. You will prove this in the exercises. Because it can be applied to completely arbitrary distributions (provided they have a known finite mean and variance), the inequality generally gives a poor bound, compared to what might be deduced if more aspects are known about the distribution involved. Note that \\[\\begin{align*} \\Pr(|X-\\mu| \\geq t\\sigma) &amp;= \\Pr\\big(\\{ X \\leq \\mu - t\\sigma \\} \\cup \\{ X \\geq \\mu + t\\sigma \\}\\big) \\\\ &amp;= 1 - \\Pr( \\mu - t\\sigma \\leq X \\leq \\mu + t\\sigma ) \\\\ &amp;= 1 - \\Pr\\big(|X-\\mu| \\leq t\\sigma\\big) \\end{align*}\\] Example 3.1 Suppose \\(X\\) has mean 0 and variance 1. By Chebyshev’s inequality, \\[\\begin{aligned} \\Pr(|X| \\geq 1) &amp;\\leq 1.00 \\\\ \\Pr(|X| \\geq 2) &amp;\\leq 0.25 \\\\ \\Pr(|X| \\geq 3) &amp;\\leq 0.11 \\end{aligned}\\] In contrast, suppose that we know that \\(X\\) is normally distributed. Then \\[\\begin{aligned} \\Pr(|X| \\geq 1) &amp;\\leq 0.318 \\\\ \\Pr(|X| \\geq 2) &amp;\\leq 0.046 \\\\ \\Pr(|X| \\geq 3) &amp;\\leq 0.003 \\end{aligned}\\] Recall the 68-95-99.7 rule when we discussed the normal distribution in Chapter2. Calculate the above probabilities in R: 2 * (pnorm(-c(1, 2, 3))) ## [1] 0.317310508 0.045500264 0.002699796 3.2.3 Cauchy-Schwartz inequality This is a very useful inequality that crops up in many different areas of mathematics, such as linear algebra, analysis, probability theory, vector algebra, etc. Lemma 3.4 (Cauchy-Schwartz inequality) Let \\(E(X^2)&lt;\\infty\\) and \\(E(Y^2)&lt;\\infty\\). Then \\[|E(XY)|^2 \\leq E(X^2)E(Y^2).\\] Subtle point: \\(|E(XY)|^2 = \\mathop{\\mathrm{E}}^2(XY)\\). Proof. Consider the expectation \\(\\mathop{\\mathrm{E}}((tX+Y)^2)\\geq 0\\) for some constant \\(t\\in\\mathbb{R}\\). Expanding out, we have \\[ \\mathop{\\mathrm{E}}((tX+Y)^2) = \\overbrace{\\mathop{\\mathrm{E}}(X^2)}^{a}t^2 + \\overbrace{2\\mathop{\\mathrm{E}}(XY)}^{b}t+ \\overbrace{\\mathop{\\mathrm{E}}(Y^2)}^{c} \\] For some constants \\(a,b,c\\in\\mathbb{R}\\), the polynomial \\(at^2 +bt + c\\) remains non-negative iff \\(a\\geq 0\\) and the discriminant \\(b^2-4ac \\leq 0\\). Thus, \\[ 4\\mathop{\\mathrm{E}}^2(XY) - 4\\mathop{\\mathrm{E}}(X^2)\\mathop{\\mathrm{E}}(Y^2) \\leq 0, \\] and dividing by 4 throughout, we have the desired result. As a consequence of the Cauchy-Schwartz inequality, we have the covariance inequality. Corollary 3.2 (The covariance inequality) Let \\(X\\) and \\(Y\\) be random variables. Then \\[\\mathop{\\mathrm{Var}}(Y) \\geq \\frac{\\mathop{\\mathrm{Cov}}(Y,X)\\mathop{\\mathrm{Cov}}(Y,X)}{\\mathop{\\mathrm{Var}}(X)}\\] You will prove the covariance inequality in one of the exercises at the end of this chapter. 3.2.4 Jensen’s inequality Before discussing the next kind of inequality, we shall first discuss convex functions. A function \\(g\\) is if for any \\(x,y\\) and any \\(\\alpha \\in [0,1]\\), \\[g(\\alpha x + (1-\\alpha)y) \\leq \\alpha g(x) + (1-\\alpha)g(y).\\] If \\(g&#39;&#39;(x)&gt;0\\) for all \\(x\\), then \\(g\\) is convex. A function \\(g\\) is if \\(-g\\) is convex. Example 3.2 Examples of convex functions: \\(g_1(x) = x^2\\) and \\(g_2(x) = e^x\\), since \\(g_1&#39;&#39;(x) = 2&gt;0\\) and \\(g_2(x)=e^x &gt; 0\\) for all \\(x\\). Examples of concave functions: \\(g_3(x) = -x^2\\) and \\(g_4(x) = \\log(x)\\). In the context of probability theory, we consider expectations of convex functions of r.v.s Lemma 3.5 (Jensen's inequality) Let \\(X\\) be a r.v. and \\(g\\) a convex function. Then, \\[\\mathop{\\mathrm{E}}\\left[g(X) \\right] \\geq g\\left(\\mathop{\\mathrm{E}}X \\right)\\] It follows directly from Jensen’s inequality, the following: \\(\\mathop{\\mathrm{E}}(X^2) \\geq \\{\\mathop{\\mathrm{E}}(X)\\}^2\\) \\(\\mathop{\\mathrm{E}}(1/X) \\geq 1 / \\mathop{\\mathrm{E}}X\\) \\(\\mathop{\\mathrm{E}}(\\log X) \\geq \\log (\\mathop{\\mathrm{E}}X)\\) "],["convergence-of-random-variables.html", "3.3 Convergence of random variables", " 3.3 Convergence of random variables Recall the limits of sequences of real numbers \\((a_n)\\), \\(n\\in\\mathbb{N}\\). Definition 3.2 (Limit of a real sequence) We call \\(a\\) the limit of the real sequence \\((a_n)\\) if for each real number \\(\\epsilon&gt;0\\), \\(\\exists\\) a natural number \\(N(\\epsilon)\\in\\mathbb{N}\\) such that, for every natural number \\(n\\geq N\\), we have \\(|a_n-a| &lt; \\epsilon\\). We write \\(\\lim_{n\\to\\infty} a_n = a\\), or simply \\(a_n \\to a\\). This also means that \\(|a_n-a| \\to 0\\) as \\(n\\to\\infty\\). For every measure of closeness \\(\\epsilon\\), the sequence’s terms are eventually that close to the limit. Some examples: If \\(a_n=c\\) for some constant \\(c\\in\\mathbb{R}\\), then \\(a_n\\to c\\). If \\(a_n=1/n\\), then \\(a_n\\to 0\\). \\(\\lim_{n\\to\\infty}(1+1/n)^n = e\\). What if \\((a_n)\\) is a random sequence (i.e. their values are not deterministic)? Does the concept of limits even make sense? Is it possible to “trap” the sequence between an upper and lower bound as the sequence progresses? This is what we will be exploring in this section. We can in fact say similar things about sequences of random variables, e.g. \\(X\\) is the limit of a sequence \\((X_n)\\) if \\(|X_n - X|\\to 0\\) as \\(n\\to\\infty\\). There are some subtle issues here: \\(|X_n-X|\\) itself is a r.v., i.e. it takes difference values in the sample space \\(\\Omega\\). Therefore, \\(|X_n - X|\\to 0\\) should hold (almost) entirely on the sample space. This calls for a probability statement. Since r.v. have distributions, we may also consider convergence of their distributions \\(F_{X_n}(x)\\to F_X(x)\\) for all \\(x\\). We need better tools to rigorously discuss the concept of convergence of r.v.s. Let \\(X_1,X_2,\\dots\\) be a sequence of r.v., and \\(X\\) be another r.v.. The main types of convergence for r.v. that we will study are as follows: Convergence in probability Convergence in distribution Convergence in mean-square 3.3.1 Convergence in probability Definition 3.3 (Convergence in probability) \\(X_n\\) converges to \\(X\\) in probability if for any constant \\(\\epsilon&gt;0\\), \\[\\lim_{n\\to\\infty} \\Pr(|X_n-X|\\geq\\epsilon) = 0.\\] We write \\(X_n\\xrightarrow{\\text{P}}X\\), or \\(\\mathop{\\mathrm{plim}}_{n\\to\\infty}X_n = X\\). An equivalent definition is \\[ \\lim_{n\\to\\infty}\\Pr(|X_n-X| &lt; \\epsilon) = 1. \\] In words: “the probability of an ‘unusual’ outcome becomes smaller and smaller as the sequence progresses”. Here, \\(X\\) may be a r.v. or a constant. Example 3.3 Let \\(X_i\\,\\overset{\\text{iid}}{\\sim}\\,\\mathop{\\mathrm{Unif}}(0,1)\\). Define another r.v. \\(Y_n=\\min\\{X_1,\\dots,X_n\\}\\). Does \\(Y_n\\) converge to something? Draw some samples: set.seed(123) X &lt;- runif(20); Y &lt;- rep(NA, 20) for (i in 1:20) Y[i] &lt;- min(X[1:i]) round(X, 2) ## [1] 0.29 0.79 0.41 0.88 0.94 0.05 0.53 0.89 0.55 0.46 0.96 0.45 0.68 0.57 0.10 ## [16] 0.90 0.25 0.04 0.33 0.95 round(Y, 2) ## [1] 0.29 0.29 0.29 0.29 0.29 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 ## [16] 0.05 0.05 0.04 0.04 0.04 A good guess with be \\(Y_n \\to 0\\), so let’s prove this. We want to show that \\[\\Pr(|Y_n-0| \\geq \\epsilon)=\\Pr(Y_n\\geq \\epsilon)\\to 0\\] as \\(n\\to \\infty\\). There are two cases, i) \\(\\epsilon &gt; 1\\) or ii) \\(\\epsilon \\leq 1\\). If i), then \\(\\Pr(Y_n\\geq \\epsilon)=0\\) and we are done. However, if \\(\\epsilon \\leq 1\\), then \\[ \\begin{aligned} \\Pr(Y_n\\geq \\epsilon) &amp;= \\Pr(\\min\\{X_1,\\dots,X_n\\} \\geq \\epsilon) \\\\ &amp;=\\Pr(X_1 \\geq \\epsilon, \\dots, X_n \\geq \\epsilon) \\\\ &amp;=\\Pr(X_1\\geq \\epsilon)\\cdots \\Pr(X_n\\geq \\epsilon) \\text{ by independence}\\\\ &amp;=(1-\\epsilon)^n \\to 0 \\end{aligned} \\] as \\(n\\to \\infty\\). Hence \\(Y_n \\xrightarrow{\\text{P}} 0\\). 3.3.2 Convergence in distribution Instead of considering the convergence of the r.v. itself, we consider the convergence of the distribution of the sequence of r.v.s. Definition 3.4 (Convergence in distribution) \\(X_n\\) converges to \\(X\\) in distribution if \\[\\lim_{n\\to\\infty}F_{X_n}(x) = F_X(x).\\] We write \\(X_n\\xrightarrow{\\text{D}}X\\). Again here \\(X\\) may be a constant, since a constant is a r.v. with probability mass concentrated on a single point. We can also write \\(X_n{\\xrightarrow{\\text{D}}} F_X\\), where \\(F_X\\) is the cdf of \\(X\\). However, the notation \\(X_n{\\xrightarrow{\\text{P}}} F_X\\) does not make sense! Convergence in probability implies convergence in distribution, but not the other way around (unless the limiting r.v. is a point mass). Example 3.4 Let \\(X\\sim\\mathop{\\mathrm{N}}(0,1)\\) and \\(X_n=-X\\) for all \\(n \\geq 1\\). Then, clearly \\(F_{X_n} \\equiv F_{X}\\) (by linearity of normal distributions). Hence, \\(X_n\\xrightarrow{\\text{D}}X\\). However, \\(X_n\\) does not converge in probability to \\(X\\), as for any \\(\\epsilon&gt;0\\), \\[\\begin{aligned} \\Pr(|X_n-X|\\geq\\epsilon) &amp;= \\Pr(2|X|\\geq\\epsilon) \\\\ &amp;= \\Pr(|X|\\geq\\epsilon/2) &gt; 0. \\end{aligned}\\] So we cannot have that \\(\\Pr(|X_n-X|\\geq\\epsilon) \\to 0\\) as \\(n\\to\\infty\\). 3.3.3 Mean-square convergence It is sometimes more convenient to consider the mean-square convergence: Definition 3.5 (Mean-square convergence) \\(X_n\\) converges in mean-square to \\(X\\) if \\[\\lim_{n\\to\\infty} \\mathop{\\mathrm{E}}\\left[(X_n - X)^2 \\right] = 0.\\] We write \\(X_n\\xrightarrow{\\text{m.s.}}X\\). It follows that from Markov’s inequality, \\[\\begin{aligned} \\Pr(|X_n-X|\\geq \\epsilon) &amp;= \\Pr(|X_n-X|^2\\geq \\epsilon^2) \\\\ &amp;\\leq \\frac{\\mathop{\\mathrm{E}}\\left[(X_n - X)^2 \\right]}{\\epsilon^2} \\end{aligned}\\] Therefore, if \\(X_n\\xrightarrow{\\text{m.s.}}X\\), it also holds that \\(X_n\\xrightarrow{\\text{P}}X\\). Convergence in mean-square implies convergence in probability, but not the other way around. Example 3.5 Let \\[ X_n = \\begin{cases} n^2 &amp;\\text{w.p. } 1/n \\\\ 0 &amp;\\text{w.p. } 1- 1/n \\end{cases}\\] Then, for any \\(\\epsilon&gt;0\\), \\(\\Pr(|X_n|\\geq\\epsilon) = \\Pr(X_n = n^2) = 1/n \\to 0\\) as \\(n\\to\\infty\\). Hence, \\(X_n{\\xrightarrow{\\text{P}}}0\\). However, \\[\\mathop{\\mathrm{E}}(X_n^2) = n^2 \\cdot \\Pr(X_n = n^2) + 0 \\cdot \\Pr(X_n = 0) = n \\to \\infty\\] hence \\(X_n\\centernot{\\xrightarrow{\\text{m.s.}}}0\\). 3.3.4 Relationship between convergences You can find the proof of the above statements in Wasserman (Theorem 5.4). The proof is fairly easy to follow but for brevity will not be repeated here. As we saw previously, Convergence in distribution does not imply convergence in probability. Convergence in probability does not imply convergence in mean-square. If \\(X_n \\xrightarrow{\\text{D}}c\\in\\mathbb{R}\\) then \\(X_n \\xrightarrow{\\text{P}}c\\). It is typically easier to prove convergence in mean-square, which thus also implies convergence in probability and in distribution. 3.3.5 Slutzky’s Theorem Theorem 3.1 (Slutzky's Theorem) Let \\(X_n\\), \\(Y_n\\), \\(X\\), and \\(Y\\) be r.v., \\(g\\) a continuous function, and \\(c\\) a real constant. Then, If \\(X_n{\\xrightarrow{\\text{P}}} X\\) and \\(Y_n{\\xrightarrow{\\text{P}}} Y\\), then \\(X_n+Y_n{\\xrightarrow{\\text{P}}} X+Y\\); \\(X_nY_n{\\xrightarrow{\\text{P}}} XY\\); and \\(g(X_n){\\xrightarrow{\\text{P}}} g(X)\\). If \\(X_n{\\xrightarrow{\\text{D}}} X\\) and \\(Y_n{\\xrightarrow{\\text{D}}} c\\), then \\(X_n+Y_n{\\xrightarrow{\\text{D}}} X+c\\); \\(X_nY_n{\\xrightarrow{\\text{D}}} cX\\); and \\(g(X_n){\\xrightarrow{\\text{D}}} g(X)\\). Caution! If \\(X_n{\\xrightarrow{\\text{D}}} X\\) and \\(Y_n{\\xrightarrow{\\text{D}}} Y\\), it does in general imply that \\(X_n+Y_n{\\xrightarrow{\\text{D}}} X+Y\\). "],["limit-theorems.html", "3.4 Limit theorems", " 3.4 Limit theorems 3.4.1 The (weak) Law of Large Numbers Perhaps the best application of convergence in probability. Theorem 3.2 (The weak law of large numbers; WLLN) Let \\(X_1,X_2,\\dots\\) be iid r.v.s with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Let \\(\\bar X_n\\) denote the sample mean, i.e. \\[\\bar X_n = \\frac{1}{n}\\sum_{i=1}^n X_i.\\] Then, as \\(n\\to\\infty\\), \\[\\bar X_n{\\xrightarrow{\\text{P}}} \\mu.\\] The LLN is very natural: When the sample size increases, the sample mean becomes more and more close to the population mean. Furthermore, the distribution of \\(\\bar X_n\\) degenerates to a single point distribution at \\(\\mu\\), the true mean. Proof. Recall that \\(\\mathop{\\mathrm{E}}(\\bar X_n)=\\mu\\) and \\(\\mathop{\\mathrm{Var}}(\\bar X_n) = \\sigma^2/n\\). Choose an \\(\\epsilon &gt;0\\) such that \\(\\epsilon=t\\sigma/\\sqrt{n}\\). By Chebyshev’s inequality, \\[\\begin{align*} \\Pr(|\\bar X_n - \\mu| \\geq \\overbrace{t\\sigma/\\sqrt{n}}^{\\epsilon}) &amp;\\leq \\frac{1}{t^2}\\\\ &amp;=\\frac{\\sigma^2}{n\\epsilon^2} \\to 0 \\end{align*}\\] as \\(n\\to\\infty\\). Hence, \\(\\bar X_n{\\xrightarrow{\\text{P}}} \\mu\\). As an illustration of the WLLN, consider an experiment where we throw a six-sided die repeatedly and independently. Let \\(X_1,X_2,\\dots\\) be the scores of the dice throws. We know that the true mean is \\(\\mu=3.5\\). Let’s simulate some dice throws: set.seed(123) (X &lt;- sample(6, size = 20, replace = TRUE)) ## [1] 3 6 3 2 2 6 3 5 4 6 6 1 2 3 5 3 3 1 4 1 Xbar &lt;- cumsum(X) / seq_along(X) round(Xbar, 2) ## [1] 3.00 4.50 4.00 3.50 3.20 3.67 3.57 3.75 3.78 4.00 4.18 3.92 3.77 3.71 3.80 ## [16] 3.75 3.71 3.56 3.58 3.45 It would be very good practice to work out the true mean (\\(\\mu=3.5\\)) of the scores of the dice throws, using the formula for the expectations of discrete probability models. 3.4.2 The Central Limit Theorem The LLN assures us that \\(\\bar X_n\\) eventually will be indistinguishable from \\(\\mu\\) w.p. 1. However, we would still be interested in the distribution of \\(\\bar X_n\\) in order to make probabilistic statements about \\(\\bar X_n\\). Theorem 3.3 (Central Limit Theorem; CLT) Let \\(X_1,\\dots,X_n\\) be iid r.v. with mean \\(\\mu\\) and variance \\(\\sigma^2\\), and let \\(\\bar X_n\\) denote the sample mean. Then \\[ \\bar Z_n := \\frac{\\bar X_n - \\mu}{\\sigma/\\surd n} \\xrightarrow{\\text D} \\mathop{\\mathrm{N}}(0,1) \\] as \\(n\\to \\infty\\). In words: “the standardised sample mean \\(\\bar Z_n\\) is approximately standard normal when the sample size is large”. This is remarkable because we assume nothing about the distribution of the individual \\(X_i\\)s! The CLT is one of the reasons why the normal distribution is the most useful and important distribution in statistics. Alternative statements for the CLT include \\(\\sqrt{n}(\\bar X_n - \\mu)/\\sigma \\approx \\mathop{\\mathrm{N}}(0,1)\\) \\(\\sqrt{n}(\\bar X_n - \\mu) \\approx \\mathop{\\mathrm{N}}(0,\\sigma^2)\\) \\(\\bar X_n - \\mu \\approx \\mathop{\\mathrm{N}}(0,\\sigma^2/n)\\) \\(\\bar X_n \\approx \\mathop{\\mathrm{N}}(\\mu,\\sigma^2/n)\\) Some other remarks: The CLT gives us information about the variability of the sample mean statistic in repeated sampling, see the slides after the next example. The CLT tells us nothing about the accuracy of any implied approximation for finite \\(n\\). However, it still yields remarkably accurate approximations in many situations, even with modest \\(n\\). A version of the proof involves mgfs, as you will see in the exercises. The CLT is responsible for the normal approximations to the binomial, Poisson, gamma, etc.! Example 3.6 Recall the dice example above. The CLT implies that \\[\\begin{equation} \\bar X_n \\approx \\mathop{\\mathrm{N}}\\left(3.5, \\frac{105}{36n}\\right), \\tag{3.2} \\end{equation}\\] since \\(\\mathop{\\mathrm{Var}}(X_i)=105/36\\). See that variance of 105/36 in the example above? Try and obtain this figure yourself using the usual definitions of variances, or better yet, employ the results from the binomial distribution. To illustrate this, we can take many samples of size \\(n\\) and compute the sample mean for each set, we then obtain many sample means. The standardised histogram of those samples resembles the normal pdf in (3.2). Here’s the R code to replicate dice rolls and the sample means. The idea is to generate a sample of size \\(n\\) of dice roll scores repeatedly B times. my_clt_fn &lt;- function(n = 5, B = 10000) { res &lt;- rep(NA, B) for (i in 1:B) { X &lt;- sample(1:6, size = n, replace = TRUE) res[i] &lt;- mean(X) } res } We can also use this to retrieve \\(\\bar X_{20}=3.45\\) using the same random seed. set.seed(123); my_clt_fn(n = 20, B = 1) ## [1] 3.45 3.4.3 Gauging the error of sample mean estimator A natural estimator for the population mean \\(\\mu=\\mathop{\\mathrm{E}}(X_i)\\) is the sample mean \\(\\bar X_n\\). By the CLT, we can easily gauge the error of this estimation as follows: \\[\\begin{aligned} \\Pr(|\\bar X_n-\\mu| &gt; \\epsilon) &amp;= \\Pr(\\big|\\overbrace{\\sqrt{n}(\\bar X_n - \\mu)/\\sigma}^{\\approx\\mathop{\\mathrm{N}}(0,1)} \\big| &gt; \\sqrt{n}\\epsilon/\\sigma) \\\\ &amp;\\approx 2\\big(1-\\Phi(\\sqrt{n}\\epsilon/\\sigma)\\big) \\end{aligned}\\] So with \\(\\epsilon\\), \\(n\\), and \\(\\sigma\\) given, we can find the value \\(\\Phi(\\sqrt{n}\\epsilon/\\sigma)\\) from the standard normal table. For instance, let \\(\\epsilon := 2\\sigma/\\sqrt{n} = 2 \\sqrt{\\mathop{\\mathrm{Var}}(\\bar X_n)}\\). Then \\[ \\begin{aligned} \\Pr(|\\bar X_n-\\mu| \\leq \\epsilon) &amp;= 1 - \\Pr(|\\bar X_n-\\mu| &gt; \\epsilon) \\\\ &amp;\\approx 2\\Phi(2) - 1\\\\ &amp;=0.954 \\end{aligned} \\] Hence, if one estimates \\(\\mu\\) by \\(\\bar X_n\\), and repeats it a large number of times, about 95% of times, \\(\\mu\\) is within \\(2 \\times \\text{s.d.}(\\bar X_n)\\) distance away from \\(\\bar X_n\\) Does this look familiar to you? Recall the “68-95-99.7” rule! 3.4.4 CLT with \\(\\sigma^2\\) unknown Typically, \\(\\sigma^2 = \\mathop{\\mathrm{Var}}(X_i)\\) is unknown in practice. We estimate it using the (unbiased) sample variance estimator \\[S_n^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\bar X_n)^2\\] Note that the estimate of \\(\\sqrt{\\mathop{\\mathrm{Var}}(\\bar X_n)} = \\sigma/\\surd n\\), given by \\(S_n/\\surd n\\), is called the standard error of the sample mean. In full, \\[\\text{SE}(\\bar X_n) = \\frac{1}{n(n-1)}\\sum_{i=1}^n (X_i - \\bar X_n)^2\\] In fact, it still holds that as \\(n\\to\\infty\\), \\[\\frac{\\bar X_n - \\mu}{S_n\\big/\\surd n} \\xrightarrow{\\text D} \\mathop{\\mathrm{N}}(0,1)\\] which implies that replacing \\(\\sigma\\) with \\(S_n\\) in CLT applications yields the same results. Phew! "],["delta-method.html", "3.5 Delta method", " 3.5 Delta method We may be interested in the distribution of a transformation of a r.v. instead of the actual r.v. itself. For this, we use the delta method. Theorem 3.4 (The delta method) Suppose that \\(X_n\\) is a sequence of r.v. satisfying \\(\\sqrt n(X_n - \\mu)/\\sigma {\\xrightarrow{\\text{D}}} \\mathop{\\mathrm{N}}(0,1)\\). Let \\(g\\) be a differentiable function s.t. \\(g&#39;(\\mu)\\neq 0\\). Then \\[ \\frac{\\sqrt n \\big(g(X_n)-g(\\mu)\\big)}{|g&#39;(\\mu)|\\sigma} {\\xrightarrow{\\text{D}}} \\mathop{\\mathrm{N}}(0,1). \\] In other words, \\[ X_n \\approx \\mathop{\\mathrm{N}}\\left( \\mu, \\sigma^2/n \\right) \\hspace{1em} \\Rightarrow g(X_n) \\approx \\mathop{\\mathrm{N}}\\left( g(\\mu), (g&#39;(\\mu))^2\\sigma^2/n \\right). \\] Example 3.7 Suppose we observe \\(X_1,\\dots,X_n\\sim\\mathop{\\mathrm{Bern}}(p)\\). A reasonable estimator for \\(p\\) is the sample mean \\(\\hat p := \\bar X_n = n^{-1}\\sum_{i=1}^n X_i\\). According to the CLT, \\(\\hat p \\approx (p, p(1-p)/n)\\) for large \\(n\\), since \\(\\mathop{\\mathrm{Var}}(X_i)=p(1-p)\\). Another popular parameter is \\(\\frac{p}{1-p}\\), the . This is a transformation of \\(p\\) using \\(g:p \\mapsto \\frac{p}{1-p}\\), for which \\(g&#39;(p) = \\frac{1}{(1-p)^2}\\). Using the delta method, we deduce that \\[ \\frac{\\hat p}{1-\\hat p} \\approx \\mathop{\\mathrm{N}}\\left(\\frac{p}{1-p}, \\frac{p}{n(1-p)^3} \\right). \\] "],["normal-random-samples.html", "3.6 Normal random samples", " 3.6 Normal random samples Given that the normal distribution is very often used, the properties of normal random samples have been studied extensively. Theorem 3.5 Let \\(\\{X_1,\\dots,X_n \\}\\) be a sample from \\(\\mathop{\\mathrm{N}}(\\mu,\\sigma^2)\\), and let \\[\\bar X = \\frac{1}{n}\\sum_{i=1}^n X_i, \\hspace{2em} S^2 = \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar X)^2, \\hspace{1em} \\text{ and } \\hspace{1em} \\text{SE}(\\bar X) = S/\\sqrt{n}.\\] Then, \\(\\bar X\\) and \\(S^2\\) are independent r.v.s \\(\\bar X \\sim \\mathop{\\mathrm{N}}(\\mu,\\sigma^2/n)\\) \\((n-1)S^2/\\sigma^2 \\sim \\chi^2_{n-1}\\) \\(\\frac{\\sqrt n (\\bar X - \\mu)}{S} = \\frac{\\bar X - \\mu}{\\text{SE}(\\bar X)} \\sim t_{n-1}\\) Given that the above theorem mentions two kinds of distribution (that you may have heard of) but we have yet to discuss, we’ll circle back to the proof othis theorem after covering the \\(\\chi^2\\) and \\(t\\) distributions. 3.6.1 \\(\\chi^2\\)-distribution The \\(\\chi^2\\)-distribution is an important distribution in statistics. It is closely linked with the normal, Student’s \\(t\\) and \\(F\\) distributions. Inference for the variance parameter \\(\\sigma^2\\) relies on \\(\\chi^2\\)-distributions. More importantly, most goodness-of-fit tests are based on \\(\\chi^2\\)-distributions. Definition 3.6 ($\\chi^2$-distribution) Let \\(Z_1,\\dots,Z_k \\,\\overset{\\text{iid}}{\\sim}\\,\\mathop{\\mathrm{N}}(0,1)\\), i.e. each \\(Z_i\\) has pdf \\(f(z_i) = (2\\pi)^{-1/2}e^{-z_i^2/2}\\) for \\(i=1,\\dots,k\\). Then, \\[X = Z_1^2 + \\dots + Z_k^2 = \\sum_{i=1}^k Z_i^2\\] follows a \\(\\chi^2\\)-distribution with \\(k\\in\\mathbb{N}\\) degrees of freedom. We write \\(X \\sim \\chi^2_k\\). Out of curiosity, the pdf of a \\(\\chi^2_k\\) distribution is \\(f(x) = Cx^{k/2-1}e^{-x/2}\\), where the normalising constant \\(C\\) is equal to \\(2^{-k/2}\\Gamma^{-1}(k/2)\\) (\\(\\Gamma(\\cdot)\\) is the gamma function). The form of the pdf is less important to know than the definition of \\(\\chi^2_k\\) distribution given in Definition 3.6. Here are some important properties of the \\(\\chi^2_k\\) distribution. \\(X\\) has support over \\([0,\\infty)\\). \\(\\mathop{\\mathrm{E}}(X)=k\\). \\(\\mathop{\\mathrm{Var}}(X) = 2k\\). If \\(X_1\\sim\\chi^2_{k_1}\\) and \\(X_2\\sim\\chi^2_{k_2}\\), and \\(X_1 \\perp X_2\\), then \\(X_1+X_2\\sim \\chi^2_{k_1+k_2}\\). There is a question at the end of this chapter where you will prove the above statements. Pdf of \\(\\chi^2_k\\) Probabilities such as \\[\\Pr(\\chi_k^2 \\leq x) = \\int_0^x f_X(\\tilde x) \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!\\tilde x\\] where \\(f_X\\) is the pdf of \\(\\chi^2_k\\) cannot be found in closed form. Instead, the integral is calculated using computer approximations for the integral above. In R, pchisq(2, df = 3) ## [1] 0.4275933 Alternatively, statistical tables are used. You will find tables for percentiles of the \\(\\chi^2\\)-distribution. That is, you are able to find the value of \\(x:=\\chi^2_k(\\alpha)\\) such that \\[\\Pr(\\chi_k^2 \\leq x) = \\int_0^x f_X(\\tilde x) \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!\\tilde x = A = 1-\\alpha\\] for various values of \\(A\\) and \\(k\\). Example 3.8 Let \\(Y_1,\\dots,Y_n\\,\\overset{\\text{iid}}{\\sim}\\,\\mathop{\\mathrm{N}}(\\mu,\\sigma^2)\\). Then, \\(Z_i = \\frac{Y_i-\\mu}{\\sigma} \\sim \\mathop{\\mathrm{N}}(0,1)\\), and hence \\[\\frac{1}{\\sigma^2} \\sum_{i=1}^n (Y_i-\\mu)^2 = \\sum_{i=1}^n Z_i^2 \\sim \\chi^2_n .\\] Note that \\[\\begin{equation} \\frac{1}{\\sigma^2} \\sum_{i=1}^n (Y_i-\\mu)^2 = \\frac{1}{\\sigma^2} \\sum_{i=1}^n (Y_i-\\bar Y_n)^2 + \\frac{n}{\\sigma^2} (\\bar Y_n -\\mu)^2. \\tag{3.3} \\end{equation}\\] Since \\(\\bar Y_n \\sim \\mathop{\\mathrm{N}}(\\mu, \\sigma^2/n)\\), it must be that \\(\\frac{n}{\\sigma^2} (\\bar Y_n -\\mu)^2 \\sim \\chi^2_1\\). Thus, by the properties of the \\(\\chi^2\\)-distribution, the decomposition in (3.3) may be written as \\(\\chi^2_n = \\chi^2_{n-1} + \\chi^2_{1}\\). In particular, we now know \\[\\frac{1}{\\sigma^2} \\sum_{i=1}^n (Y_i-\\bar Y_n)^2 \\sim \\chi^2_{n-1} .\\] 3.6.2 Student’s \\(t\\)-distribution This is another important distribution in statistics, because: The \\(t\\)-test is a widely used distribution for statistical tests in many application. Confidence intervals for normal mean with unknown variance may be constructed based on the \\(t\\)-distribution. Definition 3.7 ($t$-distribution) Suppose we have two r.v. \\(Z\\sim\\mathop{\\mathrm{N}}(0,1)\\) and \\(X\\sim\\chi^2_k\\) such that \\(X\\) and \\(Z\\) are independent. Then, the distribution of the random variable \\[T = \\frac{Z}{\\sqrt{X/k}}\\] is called the \\(t\\)-distribution with \\(k\\in\\mathbb{N}\\) degrees of freedom. We write \\(T\\sim t_k\\). The pdf for \\(T \\sim t_k\\) is given by \\[f(t) \\propto \\left(1 + \\frac{t^2}{k} \\right)^{-\\frac{k+1}{2}},\\] but once again the actual form of the pdf is not as important as the definition of the \\(t\\)-distribution. Some important properties of the \\(t\\)-distribution: \\(T\\) is continuous and symmetric over \\((-\\infty,\\infty)\\). \\(\\mathop{\\mathrm{E}}(T)=0\\), provided \\(\\mathop{\\mathrm{E}}(|T|) &lt; \\infty\\) (\\(k&gt;1\\)). \\(\\mathop{\\mathrm{Var}}(T) = \\frac{k}{k-2}\\). Technically, \\(k\\in\\mathbb{R}\\), but we will usually deal with \\(k\\in\\mathbb{N}\\). Pdf of \\(t_k\\) Figure 3.1: William Sealy Gosset. 13 June 1876 – 16 October 1937. The \\(t\\)-distribution20 has what is known as heavy tails. That is, if \\(T\\sim t_k\\), its mgf is undefined and hence \\(\\mathop{\\mathrm{E}}(|T|^k) = \\infty\\). Comparing this to the normal distribution: \\(X\\sim\\mathop{\\mathrm{N}}(\\mu,\\sigma^2)\\), \\(\\mathop{\\mathrm{E}}(|X|^k) &lt; \\infty\\) for any \\(k&gt;0\\). This ‘heavy-tails’ property is a useful property in modelling abnormal phenomena or outliers (e.g. in financial or insurance data). c.f. “robust statistics” The connection between the \\(t_k\\) distribution and the normal distribution, is that the \\(t_k\\) actually approaches the standard normal as the degrees of freedom increases. Lemma 3.6 \\(t_k \\xrightarrow{\\text{D}} \\mathop{\\mathrm{N}}(0,1)\\) as \\(k\\to\\infty\\). Proof. If \\(X\\sim\\chi^2_k\\), then by definition \\(X = Z_1^2 + \\dots + Z_k^2\\), where \\(Z_i\\,\\overset{\\text{iid}}{\\sim}\\,\\mathop{\\mathrm{N}}(0,1)\\). By the LLN, \\[\\frac{X}{k} = \\frac{Z_1^2 + \\dots + Z_k^2}{k} \\xrightarrow{\\text{P}} \\mathop{\\mathrm{E}}(Z_1^2) = 1.\\] as \\(k\\to\\infty\\). Therefore, \\(\\sqrt{X/k} \\xrightarrow{\\text{P}} 1\\), and in particular, \\[T = \\frac{Z}{\\sqrt{X/k}} \\xrightarrow{\\text{D}} \\mathop{\\mathrm{N}}(0,1)\\] following Slutzky’s theorem. 3.6.3 Proof of Theorem 3.5 Back to this theorem. Let’s prove it. follows directly from properties of normal distributions, and earlier we showed that \\(\\frac{1}{\\sigma^2}\\sum_{i=1}^n (X_i - \\bar X)^2 \\sim \\chi^2_{n-1}\\) which settles iii. To prove i., consider any \\(X_j\\), \\(j\\in\\{1,\\dots,n\\}\\) and \\(\\mathop{\\mathrm{Cov}}( X_j - \\bar X, \\bar X)\\): \\[\\begin{aligned} \\mathop{\\mathrm{Cov}}( X_j - \\bar X, \\bar X) &amp;= \\mathop{\\mathrm{Cov}}( X_j, \\bar X) - \\mathop{\\mathrm{Cov}}(\\bar X, \\bar X) \\\\ &amp;= \\mathop{\\mathrm{Cov}}\\left(X_j, \\frac{1}{n}\\sum_{i=1}^n X_i\\right) - \\mathop{\\mathrm{Var}}(\\bar X) \\\\ &amp;= \\frac{1}{n} \\sum_{i=1}^n \\mathop{\\mathrm{Cov}}(X_j,X_i) - \\sigma^2/n = \\sigma^2 /n - \\sigma^2 /n = 0 \\end{aligned}\\] Since the covariance is zero and they are normal, they are independent. Following this, if \\(\\bar X\\) is independent of \\(X_j - \\bar X\\) for any \\(j\\), it stands to reason that \\(\\bar X\\) is also independent of \\(\\tilde {\\boldsymbol X}= (X_1-\\bar X,\\dots,X_n-\\bar X)^\\top\\), and also of \\[\\tilde{\\boldsymbol X}^\\top\\tilde{\\boldsymbol X}= \\begin{pmatrix} X_1-\\bar X &amp; \\cdots &amp; X_n-\\bar X \\end{pmatrix} \\begin{pmatrix} X_1-\\bar X \\\\ \\vdots \\\\ X_n-\\bar X \\end{pmatrix} = \\sum_{i=1}^n (X_i - \\bar X)^2 = (n-1)S^2,\\] and thus also of \\(S^2\\). Here we used the fact that if \\(X \\perp Y_i\\), then \\(g(X)\\perp g(Y_i)\\), and also \\(g(X) \\perp \\{g(Y_1) + \\cdots + g(Y_n)\\}\\). Finally, putting everything together, \\[\\frac{\\overbrace{\\sqrt n(\\bar X - \\mu)/\\sigma}^{\\mathop{\\mathrm{N}}(0,1)}}{\\sqrt{\\frac{\\overbrace{(n-1)S^2/\\sigma^2}^{\\chi^2_{n-1}}}{n-1}}} = \\frac{\\bar X - \\mu}{S/\\sqrt{n}} = \\frac{\\bar X - \\mu}{\\text{SE}(\\bar X)} \\sim t_{n-1}.\\] This is why for normal distributions where \\(\\sigma^2\\) is unknown, and is estimated by the unbiased sample variance \\(s^2\\), the standardised sample mean follows a \\(t\\)-distribution! This gives rise to the \\(t\\)-test. 3.6.4 \\(F\\)-distribution The \\(F\\)-distribution is another notable distribution in statistics. It commonly arises as the null distribution of a test statistic, particularly in the analysis of variance (ANOVA). Definition 3.8 ($F$-distribution) Let \\(X_1 \\sim \\chi^2_{k_1}\\) and \\(X_2 \\sim \\chi^2_{k_2}\\). Then, the distribution of \\[Y = \\frac{X_1/k_1}{X_2/k_2}\\] is called the \\(F\\)-distribution with \\((k_1,k_2)\\) degrees of freedom. We write \\(Y\\sim F_{k_1,k_2}\\). Not even going to bother writing down the pdf! See for yourself: https://en.wikipedia.org/wiki/F-distribution. Remember the definition, though. Some important properties of the \\(F\\)-distribution: \\(Y\\) is continuous and has support over \\([0,\\infty)\\), provided \\(k_1&gt;1\\). \\(\\mathop{\\mathrm{E}}(Y)=\\frac{k_2}{k_2 - 2}\\), provided \\(k_2&gt;2\\). \\(\\mathop{\\mathrm{Var}}(Y) = \\frac{2k_2^2(k_1+k_2-2)}{k_1(k_2-2)^2(k_2-4)}\\), provided \\(k_2&gt;4\\). Technically, \\(k_1,k_2\\in\\mathbb{R}_{&gt;0}\\), but we will usually deal with \\(k_1,k_2\\in\\mathbb{N}\\). If \\(Y\\sim F_{k_1,k_2}\\), then \\(Y^{-1}\\sim F_{k_2,k_1}\\). If \\(T\\sim t_{k}\\), then \\(T^2 \\sim F_{1,k}\\). Attempt to prove some of these in the exercises! 3.6.5 The analysis of variance The ANOVA, despite its name, is a (collection of) methods used to analyse differences among group means in a sample. The setup is as follows: Let \\(Y_{ij}\\sim\\mathop{\\mathrm{N}}(\\mu_j,\\sigma^2)\\), \\(i=1,\\dots,n_j\\) and \\(j=1,\\dots,m\\) with both \\(\\mu_j\\) and \\(\\sigma^2\\) unknown. Let \\(n=\\sum_{j=1}^m n_j\\) be the total sample size. Define the grand mean \\(\\bar Y = n^{-1}\\sum_{i,j} Y_{ij}\\); and the group means \\(\\bar Y_j = n_j^{-1} \\sum_{i=1}^{n_j} Y_{ij}\\), \\(j=1,\\dots,m\\). Consider the “total sum of squares” \\(TSS = \\sum_{i,j}(Y_{ij} - \\bar Y)^2\\), which can be decomposed into \\[TSS = {\\color{gray!70}\\overbrace{\\color{black}\\sum_{i,j} (Y_{ij} - \\bar Y_j)^2}^{WSS}} + {\\color{gray!70}\\overbrace{\\color{black}\\sum_{j} n_j(\\bar Y_j - \\bar Y)^2}^{BSS}}\\] where \\(WSS\\) is the “within sum of squares” (how much variation among individuals in each group); and \\(BSS\\) is the “between sum of squares” (how much variation in the mean among groups). There is a concept of degrees of freedom: \\(n-1\\) in the TSS, \\(m-1\\) in the BSS, and therefore \\(n-m\\) in the WSS. This gives rise to the ANOVA table: Source SS d.f. MSS F-statistic Between \\(\\sum_{j} n_j(\\bar Y_j - \\bar Y)^2\\) \\(m-1\\) \\(\\frac{\\sum_{j} n_j(\\bar Y_j - \\bar Y)^2}{m-1}\\) \\(\\frac{\\sum_{j} n_j(\\bar Y_j - \\bar Y)^2/(m-1)}{\\sum_{i,j} (Y_{ij} - \\bar Y_j)^2/(n-m)}\\) Within \\(\\sum_{i,j} (Y_{ij} - \\bar Y_j)^2\\) \\(n-m\\) \\(\\frac{\\sum_{i,j} (Y_{ij} - \\bar Y_j)^2}{n-m}\\) Total \\(\\sum_{i,j}(Y_{ij} - \\bar Y)^2\\) \\(n-1\\) Suppose we want to test the hypothesis that all group means are identical (i.e. \\(\\mu_j=\\mu, \\forall j\\)), what is the distribution of \\(F\\)? We have seen that \\[TSS/\\sigma^2 = \\frac{1}{\\sigma^2}\\sum_{i,j}(Y_{ij} - \\bar Y)^2 \\sim \\chi^2_{n-1}.\\] In fact, we can also show similarly that \\[WSS/\\sigma^2 =\\frac{1}{\\sigma^2}\\sum_{i,j} (Y_{ij} - \\bar Y_j)^2 \\sim \\chi^2_{n-m}.\\] Using these two facts, we deduce that \\[BSS/\\sigma^2=\\frac{1}{\\sigma^2}\\sum_{j} n_j(\\bar Y_j - \\bar Y)^2 \\sim \\chi^2_{m-1}\\] from the property of \\(\\chi^2\\)-distributions. So now, \\[\\begin{aligned} F = \\frac{\\text{mean }BSS}{\\text{mean }WSS} = \\frac{ \\overbrace{1/\\sigma^2\\sum_{j} n_j(\\bar Y_j - \\bar Y)^2}^{\\chi^2_{m-1}} / (m-1)}{ \\overbrace{1/\\sigma^2\\sum_{i,j}(Y_{ij} - \\bar Y_j)^2}^{\\chi^2_{n-m}} / (n-m)} \\end{aligned}\\] is a ratio of two \\(\\chi^2\\)-distributions, which means that \\(F\\) follows an \\(F\\)-distribution with \\((m-1,n-m)\\) degrees of freedom. Figure 3.2: Sir Ronald Aylmer Fisher. 17 February 1890 – 29 July 1962. Explore the \\(t\\)-distribution vs normal distribution here: https://eripoll12.shinyapps.io/t_Student/↩︎ "],["exercises-2.html", "3.7 Exercises", " 3.7 Exercises In this exercise, you will prove the central limit theorem using moment generating functions. Let \\(X_1,X_2,\\dots\\) be a sequence of iid random variables with mean \\(\\mathop{\\mathrm{E}}(X_i)=\\mu\\) and variance \\(\\mathop{\\mathrm{Var}}(X_i)=\\sigma^2&gt;0\\), and whose mgfs exist in a neighbourhood of 0 (i.e., \\(M_{X_i}(t)\\) exists for \\(|t|&lt;h\\) for some positive \\(h\\)). Define \\(Y_i=(X_i-\\mu)/\\sigma\\), and let \\(M_Y(t)\\) denote the common mgf of the \\(Y_i\\)s. Show that \\(M_Y^{(0)} = 1\\), \\(M_Y^{(1)} = 0\\), and \\(M_Y^{(2)} = 1\\). Using the properties of mgfs, show that the mgf of \\(Z := \\frac{\\sqrt n(\\bar X_n - \\mu)}{\\sigma}\\) is given by \\[\\begin{equation}\\label{eq:clt} M_Z(t) = \\big(M_Y(t/\\sqrt n) \\big)^n. \\end{equation}\\] Recall that the Taylor series expansion of a real function \\(g(x)\\) around \\(a\\in\\mathbb{R}\\) is given by the power series \\[ g(x) = \\sum_{k=0}^\\infty \\frac{g^{(k)}}{k!}(x-a)^k = g(a) + \\frac{g&#39;(a)}{1!}(x-a) + \\frac{g&#39;&#39;(a)}{2!}(x-a)^2 + \\cdots . \\] By Taylor expanding \\(M_Y(t/\\sqrt n)\\) about 0, show that \\[ M_Y(t/\\sqrt n) = 1 + \\frac{(t/\\sqrt n)^2}{2!} + R_n \\] where \\(R_n\\) is a remainder term from the Taylor series. Hence, using the fact that \\(nR_n\\to 0\\) as \\(n\\to\\infty\\), show that the limiting distribution of \\(Z\\) is \\(\\mathop{\\mathrm{N}}(0,1)\\). Suppose we have a sequence \\(X_1,X_2,\\dots\\) of iid random variables with \\(\\mathop{\\mathrm{E}}(X_i)=\\mu\\) and \\(\\mathop{\\mathrm{Var}}(X_i)=\\sigma^2&lt;\\infty\\). Show that the statistic \\(S_n^2 = \\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar X_n)^2\\), where \\(\\bar X_n\\) is the sample mean, is an unbiased and consistent estimator for \\(\\sigma^2\\) (i.e. it converges in probability to \\(\\sigma^2\\)). Hint: Show that \\(S_n^2 = c_n n^{-1}\\sum_{i=1}^n X_i^2 - d_n \\bar X_n^2\\) where \\(c_n,d_n \\to 1\\) as \\(n\\to\\infty\\). Apply the LLN to \\(n^{-1}\\sum_{i=1}^n X_i^2\\) and to \\(\\bar X_n^2\\). Then use Slutzky’s theorem. Using Slutzky’s theorem, show that \\[ \\frac{\\sqrt n(\\bar X_n - \\mu)}{S_n} \\xrightarrow{\\text{ D }} \\mathop{\\mathrm{N}}(0,1). \\] Hint: First argue that \\(\\sigma/S_n \\xrightarrow{\\text{P}} 1\\) using Slutzky’s theorem. You may use the fact that \\(g(x)=c/\\sqrt{x}\\) is continuous for \\(x&gt;0\\), where \\(c\\in\\mathbb{R}_{&gt;0}\\) is a constant. Then use the CLT. Suppose that \\(\\bar X\\) and \\(S^2\\) are calculate from an iid random sample \\(X_1,\\dots,X_n\\) with \\(\\mathop{\\mathrm{Var}}(X_i)=\\sigma^2\\). We know that \\(\\mathop{\\mathrm{E}}(S^2)=\\sigma^2\\) from Q2(a). Prove that \\(\\mathop{\\mathrm{E}}(S)\\leq \\sigma\\). Hint: Use Jensen’s inequality. Fill in the details of the proof of the delta method: Suppose that \\(X_n\\) is a sequence of r.v.s satisfying \\(\\sqrt n (X_n-\\mu)/\\sigma \\xrightarrow{\\text{D}}\\mathop{\\mathrm{N}}(0,1)\\), and let \\(g\\) be a differentiable function s.t. \\(g&#39;(\\mu)\\neq 0\\). Show that \\(X_n \\xrightarrow{\\text{P}} \\mu\\) in probability. Show, using Slutzky’s theorem, that \\[ \\frac{\\sqrt n(g(X_n) - g(\\mu)) }{|g&#39;(\\mu)|\\sigma} \\xrightarrow{\\text{ D }} \\mathop{\\mathrm{N}}(0,1). \\] Hint: Use the Taylor approximation \\(g(X_n)\\approx g(\\mu)+g&#39;(\\mu)(X_n-\\mu)\\). Let \\(\\{X_1,\\dots,X_n\\}\\) be a random sample from a \\(\\mathop{\\mathrm{N}}(\\mu,\\sigma^2)\\) population. Let \\(M=\\sum_{i=1}^n (X_i - \\bar X)^2\\), where \\(\\bar X\\) is the sample mean. Work out the distribution of \\(M/\\sigma^2\\). Let \\(\\alpha = 0.05\\). Using the \\(\\chi^2\\) probability tables, determine the values of \\(\\chi^2_{14}(\\alpha/2)\\) and \\(\\chi^2_{14}(1-\\alpha/2)\\), i.e. the top and bottom \\(\\alpha/2\\) point of the \\(\\chi^2_{14}\\) distribution where \\(\\Pr\\big(Y &gt; \\chi^2_{k}(a)\\big) = a\\) when \\(Y\\sim\\chi^2_k\\). Suppose that we plan to take a random sample of size \\(n\\) from a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma=2\\). Suppose \\(\\mu=4\\) and \\(n=20\\). What is the probability that the mean \\(\\bar X\\) of the sample is greater than 5? What is the probability that \\(\\bar X\\) is smaller than 3? What is \\(\\Pr(|\\bar X - \\mu| \\leq 1)\\) in this case? How large should \\(n\\) be in order that \\(\\Pr(|\\bar X - \\mu| \\leq 0.5) \\geq 0.95\\) for every possibly value of \\(\\mu\\)? It is claimed that the true value of \\(\\mu\\) is 5 in a population. A random sample of size \\(n=100\\) is collected from this population, and the mean for this sample is \\(\\bar X=5.8\\). Based on the result in (b), what would you conclude from this value of \\(\\bar X\\)? In all of the following sub-questions, use only probability tables and distributional properties of the relevant random variables to calculate the required probabilities. If \\(Z\\) is a random variable with a standard normal distribution, what is \\(\\Pr(Z^2 &lt; 3.841)\\)? Suppose that \\(X_1\\) and \\(X_2\\) are independent \\(\\mathop{\\mathrm{N}}(0,4)\\) random variables. Compute \\(\\Pr(X_1^2 &lt; 36.84 - X_2^2)\\). Suppose that \\(X_1,X_2,X_3\\,\\overset{\\text{iid}}{\\sim}\\,\\mathop{\\mathrm{N}}(0,1)\\), while \\(Y\\) independently follows a \\(\\chi^2_5\\) distribution. Compute \\(\\mathop{\\mathrm{P}}(X_1^2+X_2^2 &lt; 7.236Y - X_3^2)\\). Let \\(X_i\\), \\(i=1,2,3\\) be independent with \\(\\mathop{\\mathrm{N}}(i,i^2)\\) distributions. For each of the following situations, use the \\(X_i\\)s to construct a statistic with the indicated distribution: \\(\\chi^2\\)-distribution with 3 degrees of freedom; \\(t\\)-distribution with 2 degrees of freedom; and \\(F\\)-distribution with 1 and 2 degrees of freedom. Let \\(\\{Y_{ij}\\}\\) be sample from \\(\\mathop{\\mathrm{N}}(\\mu_j,\\sigma^2)\\), \\(i=1,\\dots,n_j\\) and \\(j=1,\\dots,m\\). In total there are \\(n=\\sum_{j=1}^m n_j\\) samples. Further, let \\(S = \\sum_{i=1}^{n_j}\\sum_{j=1}^m (Y_{ij} - \\bar Y)^2\\), where \\(\\bar Y = \\frac{1}{n}\\sum_{i=1}\\sum_{j=1}^m Y_{ij}\\). Define the sample group means to be \\(\\bar Y_j = \\frac{1}{n_j}\\sum_{i=1}^{n_j} Y_{ij}\\). Add and subtract the sample group mean \\(\\bar Y_j\\) into the squared sum in \\(S\\) to show that \\[ \\sum_{i=1}^{n_j}\\sum_{j=1}^m (Y_{ij} - \\bar Y)^2 = \\sum_{i=1}^{n_j}\\sum_{j=1}^m (Y_{ij} - \\bar Y_j)^2 + \\sum_{j=1}^m n_j (\\bar Y_j-\\bar Y)^2 \\] What is the distribution of \\(\\bar Y\\) and \\(\\bar Y_j\\)? Assuming that \\(\\mu_j=\\mu\\), for all \\(j=1,\\dots,m\\) and using your answer to (b), determine then the following distributions \\(\\frac{1}{\\sigma^2}\\sum_{i=1}^{n_j}\\sum_{j=1}^m (Y_{ij} - \\mu)^2\\) \\(\\frac{n}{\\sigma^2}(\\bar Y - \\mu)^2\\) \\(\\frac{1}{\\sigma^2}\\sum_{i=1}^{n_j}\\sum_{j=1}^m (Y_{ij} - \\bar Y)^2\\) \\(\\frac{1}{\\sigma^2}\\sum_{j=1}^m n_j (\\bar Y_j-\\mu)^2\\) \\(\\frac{1}{\\sigma^2}\\sum_{i=1}^{n_j}\\sum_{j=1}^m (Y_{ij} - \\bar Y_j)^2\\) Hint: Use the sum of squares decomposition with \\(\\bar Y\\) and \\(\\bar Y_j\\), and then use the properties of \\(\\chi^2\\)-distributions. This question relates to the normal approximations of several commonly used distributions. Let \\(X_1,\\dots,X_n\\,\\overset{\\text{iid}}{\\sim}\\,f(x)\\). Answer the following questions in cases where \\(f(x)\\) is the pmf/pdf of \\(\\mathop{\\mathrm{Bern}}(p)\\), \\(\\mathop{\\mathrm{Poi}}(\\lambda/n)\\), and \\(\\Gamma(\\alpha/n,\\beta)\\): What is the distribution of \\(Y=\\sum_{i=1}^n X_i\\)? Using the CLT, what is the (approximate) distribution of \\(Y/n\\)? What can you then say about the distribution of \\(Y\\) as \\(n\\to\\infty\\)? Hand-in questions State and prove Chebyshev’s inequality for a random variable \\(X\\). [3 marks] Let \\(X\\sim\\chi^2_k\\). Prove that \\(\\mathop{\\mathrm{E}}(X) = k\\) and \\(\\mathop{\\mathrm{Var}}(X)=2k\\). [2 marks] If \\(X_1\\sim\\chi^2_{k_1}\\), \\(X_2\\sim\\chi^2_{k_2}\\), and \\(X_1 \\perp X_2\\), show that \\(X_1+X_2 \\sim \\chi^2_{k_1+k_2}\\). [2 marks] Let \\(\\lambda_n=1/n\\) for \\(n=1,2,\\dots\\). Let \\(X_n\\sim\\mathop{\\mathrm{Poi}}(\\lambda_n)\\). Show that \\(X_n\\xrightarrow{\\text P}0\\) as \\(n\\to\\infty\\). [2 marks] Let \\(Y_n=nX_n\\). Show that \\(Y_n\\xrightarrow{\\text P}0\\) as \\(n\\to\\infty\\). [2 marks] Let \\(\\bar X_n\\) and \\(S_n^2\\) be the usual mean and variance statistics for the sample \\(X_1,\\dots,X_n\\). Suppose that a new observation \\(X_{n+1}\\) becomes available, show that \\(\\bar X_{n+1}=(X_{n+1} + n\\bar X_n) / (n+1)\\). [1 mark] \\(nS_{n+1}^2=(n-1)S_n^2 + n(X_{n+1}-\\bar X_n)^2 / (n+1)\\) [3 marks] "],["point-estimation.html", "Chapter 4 Point estimation", " Chapter 4 Point estimation Learning objectives By the end of this chapter, you will be able to: do this do that and this Readings Casella and Berger (2002) Chapter 6, sections 6.1, 6.2 (excluding 6.2.3 and 6.2.4), and 6.3 (excluding 6.3.2). Chapter 7, sections 7.1, 7.2 (excluding 7.2.3 and 7.2.4), and 7.3 (excluding 7.3.4). Chapter 10, sections 10.1 (excluding 10.1.4). Wasserman (2004) Chapter 6, sections 6.1, 6.2, 6.3.1 Chapter 9, sections 9.1–9.5, 9.7–9.9 Topics not covered here: Ancillary statistics, complete statistics, Basu’s theorem, the formal likelihood principle, Bayes estimators, the EM algorithm, loss function optimality, equivariance of MLE, (asymptotic) relative efficiency, bootstrap se, robustness, \\(M\\)-estimators. "],["the-likelihood.html", "4.1 The likelihood", " 4.1 The likelihood Consider a statistical model for a random vector \\({\\boldsymbol X}= ({\\boldsymbol X}_1,\\dots,{\\boldsymbol X}_n)^\\top\\) whose distribution depends on (an unknown) parameter \\(\\theta\\). Write \\(f({\\boldsymbol x}|\\theta)\\) for the joint pdf/pmf of \\({\\boldsymbol X}\\) when \\(\\theta\\) is known. Then, given \\({\\boldsymbol X}={\\boldsymbol x}\\) is observed, the function of \\(\\theta\\) defined by \\[ L(\\theta|{\\boldsymbol x}) = f({\\boldsymbol x}|\\theta) \\] is called the likelihood function for \\(\\theta\\) based on data \\({\\boldsymbol x}\\). Note the key distinction between \\(f\\), which is considered a function of \\({\\boldsymbol x}\\) (and, for example, must sum or integrate to 1) \\(L\\), which is considered a function of \\(\\theta\\). For any fixed value of \\(\\theta\\), say \\(\\theta=\\theta_1\\), \\(L(\\theta_1|{\\boldsymbol x})\\) is a statistic–a scalar-valued transformation of the observed values of \\({\\boldsymbol X}={\\boldsymbol x}\\). The purpose of \\(L(\\theta|{\\boldsymbol x})\\) is to compare the plausibility of different candidate values of \\(\\theta\\), given the observed data \\({\\boldsymbol x}\\). If \\(L(\\theta_1|{\\boldsymbol x}) &gt; L(\\theta_2|{\\boldsymbol x})\\), then the data \\({\\boldsymbol x}\\) were more likely to occur under the hypothesis that \\(\\theta=\\theta_1\\) than under the hypothesis that \\(\\theta=\\theta_2\\). In that sense, \\(\\theta_1\\) is a more plausible value than \\(\\theta_2\\) for the unknown parameter \\(\\theta\\). Example 4.1 Consider a sequence of \\(n\\) coin tosses, and let \\(X_i\\) denote the outcome of the \\(i\\)th coin toss. Assume that \\(X_i\\sim\\mathop{\\mathrm{Bern}}(p)\\), where \\(p\\) is the probability of heads. We know that the total number of heads \\(\\sum_{i=1}^n X_i\\) is distributed \\(\\mathop{\\mathrm{Bin}}(n,p)\\). Suppose the outcome of \\(n=10\\) coin tosses happens to be \\[{\\boldsymbol x}= \\{H, T, H, T, T, H, H, T, H, H \\}.\\] Then \\(L(0.6|{\\boldsymbol x}) &gt; L(0.5|{\\boldsymbol x})\\). 4.1.1 Calculating the likelihood In R, the function dbinom() computes the pmf for the binomial distribution. That is, suppose that we have \\(X\\sim\\mathop{\\mathrm{Bin}}(10,0.6)\\) and we wanted to calculate \\(\\Pr(X=x)\\) we type dbinom(x = 0:10, size = 10, prob = 0.6) %&gt;% round(digits = 4) ## [1] 0.0001 0.0016 0.0106 0.0425 0.1115 0.2007 0.2508 0.2150 0.1209 0.0403 ## [11] 0.0060 Since \\(L(\\theta|{\\boldsymbol x}) = f({\\boldsymbol x}|\\theta)\\), we use the same dbinom() to calculate the likelihood, except now we are interested in the value of the likelihood of a range of parameter values \\(p\\in[0,1]\\) given a particular occurrence (e.g. getting 6 heads): dbinom(x = 6, size = 10, prob = seq(0, 1, by = 0.1)) %&gt;% round(digits = 4) ## [1] 0.0000 0.0001 0.0055 0.0368 0.1115 0.2051 0.2508 0.2001 0.0881 0.0112 ## [11] 0.0000 Plot of the likelihood 4.1.2 Likelihood ratio Definition 4.1 (Likelihood ratio) The relative plausibility of candidate parameter values, \\(\\theta_1\\) and \\(\\theta_2\\) say, is measured by the likelihood ratio \\[ \\frac{L(\\theta_1|{\\boldsymbol x})}{L(\\theta_2|{\\boldsymbol x})} \\] Interpretation: for example, if \\(\\frac{L(\\theta_1|{\\boldsymbol x})}{L(\\theta_2|{\\boldsymbol x})} = 10\\), then the observed data \\({\\boldsymbol x}\\) were 10 times more likely under truth \\(\\theta_1\\) than under truth \\(\\theta_2\\). The use of likelihood ratios to compare the plausibility of different \\(\\theta\\) values means that any constant factor in the likelihood–that is, any factor not depending on \\(\\theta\\)–can be neglected. Example 4.2 Suppose \\(X_i\\sim\\mathop{\\mathrm{Poi}}(\\lambda)\\) independently (\\(i=1,\\dots,n\\)) and we have observed \\({\\boldsymbol X}={\\boldsymbol x}\\). Here, \\[\\begin{align*} L(\\lambda|{\\boldsymbol x}) = f(x_1,\\dots,x_n|\\lambda) &amp;= \\prod_{i=1}^n \\frac{e^{-\\lambda}\\lambda^{x_i}}{x_i!} \\\\ &amp;= \\text{const.}\\times e^{-n\\lambda}\\lambda^{\\sum_{i=1}^n x_i} \\end{align*}\\] The product \\(\\frac{1}{x_1!}\\cdots \\frac{1}{x_n!}\\) are not needed, since they do not involve \\(\\lambda\\). The non-constant part of the likelihood depends on \\({\\boldsymbol x}\\) only through \\(T({\\boldsymbol x})=\\sum_{i=1}^n x_i\\). As a remark, the function \\(T({\\boldsymbol x})=\\sum_{i=1}^n x_i\\) is called a sufficient statistic for \\(\\theta\\): the value of \\(T({\\boldsymbol x})\\) is all that is needed in order to compute the likelihood (ignoring constants)! 4.1.3 Log likelihood In practice, especially when observations are independent, it is usually most convenient to work with the (natural) logarithm of the likelihood, \\[ l(\\theta) = \\log L(\\theta|{\\boldsymbol x}), \\] since this converts products into sums, which are easier to handle. Example 4.3 \\(n\\) independent Poisson continued. \\[\\begin{align*} l(\\lambda|{\\boldsymbol x}) &amp;= \\log \\prod_{i=1}^n \\frac{e^{-\\lambda}\\lambda^{x_i}}{x_i!} \\\\ &amp;= \\text{const.}-n\\lambda + \\left( \\sum_{i=1}^n x_i \\right)\\log\\lambda \\end{align*}\\] In terms of the log likelihood, then, any two candidate values of \\(\\theta\\) are compared via the log-likelihood-ratio, \\[ \\log \\frac{L(\\theta_1|{\\boldsymbol x})}{L(\\theta_2|{\\boldsymbol x})} = l(\\theta_1) - l(\\theta_2) \\] On the log scale, it is additive constants that can be ignored. "],["sufficiency.html", "4.2 Sufficiency", " 4.2 Sufficiency We have introduced the notion of sufficient statistic already, informally, as a data summary that provides all that is needed in order to compute the likelihood. Here we will give a formal definition, and then prove the factorization theorem, which provides a straightforward way of checking whether a particular statistic is sufficient allows a sufficient statistic, to be identified by simple inspection of the likelihood function (as we did in the example of \\(n\\) Poissons) Definition 4.2 A statistic \\(T({\\boldsymbol X})\\) is said to be a sufficient statistic for \\(\\theta\\) if the conditional distribution of \\({\\boldsymbol X}\\), given the value of \\(T({\\boldsymbol X})\\), does not depend on \\(\\theta\\). In this precise sense, a sufficient statistic \\(T({\\boldsymbol X})\\) carries all of the information about \\(\\theta\\) that is contained in \\({\\boldsymbol X}\\). The notion is that, given the observed value \\(T({\\boldsymbol x})\\) of \\(T({\\boldsymbol X})\\), all further knowledge about \\({\\boldsymbol x}\\) is uninformative about \\(\\theta\\). In particular, this is useful for data reduction: if \\(T({\\boldsymbol X}) \\in \\mathbb{R}\\) is a scalar sufficient statistic, then all of the information in \\(\\{X_1,\\dots,X_n\\}\\) relating to \\(\\theta\\) is contained in the single-number summary \\(T({\\boldsymbol X})\\). 4.2.1 The factorisation theorem It is difficult to use the definition to check if a statistic is sufficient or to find a sufficient statistic. Luckily, there is a theorem that makes it easy to find sufficient statistics. Theorem 4.1 A statistic \\(T({\\boldsymbol X})\\) is sufficient for \\(\\theta\\) if and only if, for all \\({\\boldsymbol x}\\) and \\(\\theta\\), \\[ f({\\boldsymbol x}|\\theta) = h({\\boldsymbol x})g(T({\\boldsymbol x})|\\theta) \\] That is to say, the density \\(f\\) can be factored into a product such that one factor \\(h\\) does not depend on \\(\\theta\\), and the other factor, which does depend on \\(\\theta\\), depends on \\({\\boldsymbol x}\\) only through the sufficient statistic \\(T({\\boldsymbol x})\\). Example 4.4 Let \\(X_1,\\dots,X_n\\) be an independent random sample from \\(\\mathop{\\mathrm{N}}(\\mu,1)\\). The pdf of \\({\\boldsymbol X}\\) can be written \\[\\begin{align*} f({\\boldsymbol x}|\\mu) &amp;= \\frac{1}{(2\\pi)^{n/2}}\\exp\\left(-\\frac{1}{2}\\sum_{i=1}^n(x_i-\\mu)^2 \\right) \\\\ &amp;= \\frac{1}{(2\\pi)^{n/2}}\\exp\\left(-\\frac{1}{2}\\sum_{i=1}^n(x_i-\\bar x + \\bar x -\\mu)^2 \\right) \\\\ &amp;= \\underbrace{\\frac{1}{(2\\pi)^{n/2}}\\exp\\left(-\\frac{1}{2}\\sum_{i=1}^n(x_i-\\bar x)^2 \\right)}_{h({\\boldsymbol x})} \\hspace{0.5em} \\underbrace{\\exp\\left(-\\frac{n}{2}(\\bar x - \\mu)^2 \\right) \\vphantom{ \\frac{1}{(2\\pi)^{n/2}}\\exp\\left(-\\frac{1}{2}\\sum_{i=1}^n(x_i-\\bar x)^2 \\right) }}_{g(\\bar x|\\mu)} \\end{align*}\\] Therefore, \\(\\bar X\\) is a sufficient statistic. Example 4.5 A town has bus routes numbered \\(1,2,\\dots,\\theta\\), with \\(\\theta\\) being unknown. Naqiyyah spends a day observing bus numbers and collects data \\(X_i\\), \\(i=1,\\dots,n\\), representing them. Each \\(X_i\\) has pmf \\(f(x|\\theta) = \\Pr(X=x) = 1/\\theta\\), so the joint pmf (assuming independence of the observations) is \\[ f({\\boldsymbol x}|\\theta) = \\begin{cases} \\frac{1}{\\theta^n} &amp;\\max(x_1,\\dots,x_n) \\leq \\theta \\\\ 0 &amp;\\text{otherwise} \\end{cases} \\] Hence, if we let \\(T({\\boldsymbol x}) = \\max(x_1,\\dots,x_n)\\) then \\[ f({\\boldsymbol x}|\\theta) = \\overbrace{1 \\vphantom{\\frac{\\mathop{\\mathrm{\\unicode{x1D7D9}}}_{t \\leq \\theta}(t)}{\\theta^n}} }^{h({\\boldsymbol x})} \\cdot \\overbrace{\\frac{\\mathop{\\mathrm{\\unicode{x1D7D9}}}_{t \\leq \\theta}(t)}{\\theta^n}}^{g(t|\\theta))}, \\] which implies that \\(T({\\boldsymbol X})=\\max(X_1,\\dots,X_n)\\) is a sufficient statistic for \\(\\theta\\). 4.2.2 Minimal sufficient statistic There clearly is no unique sufficient statistic in any problem. For if \\(T({\\boldsymbol X})\\) is a scalar sufficient statistic, then, for example \\(s(T({\\boldsymbol X}))\\) is sufficient, for every1-1 function \\(s(\\cdot)\\). The pair \\(\\{T({\\boldsymbol X}),X_1\\}\\) is sufficient too. The full data set \\(\\{X_1,\\dots,X_n\\}\\) is always (trivially) sufficient. Use the factorisation theorem to check these assertions, or convince yourself with suitable examples! The idea of a minimal sufficient statistic is to eliminate redundancy of the kind evident in ii. or iii. (but not i.) above, in order to achieve maximal reduction of the data from \\({\\boldsymbol X}\\) to \\(T({\\boldsymbol X})\\). Definition 4.3 (Minimal sufficient statistic) A sufficient statistic \\(S({\\boldsymbol x})\\) is said to be minimal sufficient if, for any other sufficient statistic \\(T({\\boldsymbol x})\\), \\(S({\\boldsymbol X})\\) is a function of \\(T({\\boldsymbol X})\\). I.e., there exists a function \\(k\\) such that \\(S({\\boldsymbol x})=k(T({\\boldsymbol x}))\\). Intuitively, a minimal sufficient statistic most efficiently captures all possible information about the parameter \\(\\theta\\). The definition is clear enough in its meaning, but is not constructive: it does not help us to find a minimal sufficient statistic in any given situation. For this, we have the following theorem. Theorem 4.2 (Lehmann-Scheffé) \\(T({\\boldsymbol x})\\) is minimal sufficient if for every sample points \\({\\boldsymbol x}\\) and \\({\\boldsymbol y}\\), \\[ \\frac{f({\\boldsymbol x}|\\theta)}{f({\\boldsymbol y}|\\theta)} \\text{ is constant in $\\theta$ } \\Leftrightarrow T({\\boldsymbol x}) = T({\\boldsymbol y}) \\] Example 4.6 Consider the r.v.s \\(X_1,\\dots,X_n \\,\\overset{\\text{iid}}{\\sim}\\,\\mathop{\\mathrm{Unif}}(\\theta,\\theta+1)\\). The joint pdf of \\({\\boldsymbol X}\\) is \\[ f({\\boldsymbol x}|\\theta) = \\begin{cases} 1 &amp; \\theta &lt; x_1,\\dots,x_n &lt; \\theta+1 \\\\ 0 &amp; \\text{otherwise} \\end{cases} \\] This can be usefully re-expressed as \\[\\begin{align*} f({\\boldsymbol x}|\\theta) &amp;= 1 \\cdot \\mathop{\\mathrm{\\unicode{x1D7D9}}}_{\\{x_1,\\dots,x_n &gt; \\theta\\}}({\\boldsymbol x})\\mathop{\\mathrm{\\unicode{x1D7D9}}}_{\\{ x_1,\\dots,x_n &lt; \\theta+1\\}}({\\boldsymbol x}) \\\\ &amp;= 1 \\cdot \\mathop{\\mathrm{\\unicode{x1D7D9}}}_{t_1=\\min(x_i)&gt;\\theta}(t_1)\\mathop{\\mathrm{\\unicode{x1D7D9}}}_{t_2=\\max(x_i)&lt;\\theta+1}(t_2) \\\\ &amp;= \\underbrace{1 \\vphantom{\\mathop{\\mathrm{\\unicode{x1D7D9}}}_{\\{t_1&gt;\\theta \\}}} }_{h({\\boldsymbol x})} \\cdot \\underbrace{\\mathop{\\mathrm{\\unicode{x1D7D9}}}_{\\{t_1&gt;\\theta \\} \\cap \\{t_2&lt;\\theta+1 \\}}}_{g(t_1,t_2|\\theta)} \\end{align*}\\] We clearly see that the two-component statistic \\[ T({\\boldsymbol X}) = \\big(\\min(X_1,\\dots,X_n), \\max(X_1,\\dots,X_n) \\big) \\] is sufficient. Furthermore, for any two sample points \\({\\boldsymbol x}\\) and \\({\\boldsymbol y}\\), \\(f({\\boldsymbol x}|\\theta)/f({\\boldsymbol y}|\\theta)\\) takes the constant value 1 (for all \\(\\theta\\) for which the ratio is defined) iff both \\(\\min(x_i)=\\min(y_i)\\) and \\(\\max(x_i)=\\max(y_i)\\). This suggests that \\(T({\\boldsymbol X})\\) is a minimal sufficient statistic for this problem. Note than then the minimal sufficient statistic in a one-parameter problem is not necessarily a scalar! Obviously, if a sufficient statistic is scalar, then it must be minimal! "],["point-estimators.html", "4.3 Point estimators", " 4.3 Point estimators Recall: \\(X_1,\\dots,X_n\\sim f(x|\\theta)\\) is a random sample, where \\(f\\) is known but the parameter \\(\\theta\\) of the pdf is unknown. Often, we may specify \\(\\theta\\in\\Theta\\), where \\(\\Theta\\) is the parameter space. Note that \\(\\theta\\) may be a vector \\(\\theta=(\\theta_1,\\dots,\\theta_p)^\\top\\). Example 4.7 For \\(\\mathop{\\mathrm{N}}(\\mu,\\sigma^2)\\), \\(\\theta = (\\mu,\\sigma^2)^\\top\\), so \\(p=2\\) and \\(\\Theta = \\mathbb{R}\\times \\mathbb{R}_{\\geq 0}\\). For \\(\\mathop{\\mathrm{Poi}}(\\lambda)\\), \\(\\theta=\\lambda\\) and \\(\\Theta=\\mathbb{R}_{\\geq 0}\\). The goal of point estimation: Provide a single “best guess” of \\(\\theta\\), based on observations \\(X_1,\\dots,X_n\\). Formally, we may write \\[ \\hat\\theta = T(X_1,\\dots,X_n) = T({\\boldsymbol X}) \\] as a point estimator for \\(\\theta\\), where \\(T({\\boldsymbol X})\\) is a statistic. We use the term “estimator” to denote the function that gives the estimate. On the other hand, an “estimate” is the realised value of the estimator function. In other words, the estimator \\(T({\\boldsymbol X})\\) is a random variable, whereas the estimate \\(T({\\boldsymbol x})\\) is a realised value for the observed data \\({\\boldsymbol X}={\\boldsymbol x}\\). The standard convention is to denote estimators/estimates of parameters with hats on the respective symbols (e.g. \\(\\hat\\theta\\)), whereas true values do not have hats (c.f. \\(\\theta\\) or \\(\\theta_0\\)). A good estimator should make \\(|\\hat\\theta-\\theta|\\) as small as possible, despite \\(\\theta\\) being unknown; and the value of \\(\\hat\\theta\\) changes with the sample observed. We will make use of the sampling properties of the r.v. \\(\\hat\\theta\\) to quantify (and qualify) its worth as an estimator for \\(\\theta\\). We will consider three main aspects of point estimation General methods for finding a point estimator Method of moments (MOM) Method of maximum likelihood (ML) Methods for assessing the performance of point estimators Bias Variance Mean squared error Large sample properties of estimators "],["method-of-moments.html", "4.4 Method of moments", " 4.4 Method of moments Definition 4.4 (Method of moments estimator) Suppose that \\(U({\\boldsymbol X})\\) is any statistic such that \\[ \\mathop{\\mathrm{E}}\\big(U({\\boldsymbol X})\\big) = m(\\theta) \\] where \\(m(\\cdot)\\) is invertible. Then \\[ \\hat\\theta = m^{-1}\\big(U({\\boldsymbol X})\\big) \\] is called the method of moments (MOM) estimator of \\(\\theta\\) based on \\(U\\). The moment here is the mean, i.e. the first moment, of \\(U({\\boldsymbol X})\\). A more precise name for this estimator would be ‘the MOM estimator based on the first moment of \\(U\\)’. There are two main situations where moments other than the first moment are needed: When \\(m(\\theta)\\) either does not involve \\(\\theta\\), or is otherwise not invertible. We might then consider using instead the second moment, \\(\\mathop{\\mathrm{E}}(U^2) = m_2(\\theta)\\), say. If \\(m_2(\\theta)\\) is invertible, then MOM based on \\(U_2\\) can be used in order to define an estimator. When \\(\\theta=(\\theta_1,\\dots,\\theta_p)^\\top\\) is a vector. I.e., there is more than one unknown parameter. The number of moments used (the number of equations to solve) must be equal to the dimensionality of \\(\\theta\\) (the number of unknowns). Example 4.8 Let \\(X_1,\\dots,X_n\\,\\overset{\\text{iid}}{\\sim}\\,\\mathop{\\mathrm{Unif}}(0,\\theta)\\). Consider \\(U({\\boldsymbol X}) = \\bar X_n = n^{-1}\\sum_{i=1}^n X_i\\), the sample mean. Then, since \\(\\mathop{\\mathrm{E}}(X_i)=\\theta/2\\), we have \\[\\begin{align*} \\mathop{\\mathrm{E}}\\big(U({\\boldsymbol X}) \\big) &amp;= \\mathop{\\mathrm{E}}\\left(\\frac{1}{n} \\sum_{i=1}^n X_i \\right) \\\\ &amp;= \\frac{1}{n} \\sum_{i=1}^n \\mathop{\\mathrm{E}}\\left( X_i \\right) = \\theta/2. \\end{align*}\\] So the MOM estimator of \\(\\theta\\) based on \\(U\\) is \\(\\hat\\theta = 2\\bar X_n\\). theta &lt;- 3 (X &lt;- runif(50, min = 0, max = theta)) %&gt;% round(3) ## [1] 2.871 1.360 2.033 1.718 0.309 2.699 0.738 0.126 0.984 2.864 2.669 2.078 ## [13] 1.922 2.983 1.967 2.126 1.632 1.782 0.867 0.441 2.889 2.707 2.072 2.386 ## [25] 0.074 1.433 2.275 0.649 0.955 0.695 0.428 1.244 1.241 1.107 0.457 0.416 ## [37] 0.699 1.398 0.798 2.573 0.137 1.327 2.397 0.366 1.683 0.620 0.383 2.260 ## [49] 2.685 1.123 2 * mean(X) # MOM estimator ## [1] 2.945851 Example 4.9 Let \\(X_1,\\dots,X_n\\,\\overset{\\text{iid}}{\\sim}\\,\\mathop{\\mathrm{Poi}}(\\lambda)\\). Consider, for example, \\(U({\\boldsymbol X})=\\sum_{i=1}^n [X_i=0]\\), the number of zeroes found in the sample. Then, since \\[\\begin{align*} \\mathop{\\mathrm{E}}([X_i=0]) &amp;= \\sum_{k=0}^\\infty [k=0]\\Pr(X_i=k) \\\\ &amp;= \\Pr(X_i=0) \\\\ &amp;= e^{-\\lambda}, \\end{align*}\\] we have that \\(\\mathop{\\mathrm{E}}\\big( U({\\boldsymbol X}) \\big) = \\sum_{i=1}^n \\mathop{\\mathrm{E}}([X_i=0]) = ne^{-\\lambda}\\). Hence, the MOM estimator for \\(\\lambda\\) based on \\(U\\) is \\[ \\hat\\lambda = -\\log(U/n). \\] "],["method-of-maximum-likelihood.html", "4.5 Method of maximum likelihood", " 4.5 Method of maximum likelihood Definition 4.5 (Maximum likelihood (ML) estimator) The ML estimator of \\(\\theta\\) is \\(\\hat\\theta\\) which is such that \\[ \\hat\\theta = \\mathop{\\mathrm{arg\\,max}}_{\\theta\\in\\Theta} L(\\theta|{\\boldsymbol X}) \\] That is, the ML estimator is the value of \\(\\theta\\) which is the most likeliest value as judged by the likelihood function, given the data that was observed. We are interested in the peak of the graph of \\(L(\\theta|{\\boldsymbol X})\\) against \\(\\theta\\). In practice, \\(\\hat\\theta\\) is most often found by locating the maximum of the log-likelihood \\(l(\\theta|{\\boldsymbol X}) = \\log L(\\theta|{\\boldsymbol X})\\), which is computationally and algebraically simpler. Unfortunately, uniqueness is not guaranteed. But in many ‘standard’ statistical models, the MLE is uniquely defined by the likelihood function. 4.5.1 Finding the MLE Typically we locate \\(\\hat\\theta\\) by solving \\(l&#39;(\\hat\\theta)=0\\), and then checking that the stationary point is a maximum. Several points on this: This still leaves open the possibility that the likelihood has multiple local maxima, at each of which the derivative is zero. It is wise to check \\(\\l(\\theta)\\) for multimodal behaviour, e.g. by drawing a sketch of the function. This strategy works for ‘simple’ enough problems, e.g. unidimensional parameters, or multidimensional parameter situations which reduce to complete information system (sets of simultaneous equations). Numerical methods can be employed if explicit analytical forms for the MLE cannot be found. These estimators are found more often by iterative procedures built into computer software (e.g. Newton-Raphson, Fisher scoring, quasi-Newton, gradient descent, conjugate gradients, etc.). Even then we might run into numerical issues (e.g. flat likelihood, multimodality, precision issues, etc.). Example 4.10 Suppose that \\(Y_1,\\dots,Y_n\\) is an iid random sample from \\(\\mathop{\\mathrm{N}}(\\mu,1)\\), with \\(\\mu\\) unknown. Then, the log-likelihood function is \\[\\begin{aligned} l(\\mu) &amp;= \\log \\left\\{(\\sqrt{2\\pi})^{-n} e^{-\\sum_{i=1}^n (Y_i-\\mu)^2/2} \\right\\} \\\\ &amp;= \\text{const.} -\\frac{1}{2}\\sum_{i=1}^n (Y_i-\\mu)^2 \\end{aligned}\\] The derivative with respect to \\(\\mu\\) gives us \\[\\begin{aligned} l&#39;(\\mu) = \\frac{1}{2}\\sum_{i=1}^n (Y_i-\\mu) \\end{aligned}\\] Equating this to zero gives the MLE for \\(\\mu\\) \\[\\begin{aligned} \\frac{1}{2}\\sum_{i=1}^n (Y_i-\\mu) &amp;= 0 \\\\ \\sum_{i=1}^n Y_i - n\\mu &amp;= 0 \\\\ \\Rightarrow \\mu &amp;= \\frac{1}{n}\\sum_{i=1}^n Y_i =: \\bar Y_n \\end{aligned}\\] Thus, \\(\\hat\\mu=\\bar Y_n\\). Finding the MLE numerically X &lt;- rnorm(n = 100, mean = 8, sd = 1) mean(X) ## [1] 8.021617 # Optimising the likelihood function lik &lt;- function(theta) -sum(dnorm(x = X, mean = theta, sd = 1, log = TRUE)) theta0 &lt;- 1 # starting value res &lt;- optim(par = theta0, fn = lik, method = &quot;BFGS&quot;, lower = -Inf, upper = Inf) res$par ## [1] 8.021617 Sometimes, a sketch of \\(l(\\theta)\\) reveals that the MLE does not satisfy \\(l&#39;(\\hat\\theta)=0\\). Let \\(X_1,\\dots,X_n\\,\\overset{\\text{iid}}{\\sim}\\,\\mathop{\\mathrm{Unif}}(0,\\theta)\\). Then the pdf of \\({\\boldsymbol X}\\) is \\(f({\\boldsymbol x}|\\theta) = 1/\\theta^n\\) for \\(X_1,\\dots,X_n &lt; \\theta\\). The likelihood is therefore \\[ L(\\theta|{\\boldsymbol X}) = \\begin{cases} \\frac{1}{\\theta^n} &amp;\\theta &gt; \\max(X_1,\\dots,X_n) \\\\ 0 &amp;\\text{otherwise} \\end{cases} \\] which is maximised at \\(\\hat\\theta = \\max(X_1,\\dots,X_n)\\). 4.5.2 Invariance of MLE The MLE is invariant under parameter transformation: Example 4.11 Let \\(\\hat\\pi\\) be the MLE for \\(\\pi\\) after observing data \\(X_1,\\dots,X_n\\,\\overset{\\text{iid}}{\\sim}\\,\\mathop{\\mathrm{Bern}}(\\pi)\\). The log-odds of an event happening is given by \\(\\nu = \\log\\big(\\pi/\\log(1-\\pi)\\big)\\), which is a one-to-one transformation of \\(\\pi\\). Therefore, the MLE for \\(\\nu\\) is given by \\[\\hat\\nu = \\log \\frac{\\hat\\pi}{1-\\hat\\pi}.\\] Note that \\(\\hat\\psi\\) can be infinite-valued, if \\(\\hat\\theta=0\\) or \\(\\hat\\theta=1\\). "],["evaluating-estimators.html", "4.6 Evaluating estimators", " 4.6 Evaluating estimators An estimator is assessed through its distribution in repeated sampling from the assumed model. A ‘good’ estimator of an unknown parameter \\(\\theta\\) is a function \\(T({\\boldsymbol X})\\) which typically, in repeated sampling, takes values that are close to the true value of \\(\\theta\\), whatever the true value of \\(\\theta\\) may be. We discuss three such properties: Bias Variance Mean squared error 4.6.1 Bias Definition 4.6 (Bias) The bias of an estimator \\(\\hat\\theta\\) is defined to be \\[\\text{Bias}_\\theta(\\hat\\theta) = \\mathop{\\mathrm{E}}_\\theta(\\hat\\theta) - \\theta.\\] The subscript \\(\\theta\\) makes clear the fact that the expectation is taken under the distribution using \\(\\theta\\) as the true value of the parameter. When \\(\\mathop{\\mathrm{E}}_\\theta(\\hat\\theta) = \\theta\\), \\(\\text{Bias}_\\theta(\\hat\\theta)=0\\) for all possible values of \\(\\theta\\), and in this case \\(\\hat\\theta\\) is called an unbiased estimator for \\(\\theta\\). Small bias, and even unbiasedness, is desirable. 4.6.2 Variance and standard error Definition 4.7 (Variance) The variance of an estimator \\(\\hat\\theta\\) is defined to be \\[\\mathop{\\mathrm{Var}}_{\\theta}(\\hat\\theta)=\\mathop{\\mathrm{E}}_\\theta\\left[\\big(\\hat\\theta - \\mathop{\\mathrm{E}}(\\hat\\theta)\\big)^2\\right]\\] This just uses the regular definition of the variance for random variables. Definition 4.8 (Standard error) The standard error of the estimator \\(\\hat\\theta\\) is defined as the standard deviation of the variance of the estimator, i.e. \\[\\text{se}(\\hat\\theta) = \\sqrt{\\mathop{\\mathrm{Var}}_\\theta(\\hat\\theta)}\\] Obviously, we desire an estimator whose variability (in repeated sampling) is low. These two properties measure different things about estimators: Bias is a measure of accuracy. Variance is a measure of precision. Figure 4.1: The difference between bias and variance. 4.6.3 Mean squared error Definition 4.9 (Mean squared error of estimator) The MSE of the estimator \\(\\hat\\theta\\) is defined as \\[\\text{MSE}_\\theta(\\hat\\theta) = \\mathop{\\mathrm{E}}_\\theta\\left[(\\hat\\theta - \\theta)^2 \\right] = \\{\\text{Bias}_\\theta(\\hat\\theta) \\}^2 + \\mathop{\\mathrm{Var}}_\\theta(\\hat\\theta).\\] As an exercise, prove the bias-variance decomposition above. Hare some hints on how to get started: hint 1 hint 2 There is a clear and direct relationship between the MSE of an estimator, and its bias and variance. For a given MSE, Reducing the bias of an estimator implies that its variance will increase. Conversely, reducing the variance of an estimator implies that bias will increase. This is known as the bias-variance trade-off. It is typically impossible to do both simultaneously. The bias-variance trade-off does not mean an estimator with low bias and low variance is impossible to achieve.It simply means improving one aspect of an estimator will worsen it in the other aspect. Example 4.12 Let \\(X_1,\\dots,X_n\\,\\overset{\\text{iid}}{\\sim}\\,\\mathop{\\mathrm{Unif}}(0,\\theta)\\). We previously found two different estimators for \\(\\theta\\): MLE: \\(\\hat\\theta_{ML} = \\max_i(X_i)\\) MOM: \\(\\hat\\theta_{MOM} = 2\\bar X\\) Let us examine these in terms of bias, variance and mse. Clearly \\(\\hat\\theta_{MOM}\\) is unbiased: \\(\\mathop{\\mathrm{E}}(\\hat\\theta_{MOM}) = 2\\mathop{\\mathrm{E}}(\\bar X) = 2\\mathop{\\mathrm{E}}(X_i) = 2\\times \\theta/2 = \\theta\\). For the bias of \\(\\hat\\theta_{ML}\\), let’s first get the pdf (of \\(\\hat\\theta_{ML}\\)). Proceed via the cdf: \\[\\begin{align*} F_{\\hat\\theta_{ML}}(x) = \\Pr(\\hat\\theta_{ML} &lt; x) = \\Pr(\\max(X_1,\\dots,X_n) &lt; x) &amp;= \\prod_{i=1}^n \\Pr(X_i&lt;x) = \\left(\\frac{x}{\\theta}\\right)^n \\end{align*}\\] Then, differentiating this gives us the pdf \\(f_{\\hat\\theta_{ML}}(x) = nx^{n-1}/\\theta^n\\). So now we find the mean: \\[ \\mathop{\\mathrm{E}}\\hat\\theta_{ML} = \\int_0^\\theta x \\frac{nx^{n-1}}{\\theta^n}\\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!x =\\left[\\frac{nx^{n+1}}{\\theta^n(n+1)} \\right]_0^\\theta = \\frac{n\\theta}{n+1} \\] Therefore, the bias is \\(\\text{Bias}(\\hat\\theta_{ML})=-\\theta/(n+1)\\neq 0\\). Note that this tends to 0 as \\(n\\to\\infty\\), but can be substantial when \\(n\\) is small. For \\(\\hat\\theta_{MOM}\\), we have \\[ \\mathop{\\mathrm{Var}}(\\hat\\theta_{MOM}) = 4\\mathop{\\mathrm{Var}}(\\bar X) = \\frac{4\\mathop{\\mathrm{Var}}(X_i)}{n} = \\frac{\\theta^2}{3n}. \\] Note that \\(\\mathop{\\mathrm{Var}}(\\hat\\theta_{MOM})\\to 0\\) as \\(n\\to\\infty\\) at the rate of \\(1/n\\). This is typical behaviour of ‘good’ estimators. For \\(\\hat\\theta_{ML}\\): \\[\\begin{align*} \\mathop{\\mathrm{Var}}(\\hat\\theta_{ML}) = \\mathop{\\mathrm{E}}(\\hat\\theta_{ML}^2) - \\mathop{\\mathrm{E}}^2(\\hat\\theta_{ML}) &amp;= \\int_0^\\theta x^2 \\frac{nx^{n-1}}{\\theta^n}\\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!x - \\frac{n^2\\theta^2}{(n+1)^2} \\\\ &amp;= \\theta^2 \\left(\\frac{n}{(n+1)^2(n+2)} \\right) \\end{align*}\\] so \\(\\mathop{\\mathrm{Var}}(\\hat\\theta_{ML})\\to 0\\) as \\(n\\to\\infty\\) but at a faster rate of \\(1/n^2\\). For \\(\\hat\\theta_{MOM}\\), we have \\[ \\text{MSE}(\\hat\\theta_{MOM}) = \\mathop{\\mathrm{Var}}(\\hat\\theta_{MOM}) = \\frac{\\theta^2}{3n}. \\] For \\(\\hat\\theta_{ML}\\): \\[\\begin{align*} \\text{MSE}(\\hat\\theta_{ML}) = \\text{Bias}(\\hat\\theta_{ML}^2) + \\mathop{\\mathrm{Var}}(\\hat\\theta_{ML}) &amp;= \\theta^2\\left( \\frac{n}{(n+1)^2(n+2)} + \\frac{1}{(n+1)^2} \\right). \\end{align*}\\] Notice that since \\(\\text{MSE}(\\hat\\theta_{MOM})\\) is \\(o(1/n)\\) and \\(\\text{MSE}(\\hat\\theta_{ML})\\) is \\(o(1/n^2)\\), \\[\\text{MSE}(\\hat\\theta_{ML}) \\leq \\text{MSE}(\\hat\\theta_{MOM})\\] for all \\(n\\) (and tends to 0 as \\(n\\to\\infty\\)). So \\(\\hat\\theta_{ML}\\), even though it is biased, is clearly to be preferred on the basis of MSE. "],["cramér-rao-lower-bound-crlb.html", "4.7 Cramér-Rao lower bound (CRLB)", " 4.7 Cramér-Rao lower bound (CRLB) It is difficult to find an estimator which simultaneously is low in bias and variance. If we instead focus on a class of unbiased estimators, then we have a theorem to benchmark their performance. Theorem 4.3 (Cramér-Rao inequality for unbiased estimators) Let \\({\\boldsymbol X}\\sim f({\\boldsymbol x}|\\theta)\\) satisfying some regularity conditions21 Let \\(\\hat\\theta=\\hat\\theta({\\boldsymbol X})\\) be an estimator, i.e. \\(\\mathop{\\mathrm{E}}_\\theta(\\hat\\theta)=\\theta\\). Then, for any \\(\\theta\\in\\Theta\\), \\[\\mathop{\\mathrm{Var}}_\\theta(\\hat\\theta) \\geq \\frac{1}{\\mathop{\\mathrm{E}}_\\theta\\left[\\left(\\frac{\\partial}{\\partial \\theta} \\log f({\\boldsymbol X}|\\theta) \\right)^2\\right]}.\\] If an estimator’s variance is close to the CRLB, it can be regarded as efficient. A class of estimators achieving the CRLB are said to be optimal, known as the minimum variance unbiased estimator (MVUE). Although, the CRLB is not necessarily achieved by any estimator. Proof. The proof is an application of the Cauchy-Schwarz inequality via the covariance inequality \\[ \\mathop{\\mathrm{Var}}(Y) \\geq \\frac{\\{\\mathop{\\mathrm{Cov}}(Y,U)\\}^2}{\\mathop{\\mathrm{Var}}(U)} \\] for r.v.s \\(U\\) and \\(Y\\). We consider the more general case for biased estimators \\(\\hat\\theta({\\boldsymbol X})\\). Let \\[\\begin{align*} U &amp;= l&#39;(\\theta) = \\frac{\\partial}{\\partial \\theta} \\log f({\\boldsymbol X}|\\theta) \\\\ Y &amp;= \\hat\\theta({\\boldsymbol X}) \\end{align*}\\] Firstly, we note that \\(\\mathop{\\mathrm{Var}}(U)=\\mathop{\\mathrm{E}}(U^2)\\) since \\(\\mathop{\\mathrm{E}}(U)=0\\): \\[ \\mathop{\\mathrm{E}}(U) = \\int\\frac{\\frac{\\partial}{\\partial \\theta} f({\\boldsymbol x}|\\theta)}{\\cancel{f({\\boldsymbol x}|\\theta)}} \\cancel{f({\\boldsymbol x}|\\theta)} \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!{\\boldsymbol x}= \\frac{\\partial}{\\partial \\theta} \\int f({\\boldsymbol x}|\\theta) \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!x = 0. \\] Further, because \\(\\mathop{\\mathrm{E}}(U)=0\\), we have \\(\\mathop{\\mathrm{Cov}}(Y,U)=\\mathop{\\mathrm{E}}(UY)-\\mathop{\\mathrm{E}}(U)\\mathop{\\mathrm{E}}(Y) = \\mathop{\\mathrm{E}}(UY)\\), and so \\[\\begin{align*} \\mathop{\\mathrm{Cov}}(Y,U) = \\mathop{\\mathrm{E}}\\left(Y \\cdot \\frac{\\partial}{\\partial \\theta} \\log f({\\boldsymbol X}|\\theta) \\right) &amp;= \\mathop{\\mathrm{E}}\\left(Y \\cdot \\frac{\\frac{\\partial}{\\partial \\theta} f({\\boldsymbol X}|\\theta)}{f({\\boldsymbol X}|\\theta)} \\right)\\\\ &amp;= \\int \\hat\\theta({\\boldsymbol x}) \\frac{\\frac{\\partial}{\\partial \\theta} f({\\boldsymbol x}|\\theta)}{\\cancel{f({\\boldsymbol x}|\\theta)}} \\cancel{f({\\boldsymbol x}|\\theta)} \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!{\\boldsymbol x}\\\\ &amp;= \\frac{\\partial}{\\partial \\theta} \\left[ \\int \\hat\\theta({\\boldsymbol x}) f({\\boldsymbol x}|\\theta) \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!{\\boldsymbol x} \\right] \\\\ &amp;= \\frac{\\partial}{\\partial \\theta} \\mathop{\\mathrm{E}}[\\hat\\theta({\\boldsymbol X})] = \\psi&#39;(\\theta). \\end{align*}\\] Here, we have assumed that the expectation of \\(\\hat\\theta({\\boldsymbol X})\\) is not \\(\\theta\\) but some function of \\(\\theta\\), \\(\\psi(\\theta)\\) say, since the estimator is biased. We have now proved the general case of the CRLB which states \\[ \\mathop{\\mathrm{Var}}(\\hat\\theta) \\geq \\frac{\\left[\\psi&#39;(\\theta) \\right]^2}{\\mathop{\\mathrm{E}}\\left[\\left(\\frac{\\partial}{\\partial \\theta} \\log f({\\boldsymbol X}|\\theta) \\right)^2\\right]}. \\] For unbiased estimators, \\(\\psi(\\theta)=\\theta\\), and hence \\[ \\psi&#39;(\\theta) = \\frac{\\partial}{\\partial\\theta}(\\theta) = 1, \\] which completes the proof. Remark: The derivative of the log-likelihood, \\(S(\\theta) = \\frac{\\partial}{\\partial \\theta} \\log f({\\boldsymbol X}|\\theta)\\), is known as the score function. The property that \\(\\mathop{\\mathrm{E}}(S(\\theta))=0\\) is fundamental to the theory of maximum likelihood. 4.7.1 Fisher information The quantity in the RHS denominator of Theorem 4.3 is known as the information number or Fisher information. Definition 4.10 (Fisher information (unidimensional)) Let \\({\\boldsymbol X}\\sim f({\\boldsymbol x}|\\theta)\\), where \\(\\theta\\in\\mathbb{R}\\). The Fisher information is defined to be the expectation of the second moment of the score function, i.e. \\[{\\mathcal I}(\\theta) = \\mathop{\\mathrm{E}}\\left[ \\left( \\frac{\\partial}{\\partial \\theta} \\log f({\\boldsymbol X}|\\theta) \\right)^2 \\right] \\in \\mathbb{R}\\] In simple terms, the Fisher information measures the amount of information that an observable random variable \\({\\boldsymbol X}\\) carries about an unknown parameter \\(\\theta\\) of the statistical model that models \\({\\boldsymbol X}\\). The Fisher information for multidimensional parameters can be defined similarly (c.f. Fisher information matrix). Lemma 4.1 Let \\({\\boldsymbol X}\\sim f({\\boldsymbol x}|\\theta)\\), where \\(\\theta\\in\\mathbb{R}\\), and \\(S(\\theta)=\\frac{\\partial}{\\partial \\theta} \\log f({\\boldsymbol X}|\\theta)\\). Under certain regularity conditions, \\(\\mathop{\\mathrm{E}}[S(\\theta)] = 0\\). \\({\\mathcal I}(\\theta) = \\mathop{\\mathrm{Var}}[S(\\theta)]\\). \\({\\mathcal I}(\\theta) = -\\mathop{\\mathrm{E}}[S&#39;(\\theta)]\\). To be proven in Ex. sheet 4! Lemma 4.2 (Fisher information is additive) Let \\(X_1,\\dots,X_n\\,\\overset{\\text{iid}}{\\sim}\\,f(x|\\theta)\\). Suppose \\({\\mathcal I}_1(\\theta)\\) is the Fisher information from a single observation \\(X_i\\), i.e. \\({\\mathcal I}_1(\\theta) = -\\mathop{\\mathrm{E}}[l&#39;&#39;(\\theta|X_i)]\\). Then the full Fisher information is \\({\\mathcal I}(\\theta) = n{\\mathcal I}_1(\\theta)\\). Proof. \\[ {\\mathcal I}(\\theta) = -\\mathop{\\mathrm{E}}[l&#39;&#39;(\\theta|{\\boldsymbol X})] = -\\mathop{\\mathrm{E}}\\left[\\sum_{i=1}^n l&#39;&#39;(\\theta|X_i)\\right] \\] Example 4.13 Let \\(X_1,\\dots,X_n\\,\\overset{\\text{iid}}{\\sim}\\,\\mathop{\\mathrm{N}}(\\mu,\\sigma^2)\\). Let \\(\\hat\\mu=\\bar X_n\\); then \\(\\mathop{\\mathrm{Var}}(\\hat\\mu)=\\sigma^2/n\\). The score function is given as \\[ l&#39;(\\mu|{\\boldsymbol X}) = \\frac{\\partial}{\\partial\\mu} \\left(\\text{const.}- \\sum_{i=1}^n \\frac{(X_i-\\mu)^2}{2\\sigma^2} \\right) = \\sum_{i=1}^n \\frac{X_i-\\mu}{\\sigma^2}, \\] while the Fisher information is obtained as \\[ {\\mathcal I}(\\mu) = \\mathop{\\mathrm{Var}}\\big(l&#39;(\\mu|{\\boldsymbol X})\\big) = \\sum_{i=1}^n \\mathop{\\mathrm{Var}}\\left(\\frac{X_i-\\mu}{\\sigma^2} \\right) = \\sum_{i=1}^n\\frac{\\mathop{\\mathrm{Var}}(X_i)}{\\sigma^4} = \\frac{n}{\\sigma^2}. \\] Hence, the CRLB is \\(\\sigma^2/n\\), and the estimator \\(\\hat\\mu=\\bar X_n\\) achieves it. Therefore, \\(\\bar X_n\\) is the MVUE of \\(\\mu\\). 4.7.2 Variance reduction: Rao-Blackwellisation We can reduce the variance of an unbiased estimator by conditioning on a sufficient statistic. Theorem 4.4 (Rao-Blackwell) Suppose that \\(U({\\boldsymbol X})\\) is unbiased for \\(\\theta\\), and \\(S({\\boldsymbol X})\\) is sufficient for \\(\\theta\\). Then the function of \\(S\\) defined by \\[ \\phi(S) = \\mathop{\\mathrm{E}}_\\theta(U|S) \\] is a statistic, i.e. \\(\\phi(S)\\) does not involve \\(\\theta\\); is an unbiased statistic, i.e. \\(\\mathop{\\mathrm{E}}(\\phi(S)) = \\theta\\); and has \\(\\mathop{\\mathrm{Var}}_\\theta(\\phi(S)) \\leq \\mathop{\\mathrm{Var}}_\\theta (U)\\), with equality iff \\(U\\) is itself a function of \\(S\\). In other words, \\(\\phi(S)\\) is a uniformly better unbiased estimator for \\(\\theta\\). Thus the Rao-Blackwell theorem provides a systematic method of variance reduction for an estimator that is not a function of the sufficient statistic. Proof. Since \\(S\\) is sufficient, the distribution of \\({\\boldsymbol X}\\) given \\(S\\) does not involve \\(\\theta\\), and hence \\(\\mathop{\\mathrm{E}}_\\theta(U({\\boldsymbol X})|S)\\) does not involve \\(\\theta\\). Further, \\(\\mathop{\\mathrm{E}}(\\phi(S)) = \\mathop{\\mathrm{E}}\\left[ \\mathop{\\mathrm{E}}(U|S) \\right] = \\mathop{\\mathrm{E}}(U) = \\theta\\). To prove the last part, note that \\[\\begin{align*} \\mathop{\\mathrm{Var}}(U) &amp;= \\mathop{\\mathrm{E}}\\left[ \\mathop{\\mathrm{Var}}(U|S) \\right] + \\mathop{\\mathrm{Var}}\\left[ \\mathop{\\mathrm{E}}(U|S) \\right] \\\\ &amp;= \\mathop{\\mathrm{E}}\\left[ \\mathop{\\mathrm{Var}}(U|S) \\right] + \\mathop{\\mathrm{Var}}(\\phi(S)) \\\\ &amp;\\geq \\mathop{\\mathrm{Var}}(\\phi(S)) \\end{align*}\\] with equality iff \\(\\mathop{\\mathrm{Var}}(U|S) =0\\), i.e. iff \\(U\\) is a function of \\(S\\). Example 4.14 Suppose we have data \\(X_1,\\dots,X_n\\,\\overset{\\text{iid}}{\\sim}\\,\\mathop{\\mathrm{Poi}}(\\lambda)\\) pertaining to the number of road accidents per day, and we want to estimate the probability of having no accidents \\(\\theta = e^{-\\lambda}=\\Pr(X_i=0)\\). An unbiased estimator of \\(\\theta\\) is \\[ U({\\boldsymbol X}) = \\begin{cases} 1 &amp; X_1 =0 \\\\ 0 &amp; \\text{otherwise} \\end{cases} \\] But this is likely to be a poor estimator, since it ignores \\(X_2,X_3,\\dots,X_n\\). We can see that \\(S({\\boldsymbol X})=\\sum_{i=1}^n X_i\\) is sufficient since the joint pdf can be expressed as \\[ f({\\boldsymbol x}|\\lambda) = \\frac{1}{x_1!\\cdots x_n!} \\cdot e^{-n\\lambda}\\lambda^{\\sum_{i=1}^nx_i}. \\] Now apply the Rao-Blackwell theorem: \\[\\begin{align*} \\phi(S) = \\mathop{\\mathrm{E}}(U|S) = \\mathop{\\mathrm{E}}\\Big(U \\, \\Big| \\, \\sum_{i=1}^n X_i = S \\Big) &amp;= \\Pr\\Big(X_1=0 \\, \\Big| \\, \\sum_{i=1}^n X_i = S \\Big)\\\\ &amp;= \\left(1 - \\frac{1}{n} \\right)^S, \\end{align*}\\] where the conditional probability in the last step comes from the Poisson-binomial relationship (see Ex sheet 2, Q11: Suppose \\(X_i\\,\\overset{\\text{iid}}{\\sim}\\,\\mathop{\\mathrm{Poi}}(\\lambda_i)\\), then \\(X_1\\big|(\\sum_{i=1}^nX_i=N)\\sim\\mathop{\\mathrm{Bin}}(N,\\pi)\\), where \\(\\pi=\\lambda_1/\\sum_{i=1}^n \\lambda_i\\)). By the Rao-Blackwell theorem, \\(\\mathop{\\mathrm{Var}}(\\phi)&lt;\\mathop{\\mathrm{Var}}(U)\\) (strict inequality since \\(U\\) is not a function of \\(S\\)), so prefer \\(\\phi(S)\\) over \\(U\\) as an estimator. But is \\(\\phi(S)=(1-1/n)^S\\) unbiased? This is guaranteed by the RB theorem. Check: Since \\(S\\sim\\mathop{\\mathrm{Poi}}(n\\lambda)\\) (sum of Poisson r.v.s is Poisson), we get \\[\\begin{align*} \\mathop{\\mathrm{E}}(\\phi(S)) &amp;= \\sum_{s=0}^\\infty \\left(1 - \\frac{1}{n} \\right)^s \\frac{e^{-n\\lambda}(n\\lambda)^s}{s!}\\times e^{-\\lambda}e^{\\lambda} \\\\ &amp;= e^{-\\lambda} \\sum_{s=0}^\\infty \\underbrace{\\frac{e^{-\\lambda(n-1)}[\\lambda(n-1)]^s}{s!}}_{\\text{pmf of }\\mathop{\\mathrm{Poi}}(\\lambda(n-1))} = e^{-\\lambda}. \\end{align*}\\] A similar calculation can give us the variance of this estimator. These regularity conditions are essentially that we are able to switch the order of integration and differentiation, and that the \\(\\mathop{\\mathrm{Var}}_\\theta(\\hat\\theta)&lt;\\infty\\). See Thm 7.3.9 of C&amp;B.↩︎ "],["large-sample-properties-of-estimators.html", "4.8 Large sample properties of estimators", " 4.8 Large sample properties of estimators All of the criteria we have considered thus far have been finite-sample criteria. In contrast, we might consider asymptotic properties which describe the behaviour as sample size becomes infinite. We shall discuss three properties: Consistency Efficiency Asymptotic normality In particular, we shall see that ML estimators are (generally) consistent, efficient (achieves CRLB), and has an asymptotic normal distribution. 4.8.1 Consistency Definition 4.11 (Consistent estimator) An estimator \\(\\hat\\theta_n := \\hat\\theta(X_1,\\dots,X_n)\\) is a consistent estimator for \\(\\theta\\) if \\(\\hat\\theta_n \\to \\theta\\) in probability as \\(n\\to\\infty\\). Consistency is a natural condition for a reasonable estimator as \\(\\hat\\theta_n\\) should converge to \\(\\theta\\) if we have a (theoretically) infinite amount of information. Therefore, a non-consistent estimator should not be used in practice! A practical way of checking consistency is to check mean square convergence: If \\(\\hat\\theta_n \\xrightarrow{m.s.} \\theta\\) then \\(\\hat\\theta_n\\) is consistent (since convergence in mean square implies convergence in probability). Further, since \\[ \\text{MSE}(\\hat\\theta_n) = \\mathop{\\mathrm{E}}\\left[(\\hat\\theta_n-\\theta)^2 \\right] = \\left\\{\\text{Bias}(\\hat\\theta_n)\\right\\}^2 + \\mathop{\\mathrm{Var}}(\\hat\\theta_n), \\] we can also check that both the bias and variance converges to 0. 4.8.2 Consistency vs unbiasedness Consistency and bias are two distinct concepts: Unbiasedness (\\(\\mathop{\\mathrm{E}}(\\hat\\theta_n) = \\theta\\)) is a statement about the expected value of the sampling distribution of the estimator. Consistency (\\(\\mathop{\\mathrm{plim}}_{n\\to\\infty}\\hat\\theta_n = \\theta\\)) is a statement relating to the sequence of estimators \\(\\hat\\theta_1, \\hat\\theta_2, \\dots\\). It tells us where the estimator is tending to as the sample size increases. Both are desirable properties of estimators, though it might be possible for one to be satisfied but not the other (see next example). As mentioned, and as we shall see, we are probably better off using a consistent but biased estimator rather than an inconsistent but unbiased estimator. Example 4.15 Let \\(X_1,\\dots,X_n\\) be a sample from \\(\\mathop{\\mathrm{N}}(\\mu,\\sigma^2)\\). Consider the following estimators for \\(\\mu\\) and \\(\\sigma^2\\): \\(\\hat\\mu=X_1\\); and \\(\\hat\\sigma^2=n^{-1}\\sum_{i=1}^n (X_i-\\bar X_n)^2\\) The estimator \\(\\hat\\mu\\) is unbiased since \\(\\mathop{\\mathrm{E}}(X_1)=\\mu\\), but it is not consistent since the distribution of \\(\\hat\\mu\\) is always \\(\\mathop{\\mathrm{N}}(\\mu,\\sigma^2)\\) and will never concentrate around \\(\\mu\\) even with infinite sample size. It is a fact that \\(\\mathop{\\mathrm{E}}(\\hat\\sigma^2) = \\frac{n-1}{n}\\sigma^2\\), which shows that \\(\\hat\\sigma^2\\) is biased in finite samples, but this bias vanishes as \\(n\\to\\infty\\). We can also show \\[\\mathop{\\mathrm{Var}}(\\hat\\sigma^2) = \\frac{2\\sigma^4(n-1)}{n^2} \\to 0\\] as \\(n\\to\\infty\\). Therefore, \\(\\text{MSE}(\\hat\\sigma^2)\\to 0\\), and \\(\\hat\\sigma^2\\) is therefore consistent. 4.8.3 Consistency of MLEs Theorem 4.5 (Consistency of MLE) Let \\(X_1,\\dots,X_n \\,\\overset{\\text{iid}}{\\sim}\\,f(x|\\theta)\\), and let \\(\\hat\\theta_n:=\\mathop{\\mathrm{arg\\,max}}_\\theta L(\\theta|{\\boldsymbol X})\\) denote the MLE of \\(\\theta\\). Let \\(\\psi(\\theta)\\) be a continuous function of \\(\\theta\\). Under certain regularity conditions, we have that for every \\(\\epsilon &gt;0\\) and every \\(\\theta\\in\\Theta\\), \\[ \\lim_{n\\to\\infty} \\Pr(|\\psi(\\hat\\theta_n) - \\psi(\\theta)| \\geq \\epsilon) = 0. \\] That is, \\(\\psi(\\hat\\theta_n)\\) is a consistent estimator of \\(\\psi(\\theta)\\). In particular, consider the identity function \\(\\psi(\\theta)=\\theta\\). Then the theorem states that the MLE \\(\\hat\\theta_n\\) is consistent. Some notes: The regularity conditions mentioned can be found in Miscellanea 10.6.2 of C&amp;B. The above theorem is stating the result for unidimensional \\(\\theta\\), but there are similar multidimensional statements too. We shall defer the proof until we discuss asymptotic normality. 4.8.4 Efficiency Efficiency of an estimator concerns the (asymptotic) variance of an estimator. The CRLB gives the benchmark for efficiency. Definition 4.12 (Asymptotic efficiency) A sequence of estimators \\(\\hat\\theta_n := \\hat\\theta(X_1,\\dots,X_n)\\) is said to be asymptotically efficient for a parameter \\(\\theta\\) if \\[ \\sqrt n \\big(\\hat\\theta_n - \\theta \\big) \\xrightarrow{\\text D} \\mathop{\\mathrm{N}}\\big(0, v(\\theta)\\big), \\] as \\(n\\to\\infty\\), where \\(v(\\theta)\\) is the Cramér-Rao lower bound \\[ v(\\theta) = \\frac{1}{\\mathop{\\mathrm{E}}\\left[\\left(\\frac{\\partial}{\\partial \\theta} \\log f(X_1|\\theta) \\right)^2\\right]} = {\\mathcal I}_1(\\theta)^{-1}. \\] Some remarks: The property that \\(a_n \\big(\\hat\\theta_n - \\theta \\big)\\) converges in distribution to \\(\\mathop{\\mathrm{N}}(0,\\sigma^2)\\) is called asymptotic normality, and \\(\\sigma^2\\) is called the asymptotic variance. An asymptotically efficient estimator has its asymptotic variance achieving the CRLB. 4.8.5 Asymptotic normality and consistency The phrase ‘efficient and consistent’ is somewhat redundant, because efficiency is defined only when the estimator is asymptotically normal, and as we shall show, asymptotic normality implies consistency. Lemma 4.3 Suppose that \\(\\hat\\theta_n\\) is an estimator for \\(\\theta\\) such that \\[ \\frac{\\sqrt n(\\hat\\theta_n - \\theta)}{\\sigma} \\xrightarrow{\\text D} \\mathop{\\mathrm{N}}(0,1) \\] then \\(\\hat\\theta_n\\) is consistent for \\(\\theta\\). Proof. Notice that \\[ \\hat\\theta_n - \\theta = \\frac{\\sigma}{\\sqrt n}\\frac{\\sqrt n(\\hat\\theta_n - \\theta)}{\\sigma} \\xrightarrow{\\text D} 0 \\] by Slutzky’s theorem. Thus, \\(\\hat\\theta_n -\\theta \\xrightarrow{\\text P} 0\\) which implies \\(\\hat\\theta_n \\xrightarrow{\\text P} \\theta\\), and hence \\(\\hat\\theta_n\\) is consistent. 4.8.6 Efficiency of MLE We’ve seen that MLEs are consistent. Under even stronger regularity conditions, we find that they are also efficient. Theorem 4.6 (Asymptotic efficiency of MLE) Let \\(X_1,\\dots,X_n \\,\\overset{\\text{iid}}{\\sim}\\,f(x|\\theta)\\), and let \\(\\hat\\theta_n:=\\mathop{\\mathrm{arg\\,max}}_\\theta L(\\theta|{\\boldsymbol X})\\) denote the MLE of \\(\\theta\\). Under certain regularity conditions, we have that \\[ \\sqrt n (\\hat\\theta_n - \\theta ) \\xrightarrow{\\text D} \\mathop{\\mathrm{N}}\\big(0, {\\mathcal I}_1(\\theta)^{-1}\\big), \\] where \\({\\mathcal I}_1(\\theta)\\) is the (unit) Fisher information for \\(\\theta\\). That is, \\(\\hat\\theta_n\\) is a consistent and asymptotically efficient estimator for \\(\\theta\\). In fact, this theorem also holds more widely–the restriction to iid cases presents a simple proof, but is not essential. Proof (Sketch). Taylor expand the score \\(l&#39;(t|{\\boldsymbol X})\\) about the parameter value \\(\\theta\\): \\[ l&#39;(t|{\\boldsymbol X}) = l&#39;(\\theta|{\\boldsymbol X}) + (t - \\theta)l&#39;&#39;(\\theta|{\\boldsymbol X}) \\] (ignoring the higher order terms). Evaluate this at the maxima \\(t=\\hat\\theta_n\\), we get \\[\\begin{align*} \\cancelto{0}{l&#39;(\\hat\\theta_n|{\\boldsymbol X})} &amp;= l&#39;(\\theta|{\\boldsymbol X}) + (\\hat\\theta_n - \\theta)l&#39;&#39;(\\theta|{\\boldsymbol X}) \\\\ \\Rightarrow \\sqrt n(\\hat\\theta_n - \\theta) &amp;= -\\frac{\\frac{1}{\\sqrt n}l&#39;(\\theta|{\\boldsymbol X})}{\\frac{1}{n}l&#39;&#39;(\\theta|{\\boldsymbol X})} \\end{align*}\\] As one of the exercises at the end of this chapter, you will show that \\[\\begin{gather*} -\\frac{1}{\\sqrt n}l&#39;(\\theta|{\\boldsymbol X}) \\xrightarrow{\\text D} \\mathop{\\mathrm{N}}\\big(0,{\\mathcal I}_1(\\theta)\\big)\\\\ \\text{and}\\\\ \\frac{1}{n}l&#39;&#39;(\\theta|{\\boldsymbol X}) \\xrightarrow{\\text P} {\\mathcal I}_1(\\theta), \\end{gather*}\\] Using Slutzky’s theorem, we get \\[ \\sqrt n(\\hat\\theta_n - \\theta) = -\\frac{\\frac{1}{\\sqrt n}l&#39;(\\theta|{\\boldsymbol X})}{\\frac{1}{n}l&#39;&#39;(\\theta|{\\boldsymbol X})} \\xrightarrow{\\text D} \\mathop{\\mathrm{N}}\\big(0,{\\mathcal I}_1(\\theta)^{-1}\\big) \\] 4.8.7 Efficiency of transformations of MLE Let \\(\\psi(\\theta)\\) be a continuous function of \\(\\theta\\). Using the delta method, the following result can be obtained: \\[ \\sqrt n \\big(\\psi(\\hat\\theta_n) - \\psi(\\theta)\\big) \\xrightarrow{\\text D} \\mathop{\\mathrm{N}}\\big(0, |\\psi&#39;(\\theta)|^2v(\\theta)\\big). \\] This is assuming that \\(\\psi(\\cdot)\\) is differentiable at the value \\(\\theta\\). Therefore, the transformed MLE \\(\\psi(\\hat\\theta)\\) is a consistent and asymptotically efficient estimator of \\(\\psi(\\theta)\\). Look back to the proof of the CRLB above and notice that the asymptotic variance of \\(\\psi(\\hat\\theta_n)\\) is exactly the general version of the CRLB (using the unit Fisher information): \\[ v(\\theta) = \\frac{\\left[\\psi&#39;(\\theta) \\right]^2}{{\\mathcal I}_1(\\theta)}. \\] 4.8.8 Application of asymptotic normality The practical implication of the theorem is that the repeated-sampling distribution of \\(\\hat\\theta_n\\), in large samples, is approximately \\[ \\hat\\theta \\approx \\mathop{\\mathrm{N}}\\left(\\theta, {\\mathcal I}(\\theta)^{-1} \\right). \\] In particular, we can calculate an approximate standard error for \\(\\hat\\theta\\) by estimating the quantity \\({\\mathcal I}(\\theta)\\). Two choices: The obvious ‘plug-in’ estimator using the expected Fisher information \\[ \\text{se}(\\hat\\theta_n) \\approx 1\\Big/\\sqrt{{\\mathcal I}(\\hat\\theta_n)}. \\] This is not usually the best choice, however. It is better (and generally more accurate) to use instead the observed Fisher information \\[ \\text{se}(\\hat\\theta_n) \\approx 1\\Big/\\sqrt{-l&#39;&#39;(\\hat\\theta_n|{\\boldsymbol X})}, \\] which is based directly on the curvature of the log-likelihood of \\(\\hat\\theta\\). Example 4.16 Suppose that \\(X_1,\\dots,X_n\\,\\overset{\\text{iid}}{\\sim}\\,\\mathop{\\mathrm{Poi}}(\\lambda)\\). Then \\[\\begin{align*} l(\\lambda|{\\boldsymbol X}) &amp;= \\text{const.}- n\\lambda +\\sum_{i=1}^n X_i \\log \\lambda \\\\ l&#39;(\\lambda|{\\boldsymbol X}) &amp;= - n +\\sum_{i=1}^n X_i / \\lambda \\\\ -l&#39;&#39;(\\lambda|{\\boldsymbol X}) &amp;= \\sum_{i=1}^n X_i / \\lambda^2 \\hspace{10pt}\\rlap{\\color{gray}\\text{(the observed Fisher information)}} \\end{align*}\\] Hence \\(l&#39;(\\lambda)=0\\) is solved at \\(\\hat\\lambda_n = \\sum_{i=1}^n X_i / n =: \\bar X_n\\). The large-sample variance of \\(\\hat\\lambda_n\\) is \\[ {\\mathcal I}(\\theta)^{-1} = \\mathop{\\mathrm{E}}\\left[-l&#39;&#39;(\\lambda|{\\boldsymbol X}) \\right]^{-1} = \\lambda^2 \\big/ \\mathop{\\mathrm{E}}\\bigg(\\sum_{i=1}^n X_i\\bigg) = \\lambda^2 / n\\lambda = \\lambda /n. \\] As a note, this variance is actually exact, since \\(\\mathop{\\mathrm{Var}}(\\hat\\lambda)=\\mathop{\\mathrm{Var}}(\\bar X_n)=\\mathop{\\mathrm{Var}}(X_i)/n=\\lambda/n\\). The estimated standard error for \\(\\hat\\lambda_n\\) is \\[ \\text{se}(\\hat\\lambda_n) \\approx 1\\Big/\\sqrt{-l&#39;&#39;(\\hat\\lambda_n|{\\boldsymbol X})} = 1\\Big/\\sqrt{n\\hat\\lambda_n/\\hat\\lambda^2} = \\sqrt{\\hat\\lambda_n/n}. \\] In this example, the plug-in estimator for \\({\\mathcal I}(\\theta)\\) happens to be the same as the observed information \\(-l&#39;&#39;(\\hat\\theta)\\). Sometimes this happens, sometimes they are different. "],["exercises-3.html", "4.9 Exercises", " 4.9 Exercises Let \\(X\\) be a sample of size 1 from a \\(\\mathop{\\mathrm{N}}(0,\\sigma^2)\\) population. Is \\(|X|\\) a sufficient statistic for \\(\\sigma\\)? Let \\(X_1,\\dots,X_n\\) be independent random variables with pdf \\[ f_{X_i}(x|\\theta) = \\begin{cases} \\exp(i\\theta - x) &amp;x\\geq i\\theta \\\\ 0 &amp;\\text{otherwise} \\end{cases} \\] for \\(i=1,\\dots,n\\). Prove that \\(T=\\min(X_i/i)\\) is a sufficient statistic for \\(\\theta\\). Let \\(X_1,\\dots,X_n\\) be a random sample from a distribution whose pdf is \\(f(x|\\theta)=(2\\pi)^{-1/2}\\exp(-(x-\\theta)^2/2)\\) for \\(x,\\theta\\in\\mathbb{R}\\). Find a minimal sufficient statistic for \\(\\theta\\). . Show that the statistic \\((\\sum_i X_i, \\sum_i X_i^2)\\) is sufficient, but not minimal sufficient, in the \\(\\mathop{\\mathrm{N}}(\\mu,\\mu)\\) family; the statistic \\(\\sum_i X_i^2\\) is minimal sufficient in the \\(\\mathop{\\mathrm{N}}(\\mu,\\mu)\\) family; and the statistic \\((\\sum_i X_i, \\sum_i X_i^2)\\) is minimal sufficient in the \\(\\mathop{\\mathrm{N}}(\\mu,\\mu^2)\\) family. One observation is taken on a discrete random variable \\(X\\) with pmf \\(f(x|\\theta)\\) where \\(\\theta\\in\\{1,2,3\\}.\\) Find the MLE of \\(\\theta\\). \\(x\\) \\(f(x|\\theta=1)\\) \\(f(x|\\theta=2)\\) \\(f(x|\\theta=3)\\) 0 1/3 1/4 0 1 1/3 1/4 0 2 0 1/4 1/4 3 1/6 1/4 1/2 4 1/6 0 1/4 Let \\(X_1,\\dots,X_n\\) be iid with one of two pdfs. If \\(\\theta=0\\), then \\[ f(x|\\theta) = \\begin{cases} 1 &amp; 0&lt;x&lt;1 \\\\ 0 &amp;\\text{otherwise} \\end{cases} \\] while if \\(\\theta=1\\), \\[ f(x|\\theta) = \\begin{cases} 1/(2\\sqrt x) &amp; 0&lt;x&lt;1 \\\\ 0 &amp;\\text{otherwise}. \\end{cases} \\] Find the MLE of \\(\\theta\\). Let \\(X_1,\\dots,X_n\\) be iid with pmf \\[ f(x|\\theta) = \\theta^x (1-\\theta)^{1-x} \\] for \\(x\\in\\{0,1\\}\\) and \\(0&lt;\\theta&lt;1/2\\). Find the MLE of \\(\\theta\\), and the MOM estimator based on \\(\\bar X\\). Find the mean squared error of each of the estimators. Which estimator is preferred? Justify your choice. If \\(X_1,\\dots,X_n\\) are iid \\(\\mathop{\\mathrm{N}}(\\mu,\\sigma^2)\\), show that the MLEs are \\[ \\hat\\mu =\\bar X \\hspace{2em}\\text{and}\\hspace{2em} \\hat\\sigma^2 = \\frac{n-1}{n}S^2, \\] and obtain the bias, variance and mean squared error of each estimator. Let \\(Y_1,\\dots,Y_n\\) be a sample from a Poisson distribution with mean \\(\\theta&gt;0\\) unknown. Let \\(Y=Y_1+\\dots+Y_n\\). Find the mean and variance of the distribution of \\(Y\\). Hint: Find out the mgf of \\(Y\\). Obtain the MLE for \\(\\theta\\) and its standard error. Suppose now that only the first \\(m\\) (\\(m&lt;n\\)) observations of the sample are known explicitly, while for the other \\(n-m\\) only their sum, \\(Z\\) say, is known. Determine the MLE of \\(\\theta\\). Let \\(X_1,\\dots,X_n\\) be a sample from \\(\\mathop{\\mathrm{Unif}}(0,\\theta)\\) where \\(\\theta&gt;0\\) is an unknown parameter. Find the MLE \\(\\hat\\theta\\) for \\(\\theta\\). Derive the distribution for \\(\\hat\\theta\\) and therefore show that \\(\\hat\\theta\\) is a consistent estimator in the sense that \\(\\hat\\theta \\xrightarrow{\\text P}\\theta\\) when \\(n\\to\\infty\\). Hint: \\(\\Pr(\\max_i X_i \\leq y)=\\prod_{i} \\Pr(X_i\\leq y)\\). Let \\(X_1,\\dots,X_n\\) be a random sample from a Bernoulli distribution, i.e. \\(\\Pr(X_i=1)=p=1-\\Pr(X_i=0)\\) for \\(i=1,\\dots,n\\) where \\(p\\in(0,1)\\) is unknown. Let \\(\\theta=p^2.\\) Find the Cramér-Rao lower bound for the variance of unbiased estimators for \\(\\theta\\). Find the MLE \\(\\hat\\theta\\) for the parameter \\(\\theta\\). Show that \\(\\mathop{\\mathrm{E}}(\\hat\\theta) \\neq \\theta\\). Prove Lemma 28 in the lecture slides: Let \\({\\boldsymbol X}\\sim f({\\boldsymbol x}|\\theta)\\), where \\(\\theta\\in\\mathbb{R}\\), and \\(S(\\theta)=\\frac{\\partial}{\\partial \\theta} \\log f({\\boldsymbol X}|\\theta)\\). Prove that under certain regularity conditions (which you may assume to hold), \\(\\mathop{\\mathrm{E}}[S(\\theta)] = 0\\) \\({\\mathcal I}(\\theta) = \\mathop{\\mathrm{Var}}[S(\\theta)]\\) \\({\\mathcal I}(\\theta) = -\\mathop{\\mathrm{E}}[S&#39;(\\theta)]\\) Suppose that \\(X_1,\\dots,X_n\\sim\\mathop{\\mathrm{Poi}}(\\lambda)\\). Let \\(\\theta=e^{-\\lambda}=\\Pr(X_i=0)\\). Consider two estimators for \\(\\theta\\): \\[ U({\\boldsymbol X}) = \\begin{cases} 1 &amp;X_1=0 \\\\ 0 &amp; \\text{otherwise} \\end{cases} \\] \\[\\phi({\\boldsymbol X})=\\left(1-\\frac{1}{n} \\right)^{\\sum_{i=1}^n X_i}\\] Using the fact that \\(S=\\sum_{i=1}^n X_i\\sim\\mathop{\\mathrm{Poi}}(n\\lambda)\\), show that \\(\\mathop{\\mathrm{Var}}(\\phi)&lt;\\mathop{\\mathrm{Var}}(U)\\). Show that \\[ \\frac{1}{\\sqrt n} l&#39;(\\theta|{\\boldsymbol X}) = \\sqrt n\\left(\\frac{1}{n} \\sum_{i=1}^n W_i \\right) \\] where \\(W=\\frac{\\partial f(X_i|\\theta)/\\partial\\theta }{f(X_i|\\theta)}\\) has mean 0 and variance \\({\\mathcal I}_1(\\theta)\\). Here \\({\\mathcal I}_1(\\theta)\\) is the unit Fisher information, i.e. the Fisher information obtained from a single observation \\(X_1\\). Now use the central limit theorem to establish the convergence to \\(\\mathop{\\mathrm{N}}(0,{\\mathcal I}_1(\\theta))\\). Show that \\[ -\\frac{1}{n} l&#39;&#39;(\\theta|{\\boldsymbol X}) = \\frac{1}{n} \\sum_{i=1}^n W_i^2 - \\frac{1}{n} \\sum_{i=1}^n\\frac{\\partial^2 f(X_i|\\theta)/\\partial\\theta^2 }{f(X_i|\\theta)} \\] and that the mean of the first piece is \\({\\mathcal I}_1(\\theta)\\) and the mean of second piece is 0. Apply the WLLN. Let \\(X_1,\\dots,X_n\\,\\overset{\\text{iid}}{\\sim}\\,\\mathop{\\mathrm{Unif}}(0,\\theta)\\). In the lectures, we found the MLE \\(\\hat\\theta=\\max\\{X_1,\\dots,X_n\\}\\) to be a biased estimator, with \\(\\text{Bias}(\\hat\\theta)=-\\theta/(n+1)\\neq 0\\). Find a bias-corrected version of \\(\\hat\\theta\\) (call it \\(\\hat\\phi\\)), of the form \\(\\hat\\phi=c\\hat\\theta\\) for a suitably chosen constant \\(c\\). What is the variance of the estimator \\(\\hat\\phi\\)? In the lectures we found that \\[ \\text{MSE}(\\hat\\theta)=\\theta^2\\left(\\frac{n}{(n+1)^2(n+2)} + \\frac{1}{(n+1)^2} \\right). \\] Find the mse of \\(\\hat\\phi\\) and compare it with the mse of \\(\\hat\\theta\\). Hand-in questions If \\(X_1,\\dots,X_n\\) is a random sample from the \\(\\Gamma(\\alpha,\\beta)\\) distribution, with \\(\\alpha\\) known, find the MLE of \\(\\beta\\). [3 marks] Let \\(X_1,\\dots,X_n\\) be a random sample from the pdf \\(f(x|\\theta)=\\theta x^{-2}\\) for \\(0 &lt; \\theta \\leq x &lt; \\infty\\). What is a sufficient statistic for \\(\\theta\\)? [3 marks] Find the MLE of \\(\\theta\\). [4 marks] Let \\(X_1,\\dots,X_n\\) be a random sample from \\(\\mathop{\\mathrm{N}}(\\mu,\\sigma^2)\\). Find the Fisher information for \\(\\mu\\) and \\(\\sigma^2\\). Hint: It may be easier to consider \\(\\theta=\\sigma^2\\) in your calculations. [4 marks] "],["hypothesis-testing.html", "Chapter 5 Hypothesis testing", " Chapter 5 Hypothesis testing Hello Learning objectives By the end of this chapter, you will be able to: do this do that and this Readings Casella and Berger (2002) Chapter 8, sections 8.1, 8.2 (8.2.1 only), and 8.3 (8.3.1, 8.3.2 and 8.3.4 only). Chapter 10, section 10.3. Wasserman (2004) All of Chapter 10 Topics not covered here: Bayesian tests, union-intersection and intersection-union tests, score test "],["introduction-2.html", "5.1 Introduction", " 5.1 Introduction The task: to assess what the data say about the plausibility of a specific hypothesis about \\(\\theta\\), e.g. a simple hypothesis of the form \\[ H_0: \\theta = \\theta_0 \\] where \\(\\theta_0\\) is a specified candidate value for \\(\\theta\\), typically corresponding to an underlying subject-matter theory. Some examples: In tossing a coin, \\(\\theta = 1/2\\) means that the coin is ‘fair’ Is the true average height of males in Brunei truly \\(\\mu=1.65\\)? In linear regression, test the significance of the slope parameter \\(\\beta_1=0\\) A hypothesis under test is often called the null hypothesis, because it often relates to the absence (or nullity) of some conceivable effect. In the coin hypothesis example, \\(\\theta=1/2\\) corresponds to absence of bias towards heads or tails. The null hypothesis is often more complex than this, specifying a set of \\(\\theta\\) values, say \\(\\theta\\in\\Theta_0\\), rather than a single value. This is known as a composite hypothesis. From Chapter 4, we already have a notion of relative plausibility for two candidate parameter values \\(\\theta_1\\) and \\(\\theta_2\\), namely the likelihood ratio \\[ \\frac{L(\\theta_1|{\\boldsymbol x})}{L(\\theta_2|{\\boldsymbol x})}. \\] Plainly, the use of the LR boils down to either “accepting” the \\(\\theta_1\\) value, or rejecting it in favour of \\(\\theta_2\\). For instance, if this ratio is found to be much larger than 1, then \\(\\theta_1\\) is much more plausible than \\(\\theta_2\\) on the basis of the data \\({\\boldsymbol x}\\). We will see how likelihood ratios are the key to an optimal assessment of the plausibility of a hypothesis. 5.1.1 A general paradigm A general paradigm: Identify, somehow, a test statistic \\(W({\\boldsymbol X})\\), which is such that larger values of \\(W\\) represent stronger evidence against \\(H_0\\); Measure the strength of the evidence against \\(H_0\\) in any realised value \\(W({\\boldsymbol x})\\) by calculating the \\(p\\)-value (see next slide). If the \\(p\\)-value is very small, then evidence as strong as \\(W({\\boldsymbol x})\\) (or stronger) is found only rarely under \\(H_0\\), and so \\(W({\\boldsymbol x})\\) represents strong evidence against \\(H_0\\). 5.1.2 \\(p\\)-values Definition 5.1 (\\(p\\)-value) Let \\(W({\\boldsymbol X})\\) be a test statistic such that large values of \\(W\\) give evidence that \\(H_1\\) is true. For each sample point \\({\\boldsymbol x}\\), define the \\(p\\)-value to be \\[ p_\\theta({\\boldsymbol x}) = \\sup_{\\theta\\in\\Theta_0} \\Pr\\!{}_\\theta\\left(W({\\boldsymbol X}) \\geq W({\\boldsymbol x}) \\right). \\] In statistical hypothesis testing, the \\(p\\)-value (or probability value) is the probability of obtaining test results at least as extreme as the results actually observed during the test, assuming that the null hypothesis is correct. Some general remarks: The \\(p\\)-value is a statistic. The \\(p\\)-value is indeed a probability value which lies between 0 and 1. The \\(p\\)-value reports the result of a test on a more continuous scale, rather than just the dichotomous decision “Reject/Do not reject \\(H_0\\)”. Example 5.1 Let \\(X_1,\\dots,X_{20} \\in \\{T,H\\}\\) be the outcomes of an experiment of tossing a coin 20 times, i.e. \\[\\Pr(X = H) = \\pi = 1 - \\Pr(X=T), \\ \\ \\ \\pi \\in (0,1).\\] Let \\(W= [X_1=H] + \\cdots + [X_{20}=H]\\). Then \\(W\\sim\\mathop{\\mathrm{Bin}}(20,\\pi)\\), and an estimate of \\(\\pi\\) is \\(\\hat\\pi=\\bar X\\). We would like to assess whether or not the hypothesis that “the coin is fair” is true. That is, \\[H_0: \\ \\pi = 0.5 \\hspace{2em}\\text{v.s.}\\hspace{2em} H_1: \\ \\pi \\neq 0.5\\] Let \\(W\\) be the test statistic, and suppose we observe \\(W=17\\). Intuitively, large values of \\(W\\) indicate evidence against the coin being fair, and would favour the Heads' outcome more thanTails’. Under the assumption \\(H_0:\\pi=0.5\\) is true, then \\[\\begin{align*} p({\\boldsymbol X}) = \\Pr_{\\pi=0.5} (W \\geq 17) = \\sum_{w=17}^{20} \\frac{20!}{w!(20-w)!} 0.5^w(1-0.5)^{20-w} = 0.0013 \\end{align*}\\] This is the (one-sided) \\(p\\)-value favouring the ‘Heads’ outcome. On the other hand, small values of \\(W\\) indicate evidence against the coin being fair; evidence in favour of a ‘Tails’ outcome being more likely than a ‘Heads’. Let \\(Y\\) be the number of Tails observed, so \\(Y=20-W\\), which has a \\(\\mathop{\\mathrm{Bin}}(20,1-\\pi)\\) distribution. The \\(p\\)-value for the observed \\(Y=20-17=3\\) observation would be \\[\\begin{align*} p({\\boldsymbol X}) = \\Pr_{\\pi=0.5} (Y \\leq 3) = \\sum_{y=0}^{3} \\frac{20!}{y!(20-y)!} 0.5^y(1-0.5)^{20-y} = 0.0013 \\end{align*}\\] This is the (one-sided) \\(p\\)-value favouring the ‘Tails’ outcome, which is the same as above due to symmetry. Combining the two \\(p\\)-values together gives the two-sided \\(p\\)-value, \\(p({\\boldsymbol X})=0.0026\\). This gives a measure of how unlikely \\(H_0:\\pi=0.5\\) holds given the observed 17 out of 20 `Heads’ outcome. As a remark, the answer cannot possibly be resulted from the estimator \\(\\hat\\pi\\), for if \\(\\hat\\pi=0.9\\), then \\(H_0\\) is unlikely to be true. if \\(\\hat\\pi=0.45\\), then \\(H_0\\) is may be true (but also may be untrue). if \\(\\hat\\pi=0.7\\), then what? Furthermore, \\(\\hat\\pi=\\bar X\\) is a random variable, so will vary from sample to sample! 5.1.3 Accept \\(H_0\\)? It is not possible to “prove” a negative. When the \\(p\\)-value is large, it means that there is a lack of evidence to prove something exists–it does not prove something does not exist! Not reject \\(\\neq\\) Accept A statistical test is to accept a hypothesis. A large \\(p\\)-value is indeed indicative of the null hypothesis being likely, but the philosophically correct attitude would be to conclude that (as opposed to accepting the null). With this in mind, note that for the most part we will be viewing the statistical testing problem as a problem in which one of two actions is going to be taken: the assertion of \\(H_0\\) or \\(H_1\\). At the end of the day, we can never know for certain what the truth is; we can only act on probability and likelihood based on the observed data. 5.1.4 Uniformity of \\(p\\)-values Here’s an interesting fact: Theorem 5.1 (Uniformity of \\(p\\)-values) If \\(\\theta_0\\) is a point null hypothesis for the parameter of continuous \\({\\boldsymbol X}\\), then a correctly calculated \\(p\\)-value \\(p_W({\\boldsymbol X})\\) based on any test statistic \\(W\\), is such that \\[ p_w({\\boldsymbol X}) \\sim \\mathop{\\mathrm{Unif}}(0,1) \\] in repeated sampling under \\(H_0\\). This result is useful especially for checking the validity of a complicated \\(p\\)-value calculation: Simulate (on a computer) several new data sets from the null distribution. For each simulated data set, apply the \\(p\\)-value calculation and save the result. Assess the collection of resulting \\(p\\)-values–do they seem to be uniformly distributed? Proof. This is a consequence of the probability integral transform: Suppose that a continuous r.v. \\(T\\) has cdf \\(F_T(t), \\forall t\\). Then the r.v. \\(Y=F_T(T)\\sim\\mathop{\\mathrm{Unif}}(0,1)\\) because: \\[ F_Y(y)=\\Pr(Y\\leq y) = \\Pr(F_T(T)\\leq y) = \\Pr\\big(T \\leq F^{-1}_T(y)\\big) = F_T\\left(F^{-1}_T(y) \\right) = y, \\] which is the cdf of a \\(\\mathop{\\mathrm{Unif}}(0,1)\\) distribution. For any data \\({\\boldsymbol x}\\), \\[ p_W({\\boldsymbol x}) = \\Pr\\!{}_{\\theta_0}\\left(W({\\boldsymbol X}) \\geq W({\\boldsymbol x}) \\right) = 1 - F\\big( W({\\boldsymbol x}) \\big), \\] where \\(F\\) is the cdf (under \\(H_0\\)) of \\(W({\\boldsymbol X})\\). Hence, \\(p_W({\\boldsymbol x})=1-Y\\) where \\(Y\\sim\\mathop{\\mathrm{Unif}}(0,1)\\) by the probability integral transform. But clearly if \\(Y\\sim\\mathop{\\mathrm{Unif}}(0,1)\\), then so is \\(1-Y\\). Probability Integral Transform "],["likelihood-ratio-test.html", "5.2 Likelihood ratio test", " 5.2 Likelihood ratio test The likelihood ratio test (LRT) is a general approach to finding a test statistic. Definition 5.2 (Likelihood ratio test) For a model with parameter space \\(\\Theta\\), the likelihood ratio test statistic for testing a specified null hypothesis \\[ H_0: \\theta \\in \\Theta_0 \\] where \\(\\Theta_0\\subset \\Theta\\), is \\[ W_{LR}({\\boldsymbol X}) = \\frac{\\sup_{\\theta\\in\\Theta} L(\\theta|{\\boldsymbol X})}{\\sup_{\\theta\\in\\Theta_0} L(\\theta|{\\boldsymbol X})}. \\] The statistic \\(W_{LR}({\\boldsymbol X})\\) measures the implausibility of the most plausibile \\(\\theta\\) value in \\(\\Theta_0\\), relative to the most plausible value in the whole of \\(\\Theta\\). Thus, larger values of \\(W_{LR}({\\boldsymbol X})\\) represent stronger evidence \\(H_0\\), i.e. large values \\(\\Rightarrow\\) reject \\(H_0\\). Note that \\[ \\hat \\theta = \\sup_{\\theta\\in\\Theta} L(\\theta|{\\boldsymbol X}) \\] is the (unconstrainted) ML estimator for \\(\\theta\\). Further, define \\[ \\tilde \\theta = \\sup_{\\theta\\in\\Theta_0} L(\\theta|{\\boldsymbol X}) \\] as the constrained ML estimator under \\(H_0\\). Then the LRT statistic can be written \\[ W_{LR}({\\boldsymbol X}) = \\frac{f(\\hat\\theta|{\\boldsymbol X})}{f(\\tilde\\theta|{\\boldsymbol X})}, \\] where \\({\\boldsymbol X}= (X_1,\\dots,X_n)^\\top \\sim f({\\boldsymbol x}|\\theta)\\). Remark: It is easy to see that \\(W_{LR}({\\boldsymbol X}) \\geq 1\\). 5.2.1 Log likelihood ratio test statistic As with the likelihood, it is often more convenient to consider the logarithm of the likelihood ratio test statistic: \\[\\begin{align*} \\log W_{LR}({\\boldsymbol X}) = \\log \\frac{L(\\hat\\theta|{\\boldsymbol X})}{L(\\tilde\\theta|{\\boldsymbol X})} &amp;= l(\\hat\\theta|{\\boldsymbol X}) - l(\\tilde \\theta|{\\boldsymbol X})\\\\ &amp;= \\log f(\\hat\\theta|{\\boldsymbol X}) - \\log f(\\tilde\\theta|{\\boldsymbol X}) \\end{align*}\\] The sampling distribution is of interest, but usually unknown, except in a few special cases. Two strategies: Identify a different statistic with an ``easy’’ distribution in the (log) LR statistic, which is an increasing function of the actual (log) LR statistic, and use this to instead. Use asymptotic results to find an approximate distribution. We’ll cover this in later sections. 5.2.2 Example: Normal with known variance Example 5.2 Suppose that \\(n\\) patients use a new drug for hypertention, and we wish to assess the drug’s effectiveness. Measurements of blood pressure are taken before and after treatment, resulting in the measured different \\(X_i\\) for patient \\(i\\). Let’s assume that The BP measurements are all iid: \\(X_1,\\dots,X_n\\,\\overset{\\text{iid}}{\\sim}\\,\\mathop{\\mathrm{N}}(\\mu,\\sigma^2)\\) with known variance. The effect of the drug is the same improvement \\(\\mu\\) for all patients. We wish to test the null hypothesis \\(H_0:\\mu=0\\). The log-likelihood is \\[ l(\\mu|{\\boldsymbol X}) = \\text{const.}- \\frac{1}{2\\sigma^2}\\sum_{i=1}^n (X_i-\\mu)^2 \\] and so, recalling that \\(\\hat\\mu=\\bar X\\), the log of the LR statistic is \\[\\begin{align*} \\log W_{LR} &amp;= l(\\hat\\mu|{\\boldsymbol X}) - l(\\tilde \\mu|{\\boldsymbol X}) \\\\ &amp;= - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n (X_i-\\bar X)^2 + \\frac{1}{2\\sigma^2}\\sum_{i=1}^n X_i^2 \\\\ &amp;= \\frac{n\\bar X^2}{2\\sigma^2}. \\end{align*}\\] Now notice that this statistic is an increasing function of \\(|\\bar X|\\). We use the distribution of the sample mean statistic, \\(\\bar X \\sim \\mathop{\\mathrm{N}}(\\mu,\\sigma^2/n)\\). So for a given data vector \\({\\boldsymbol X}={\\boldsymbol x}\\), the \\(p\\)-value is \\[\\begin{align*} \\Pr_{\\mu=0} \\left( | \\bar X | \\geq | \\bar x | \\right) &amp;= 2\\Pr_{\\mu=0} \\left( \\bar X \\geq | \\bar x | \\right) \\\\ &amp;=2\\Pr \\left( \\frac{\\bar X-0}{\\sigma/\\sqrt n} \\geq \\frac{|\\bar x|-0}{\\sigma/\\sqrt n} \\right) \\\\ &amp;= 2\\big(1-\\Phi(\\sqrt n |\\bar x|/\\sigma)\\big) \\end{align*}\\] Let’s put in some numbers: \\(n=10\\) patients \\(\\sigma=4.3\\) mmHg \\(\\bar x = -12.8\\) mmHg –an apparent reduction in average blood pressure after treatment. Now compute the \\(p\\)-value: \\[\\begin{align*} p(\\bar x) &amp;= 2\\left(1-\\Phi\\left( \\frac{\\sqrt n |\\bar x|}{\\sigma} \\right)\\right) = 2\\left(1-\\Phi\\left( \\frac{\\sqrt{10} \\times 12.8}{4.3} \\right)\\right) \\approx 10^{-11} \\end{align*}\\] A very small value indeed, indicating very strong evidence against the null hypothesis (i.e. clear evidence the drug has an effect). However, note the assumptions above. Are they realistic? 5.2.3 Example: Normal with unknown variance (\\(t\\)-test) Example 5.3 Suppose that \\({\\boldsymbol X}=(X_1,\\dots,X_n)^\\top\\) is a random sample from \\(\\mathop{\\mathrm{N}}(\\mu,\\sigma^2)\\). We are interested in testing hypotheses \\[H_0: \\ \\mu = \\mu_0 \\hspace{1em}\\text{v.s.}\\hspace{1em} H_1: \\ \\mu \\neq \\mu_0,\\] where \\(\\mu_0\\) is given, and \\(\\sigma^2\\) is unknown and is a nuisance parameter. Recall the log-likelihood function as being \\[l(\\mu,\\sigma^2|{\\boldsymbol X}) = \\text{const.}-\\frac{n}{2}\\log\\sigma^2 -\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (X_i-\\mu)^2,\\] and maximising this without restriction yields \\[\\hat\\mu = \\bar X \\hspace{1em}\\text{and}\\hspace{1em} \\hat\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^n(X_i-\\bar X)^2\\] On the other hand, under \\(H_0\\), \\(\\mu\\) is fixed at \\(\\mu_0\\), while the constrained MLE for \\(\\sigma^2\\) is \\[\\tilde\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^n(X_i-\\mu_0)^2.\\] The LR statistic (after simplification) is then \\[W_{LR} = \\frac{L(\\hat\\mu,\\hat\\sigma^2)}{L(\\mu_0,\\tilde\\sigma^2)} = \\left( \\frac{\\tilde\\sigma^2}{\\hat\\sigma^2}\\right)^{n/2}.\\] Since \\(\\tilde\\sigma^2 = \\hat\\sigma^2 + (\\bar X - \\mu_0)^2\\), it holds that \\(\\tilde\\sigma^2/\\hat\\sigma^2 = 1 + T^2/(n-1)\\), where \\[T = \\frac{\\bar X - \\mu_0}{\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2 \\Big/ n}} = \\frac{\\bar X - \\mu_0}{S/\\sqrt n} \\sim t_{n-1}.\\] We thus see that \\(W_{LR}\\) is an increasing function of \\(|T|\\), and hence the \\(p\\)-value in this case is obtained from a table of the \\(t_{n-1}\\) distribution rather than the standard normal. This, the so-called \\(t\\)-test, is probably the most commonly used of all procedures in statistical practice! Now you know how it is derived… For the \\(t\\)-test, under \\(H_0\\), \\(X_i\\,\\overset{\\text{iid}}{\\sim}\\,\\mathop{\\mathrm{N}}(\\mu_0,\\sigma^2)\\). So we simulate a data set \\(\\{X_1,\\dots,X_{n}\\}\\) using these parameters: \\(n=15, \\sigma=2, \\mu_0=2\\). X &lt;- rnorm(n = 15, mean = 2, sd = 2) round(X, 3) ## [1] 4.448 2.720 2.802 2.221 0.888 5.574 2.996 -1.933 3.403 1.054 ## [11] -0.136 1.564 -0.052 0.542 0.750 The \\(p\\)-value for the \\(t\\)-test is \\(\\Pr\\big(|Y|&gt; |\\sqrt n(\\bar x - \\mu_0)/s|\\big)\\), where \\(Y\\sim t_{n-1}\\) (the two-tail probability of “extreme events”). For instance, test.stat.obs &lt;- abs(sqrt(15) * (mean(X) - 2) / sd(X)) pval &lt;- 2 * pt(test.stat.obs, df = 15 - 1, lower.tail = FALSE) pval ## [1] 0.6800548 Simulate this B=10000 times in a for loop: B &lt;- 10000 res &lt;- rep(NA, B) # create a vector to collect the p-values for (i in 1:B) { X &lt;- rnorm(n = 15, mean = 2, sd = 2) test.stat.obs &lt;- abs(sqrt(15) * (mean(X) - 2) / sd(X)) pval &lt;- 2 * pt(test.stat.obs, df = 15 - 1, lower.tail = FALSE) res[i] &lt;- pval } head(res) ## [1] 0.42203902 0.73961301 0.57621365 0.35418373 0.06711971 0.10304763 Plot a histogram of the simulated \\(p\\)-values. We should observe uniformity: "],["the-neyman-pearson-approach.html", "5.3 The Neyman-Pearson approach", " 5.3 The Neyman-Pearson approach The ‘Neyman-Pearson’ approach to testing hypotheses is to reject \\(H_0\\) if \\(W({\\boldsymbol X})\\in R\\), where \\(R\\) is a suitably defined critical region. If \\(W\\) is designed to measure the evidence against \\(H_0\\), then most often \\(R\\) takes the form \\[ R = \\{{\\boldsymbol x}\\mid W({\\boldsymbol x}) \\geq c \\} \\] for some constant \\(c\\). Example 5.4 From Example 5.2, we saw that \\(W = \\exp(n\\bar X^2/2\\sigma^2)\\) for testing \\(H_0:\\mu=0\\) from a normal sample with known variance. The rejection region is therefore \\[\\begin{align*} R &amp;= \\{{\\boldsymbol x}\\mid \\exp(n\\bar X^2/2\\sigma^2) \\geq c \\} \\\\ &amp;= \\left\\{{\\boldsymbol x}\\,\\Big|\\, |\\bar X| \\geq \\sqrt {2\\sigma^2\\log c /n} \\right\\} \\end{align*}\\] So the LR test rejects \\(H_0:\\mu=0\\) if the sample mean exceeds a specified amount. 5.3.1 Performance of a test In deciding to “accept” or reject the null hypothesis \\(H_0\\), an experimenter might be making a mistake. The performance of a test is measured by two criteria: the size and power of a test. Definition 5.3 (Size of a test) For \\(0\\leq\\alpha\\leq 1\\), the size \\(\\alpha\\) of a test is defined as \\[ \\alpha := \\sup_{\\theta\\in\\Theta_0} \\Pr\\!{}_\\theta\\left( W({\\boldsymbol X}) \\in R \\right) \\] The size of a test measures the probability of rejecting the null hypothesis under the assumption that the null hypothesis is true. Definition 5.4 (Power of a test) For \\(0\\leq B(\\theta)\\leq 1\\), the power \\(B(\\theta)\\) of a test is defined as \\[ B(\\theta) := \\Pr\\!{}_\\theta\\left( W({\\boldsymbol X}) \\in R \\right), \\hspace{2em} \\theta\\not\\in\\Theta_0 \\] The power function of a test is defined as the probability of rejecting the null hypothesis correctly (i.e. \\(\\theta\\not\\in\\Theta_0\\)) in favour of the alternative. A good test \\((W,R)\\) has size \\(\\alpha\\) and power \\(B(\\theta)\\) at all values of \\(\\theta\\) outside of the null hypothesis. Example 5.5 Continuation of normal example with known variance: \\(X_1,\\dots,X_n\\,\\overset{\\text{iid}}{\\sim}\\,\\mathop{\\mathrm{N}}(\\mu,\\sigma^2)\\) with \\(\\sigma^2\\) known and null hypothesis \\(H_0:\\mu=0\\). The rejection region from Example 5.4 is alternatively written as \\[ R = \\{{\\boldsymbol x}\\mid \\exp(n\\bar X^2/2\\sigma^2) \\geq c \\} = \\bigg\\{{\\boldsymbol x}\\,\\Big|\\, \\left| \\frac{\\bar X}{\\sigma/\\sqrt n} \\right| \\geq \\sqrt {2\\log c } \\bigg\\}. \\] So for instance, \\(R=\\{{\\boldsymbol x}\\mid |\\sqrt n \\bar X/\\sigma| \\geq 1.96 \\}\\) is a critical region of size 0.05. For our illustrative data, with \\(\\sigma=4.3\\) and \\(\\bar x = -12.8\\), \\[ \\left| \\frac{\\bar X}{\\sigma/\\sqrt n} \\right| = \\left| \\frac{-12.8}{4.3/\\sqrt 10} \\right| = 9.413 &gt; 1.96. \\] For a test of size \\(\\alpha=0.05\\), the power of the test is \\[\\begin{align*} B(\\mu) &amp;= \\Pr\\bigg\\{\\left| \\frac{\\bar X {\\color{gray}-\\mu +\\mu}}{\\sigma/\\sqrt n} \\right| \\geq 1.96 \\bigg\\} \\\\ &amp;= \\Pr\\bigg\\{\\frac{\\bar X -\\mu}{\\sigma/\\sqrt n} \\leq -1.96 -\\frac{\\mu}{\\sigma/\\sqrt n} \\bigg\\} + \\Pr\\bigg\\{\\frac{\\bar X -\\mu}{\\sigma/\\sqrt n} \\geq 1.96 -\\frac{\\mu}{\\sigma/\\sqrt n} \\bigg\\} \\\\ &amp;=\\Phi\\left(-1.96 -\\sqrt n \\mu / \\sigma \\right) + \\left[1 - \\Phi\\left(1.96 -\\sqrt n \\mu / \\sigma \\right)\\right] \\end{align*}\\] This represents the two tail probabilities based on the rejection region. For our illustrated example (\\(\\sigma=4.3, n=10\\)), the power function is plotted below. This plots the power of the test assuming some value of \\(\\mu\\) is true. If \\(\\mu=-12.8\\) (as observed in the data) then the power is almost 1! 5.3.2 Relation to \\(p\\)-values The conclusion of the test (with the illustrated data) is that “\\(H_0\\) is rejected at the 5% level (of significance)”. This is interpeted to mean If \\(H_0\\) were true, we would reject \\(H_0\\) using this test only 5% of the time in repeated sampling. So this is fairly strong evidence against \\(H_0\\). But that is not a very informative summary of the evidence! In fact, with these data, we would also reject \\(H_0\\) at the 1% level, and at the 0.1% level, etc. It would be much more informative to ask: “What is the smallest size of test based on \\(W\\) that would reject \\(H_0\\) based on the data \\({\\boldsymbol x}\\)?” The answer is precisely the \\(p\\)-value, \\(p_W({\\boldsymbol x})\\). So the two approaches are closely linked, with the \\(p\\)-value giving the most informative assessment of the strength of evidence against \\(H_0\\). "],["type-i-and-ii-errors.html", "5.4 Type I and II errors", " 5.4 Type I and II errors The quantities \\(\\alpha\\) and \\(\\beta(\\theta):= 1-B(\\theta)\\) are called the probability of a ‘Type I error’ and a ‘Type II error’ respectively. Definition 5.5 (Type I and II error) The Type I error (false positive) is defined to be \\[ \\alpha = \\Pr(\\text{Reject } H_0 \\mid H_0 \\text{ is true}). \\] The Type II error (false negative) is defined to be \\[ \\beta(\\theta) = \\Pr(\\text{Fail to reject } H_0 \\mid H_0 \\text{ is false}) = 1 - B(\\theta). \\] Figure 5.1: Summary of Type I and II errors. \\(H_0\\) is true \\(H_1\\) is false Do not reject \\(H_0\\) Correct inference (true negative) prob. = \\(1-\\alpha\\) Type II error (false positive) prob. = \\(\\alpha\\) Reject \\(H_0\\) Type I error (false positive) prob. = \\(\\alpha\\) Correct inference (true negative) prob. = \\(1-\\beta(\\theta)\\) 5.4.1 Minimising errors The aim is to make both Type I and II errors as small as possible, simultaneously. However, for a large value of \\(c\\) in the rejection region will give small \\(\\alpha\\) and large \\(\\beta\\), and vice versa for a small value of \\(c\\). This conflict is usually resolved by fixing \\(\\alpha\\), say at 0.05 or 0.01, and then using a test \\((W,R)\\) that makes \\(\\beta(\\theta)\\) as small as possible for all \\(\\theta\\not\\in\\Theta_0\\). Some remarks: Suppose that \\(H_0\\) is true, rejection of the null hypothesis occurs if \\(p\\)-value is small. But the probability of this error (Type I) is not greater than the size of the test \\(\\alpha\\). Hence, it is under control. Unfortunately, we do not have explicit control on the probability \\(\\beta\\) of making a Type II error. But we can certainly gauge the conditions resulting in large \\(\\beta\\) and try to avoid them. It is more conclusive to end a test with \\(H_0\\) rejected, as the decision “Not reject” does not imply that \\(H_0\\) is accepted. 5.4.2 Optimality of the LR test If we can’t control the Type II error of a test, are we out of luck? The Neyman-Pearson approach provides some neat theory! Lemma 5.1 (Neyman-Pearson) Consider testing the simple hypothesis \\(H_0:\\theta=\\theta_0\\), suppose that \\(\\theta_1\\) is any other candidate value of \\(\\theta\\); \\(W_{LR}({\\boldsymbol X}) = \\frac{L(\\theta_1|{\\boldsymbol X})}{L(\\theta_0|{\\boldsymbol X})}\\); \\(R_{LR} = \\{{\\boldsymbol x}\\mid W_{LR}({\\boldsymbol X}) \\geq c \\}\\) s.t. \\(\\Pr_{\\theta_0}(W_{LR} \\in R_{LR})=\\alpha\\). Then no other size \\(\\alpha\\) test pair \\((W,R)\\) has \\(\\Pr_{\\theta_1}(W \\in R)\\) greater than \\(\\Pr_{\\theta_1}(W_{LR} \\in R_{LR})\\). The proof is omitted (see for e.g. C&amp;B Thm 8.3.12 or on Wikipedia). The implication is that since this result applies for every possible value of \\(\\theta_1\\), the LR test \\((W_{LR}, C_{LR})\\) is said to be the uniformly most powerful (UMP) test of size \\(\\alpha\\). This makes the use of \\(W_{LR}\\) very compelling for hypothesis testing, whether via the \\(p\\)-value approach or the critical-region approach. "],["one-sided-tests.html", "5.5 One-sided tests", " 5.5 One-sided tests Sometimes we wish to measure the evidence (against \\(H_0\\)) in one direction only. Example 5.6 Suppose \\(X_1,\\dots,X_n\\,\\overset{\\text{iid}}{\\sim}\\,\\mathop{\\mathrm{N}}(\\mu,\\sigma^2)\\) with \\(\\sigma^2\\) known. Consider testing \\[ H_0: \\mu \\leq 0 \\hspace{1em}\\text{v.s.}\\hspace{1em} H_1: \\mu &gt; 0 \\] The unrestricted MLE is \\(\\hat\\mu=\\bar X\\), while the restricted MLE is \\(\\tilde\\mu=0\\) if \\(\\bar X&gt;0\\). So for \\(\\bar X &gt; 0\\), we have (as before) \\[ W_{LR}({\\boldsymbol X}) = \\frac{L(\\hat\\mu|{\\boldsymbol X})}{L(0|{\\boldsymbol X})} = \\exp\\left(n\\bar X^2/2\\sigma^2\\right). \\] But if \\(\\bar X\\leq 0\\), \\(W_{LR}({\\boldsymbol X})=1\\), because \\(\\hat\\mu=0\\) in such a case. The \\(p\\)-value from data \\({\\boldsymbol x}\\) is (using the monotonicity of \\(\\bar X\\) in the LRT statistic) \\[ p({\\boldsymbol x}) = \\begin{cases} \\Pr(\\bar X &gt; \\bar x) = 1-\\Phi(\\sqrt n \\bar x / \\sigma) &amp;\\bar x &gt; 0 \\\\ 1 &amp;\\bar x \\leq 0 \\end{cases} \\] Hence, relative to the ‘two-sided’ test that we saw previously, the \\(p\\)-value is if \\(\\bar x &gt; 0\\), and ignores the precise value of \\(\\bar x\\) if \\(\\bar x \\leq 0\\). It’s a good idea to sketch the likelihood function above. Further remarks: Performing a one-sided test instead of a two-sided test thus makes any apparent evidence against \\(H_0\\) seem stronger (since the \\(p\\)-value is halved). In practice there are rather few situations where performing a one-sided test, which assumes that we know in advance that departures from \\(H_0\\) are in one direction only, can be justified. When assessing the effect of a new drug, for example, the convention is to assess evidence for an effect in either direction, positive or negative. The two-sided test is said to be more conservative than the one-sided test: The one-sided test risks over-stating the strength of evidence against \\(H_0\\) if the underlying assumption–that evidence against \\(H_0\\) counts in one direction only–is actually false. "],["approximate-tests.html", "5.6 Approximate tests", " 5.6 Approximate tests 5.6.1 Asymptotic distribution of LRTs We cannot always derive easily the distribution of \\(W_{LR}\\) under \\(H_0\\). But a general large-sample approximation to the null distribution of \\(W_{LR}\\) comes from the following result Theorem 5.2 For testing \\(H_0:\\theta=\\theta_0\\) against \\(H_1:\\theta \\neq\\theta_0\\), suppose \\(X_1,\\dots,X_n\\,\\overset{\\text{iid}}{\\sim}\\,f(x|\\theta)\\), and \\(f(x|\\theta)\\) satisfies the usual regularity conditions. Let \\(\\hat\\theta_n\\) be the MLE for \\(\\theta\\). Then under \\(H_0\\), as \\(n\\to\\infty\\), \\[ -2\\log \\left[ \\frac{L(\\theta_0|{\\boldsymbol X})}{L(\\hat\\theta_n|{\\boldsymbol X})} \\right]= 2\\log W_{LR}({\\boldsymbol X}) \\xrightarrow{\\text{D}} \\chi^2_1. \\] For the two-sided testing situation, we can always get an approximate \\(p\\)-value for the observed data as \\(p({\\boldsymbol x}) = \\Pr\\big(Y\\geq 2\\log W_{LR}({\\boldsymbol x})\\big)\\), where \\(Y\\sim\\chi^2_1\\). Remarkably, this result applies whatever the distribution of the \\(X_i\\)s are. It is partly a result of the asymptotic normality of \\(\\hat\\theta\\) (see proof). Proof. Taylor expanding \\(l(\\theta|bX)\\) around \\(\\hat\\theta\\) gives \\[ l(\\theta|{\\boldsymbol X}) = l(\\hat\\theta|{\\boldsymbol X}) + \\cancel{(\\theta-\\hat\\theta)l&#39;(\\hat\\theta|{\\boldsymbol X})} + \\frac{(\\theta-\\hat\\theta)^2}{2!}l&#39;&#39;(\\hat\\theta|{\\boldsymbol X}) + \\cdots \\] Consider then quantity \\(2\\log W_{LR}\\) under the assumption that \\(H_0:\\theta=\\theta_0\\) is true: \\[\\begin{align*} 2\\log W_{LR} &amp;= 2l(\\hat\\theta|{\\boldsymbol X}) - 2l(\\theta_0|{\\boldsymbol X}) \\\\ &amp;\\approx \\cancel{2l(\\hat\\theta|{\\boldsymbol X})} - \\cancel{2l(\\hat\\theta|{\\boldsymbol X})} - (\\theta_0-\\hat\\theta)^2 l&#39;&#39;(\\hat\\theta|{\\boldsymbol X}) \\end{align*}\\] Recall that \\(-l&#39;&#39;(\\hat\\theta|{\\boldsymbol X})\\) is the so-called (Part 4 slides, p.71), and that \\(-\\frac{1}{n}l&#39;&#39;(\\hat\\theta|{\\boldsymbol X}) \\xrightarrow{\\text P} {\\mathcal I}_1(\\theta_0)\\) (Ex. sheet 4, Q14b). Since MLEs are, under certain regularity conditions, asymptotically efficient, we have that as \\(n\\to\\infty\\) under \\(H_0\\), \\[ \\sqrt n(\\hat\\theta - \\theta_0) \\xrightarrow{\\text D} \\mathop{\\mathrm{N}}\\big(0,{\\mathcal I}_1(\\theta_0)^{-1}\\big). \\] It follows that \\(\\sqrt{{\\mathcal I}_1(\\theta_0)}\\cdot \\sqrt n(\\hat\\theta - \\theta_0) \\xrightarrow{\\text D} \\mathop{\\mathrm{N}}(0,1)\\) and that \\[ {\\mathcal I}_1(\\theta_0) \\cdot n(\\hat\\theta - \\theta_0)^2 \\xrightarrow{\\text D} \\chi^2_1, \\] and hence \\[ 2\\log W_{LR} = -\\frac{l&#39;&#39;(\\hat\\theta|{\\boldsymbol X})}{n}\\cdot n(\\hat\\theta - \\theta_0)^2 \\xrightarrow{\\text D} \\chi^2_1 \\] by application of Slutzky’s theorem. 5.6.2 Wilk’s theorem The above theorem can be extended to cases where the null hypothesis concerns vectors of parameters, i.e. \\(\\Theta\\subseteq \\mathbb{R}^p\\). We state it here without proof. Example 5.7 Let \\(X_1,\\dots,X_n\\) be independent, and \\(X_i\\sim\\mathop{\\mathrm{N}}(\\mu_i,1)\\). Consider the null hypothesis \\[H_0: \\mu_1 = \\cdots = \\mu_n.\\] The likelihood function (up to a constant of proportionality) is \\[L(\\mu_1,\\dots,\\mu_n) \\propto \\exp\\left\\{-\\frac{1}{2}\\sum_{i=1}^n (X_i-\\mu_i)^2 \\right\\},\\] Then, the unconstrained MLE are \\(\\hat\\mu_i=X_i\\), while the constrained MLE is \\(\\tilde\\mu=\\bar X\\). Hence, \\[W_{LR} = \\frac{L(\\hat\\mu_1,\\dots,\\hat\\mu_n)}{L(\\tilde\\mu,\\dots,\\tilde\\mu)} = \\exp\\left\\{\\frac{1}{2}\\sum_{i=1}^n (X_i-\\bar X)^2 \\right\\}.\\] The asymptotic distribution of \\(2\\log W_{LR}\\) is \\[2\\log W_{LR} = \\sum_{i=1}^n (X_i-\\bar X)^2 \\xrightarrow{\\text{D}} \\chi^2_{n-1}\\] as \\(n\\to\\infty\\) by Wilk’s theorem. Thus, the null hypothesis is rejected for large values of \\(2\\log W_{LR}\\) as compared to the \\(\\chi^2_{n-1}\\) distribution. The (approximate) \\(p\\)-value is \\[ p({\\boldsymbol x}) = \\Pr\\left(Y &gt; \\sum_{i=1}^n (x_i-\\bar x)^2 \\right), \\hspace{2em} Y\\sim\\chi^2_{n-1} \\] It turns out that \\(2\\log W_{LR}\\) has an \\(\\chi^2_{n-1}\\) distribution since \\((n-1)^{-1}2\\log W_{LR}=S^2\\) (the unbiased sample variance), and we saw previously that \\((n-1)S^2/\\sigma^2 \\sim\\chi^2_{n-1}\\). 5.6.3 The Wald test Another common method of constructing a large-sample test statistic is based on an estimator that has an asymptotic normal distribution (e.g. the MLE). Definition 5.6 (Wald test) Let \\(X_1,\\dots,X_n \\,\\overset{\\text{iid}}{\\sim}\\,f(x|\\theta)\\) and suppose we would like to test \\(H_0:\\theta=\\theta_0\\). Let \\(\\hat\\theta_n\\) be an estimator for \\(\\theta\\) which is asymptotically normal, i.e. as \\(n\\to\\infty\\), \\[ \\sqrt n (\\hat\\theta_n - \\theta) \\xrightarrow{\\text D} \\mathop{\\mathrm{N}}\\left(0,{\\mathcal I}_1(\\theta)^{-1}\\right), \\] where \\({\\mathcal I}_1(\\theta)\\) is the (unit) Fisher information about \\(\\theta\\). Write \\(\\text{se}(\\hat\\theta_n)\\) as the estimate of the s.d. of \\(\\hat\\theta_n\\), \\(n/\\sqrt{{\\mathcal I}_1(\\theta)^{-1}}\\). A Wald test is a test based on a statistic of the form \\[ Z_n := \\frac{\\hat\\theta_n - \\theta_0}{\\text{se}(\\hat\\theta_n)} \\approx \\mathop{\\mathrm{N}}(0,1) \\] where \\(\\theta_0\\) is the hypothesised value of \\(\\theta\\) (under \\(H_0\\)). Some remarks regarding the Wald test: The asymptotic efficiency property actually affords us \\[ Z_n := \\frac{\\hat\\theta_n - \\theta_0}{\\text{sd}(\\hat\\theta_n)} \\approx \\mathop{\\mathrm{N}}(0,1) \\] but \\(\\text{sd}(\\hat\\theta_n)\\) may depend on some unknown parameters. If \\(\\text{sd}(\\hat\\theta_n) / \\text{se}(\\hat\\theta_n) \\xrightarrow{\\text P} 1\\) then we may use the standard error instead. As discussed in Chapter 4, there are two versions of obtaining the standard error: Using the plug-in estimator: \\(\\text{se}(\\hat\\theta_n) = 1\\Big/\\sqrt{{\\mathcal I}(\\hat\\theta_n)}\\) Using the observed Fisher information: \\(\\text{se}(\\hat\\theta_n) = 1\\Big/\\sqrt{-l&#39;&#39;(\\hat\\theta_n|{\\boldsymbol X})}\\) The Wald test is very practical since there are no assumptions made on the distribution of the data \\(X_i\\). Of course, it is an approximate test and the “reliability” of the test depends on the sample size. In fact, the Wald test can be shown to have an asymptotic size \\(\\alpha\\) and power 1 22. There are some disadvantages to the Wald test: The Wald test is not invariant to a non-linear transformation/reparameterisation of the hypothesis. One might get different answers to the test of \\(H_0:\\theta=1\\) and \\(H_0:\\log\\theta=0\\) (although they ask the same thing). The reason for this is there is no relationship (in general) between the two standard errors (e.g. \\(\\text{se}(\\hat\\theta_n)\\) and \\(\\text{se}(\\log \\hat\\theta_n)\\)) so they need to be approximated somewhat independently. The Wald test actually uses two approximations: 1) the normality from the asymptotic efficiency property; and 2) the use of (approximate) standard errors. In contrast, the LRT only uses “one” approximation, and that is the large-sample \\(\\chi^2\\) distribution of \\(2\\log W_{LR}\\). Example 5.8 To deal with a coffee shop’s customer complaint that the amount of chilled coffee in their bottled drinks is less than the advertised 300ml, 20 bottles were decanted and the coffee measured, yielding data \\(X_i\\) as follows: 282 301 311 271 293 268 302 301 293 256 278 301 309 294 282 281 305 301 285 279 The sample mean and the (unbiased) sample standard deviation are \\[ \\bar x = 289.7 \\text{ ml} \\hspace{3em} s = 14.8. \\] which are taken as estimates of the population mean and standard deviation \\(\\mu\\) and \\(\\sigma\\) respectively. By the CLT, the sample mean estimator is asymptotically efficient: \\(\\sqrt{n} \\, (\\bar X-\\mu) \\xrightarrow{\\text D}\\mathop{\\mathrm{N}}\\left(0, \\sigma^2\\right)\\). From this, an (approximate) standard error of the estimator \\(\\bar X\\) is \\(\\text{se}(\\bar X)=s/\\sqrt n\\). To test \\[ H_0: \\ \\mu = 300 \\hspace{1em}\\text{v.s.}\\hspace{1em} H_1: \\ \\mu &lt; 300, \\] we apply the Wald test with an observed test statistic value of \\[ z = \\frac{\\bar X - 300}{14.8/\\sqrt{20}} = -3.121. \\] The critical region for a test of size \\(\\alpha=0.01\\) is \\(\\left\\{{\\boldsymbol x}\\mid Z \\leq -2.326 \\right\\}\\). Thus the test rejects \\(H_0:\\mu=300\\) at the 1% significance level. Alternatively, the \\(p\\)-value can be calculated: \\[ p({\\boldsymbol x}) = \\Pr_{\\mu=300}\\left(\\frac{\\bar X - \\mu}{\\text{se}(\\bar X)} \\leq \\frac{\\bar x - \\mu}{\\text{se}(\\bar X)}\\right) = \\Phi\\left(-3.121 \\right) = 0.0009 \\] Either way, the conclusion is that there is significant evidence which supports the claim that the bottled coffee is less than the advertised value of 300 ml. Check out §10.3.2 in C&amp;B↩︎ "],["exercises-4.html", "5.7 Exercises", " 5.7 Exercises Suppose we observe \\(n\\) iid \\(\\mathop{\\mathrm{Bern}}(\\theta)\\) random variables, denoted by \\(Y_1,\\dots,Y_n\\). Show that the LRT of \\(H_0:\\theta \\leq \\theta_0\\) versus \\(H_1:\\theta&gt;\\theta_0\\) will reject \\(H_0\\) if \\(\\sum_{i=1}^n Y_i &gt; c\\). A sample \\(X\\) of size 1 was obtained where \\(X\\) has one of the following distributions: \\(X\\) \\(H_0\\) \\(H_1\\) \\(x_1\\) 0.2 0.1 \\(x_2\\) 0.3 0.4 \\(x_3\\) 0.3 0.1 \\(x_4\\) 0.2 0.4 Compare the likelihood ratio \\(W\\) for each possible value \\(X\\), and order the \\(x_i\\) according to \\(W\\). What is the likelihood ratio test of \\(H_0\\) versus \\(H_1\\) at level \\(\\alpha=0.2\\)? What is the test at level \\(\\alpha=0.5\\)? A random sample, \\(X_1,\\dots,X_n\\) is drawn from a Pareto population with pdf \\[ f(x|\\theta,\\nu) = \\frac{\\theta \\nu^\\theta}{x^{\\theta + 1}} \\mathop{\\mathrm{\\unicode{x1D7D9}}}_{[\\nu,\\infty)}(x), \\theta,\\nu &gt;0. \\] Find the MLEs of \\(\\theta\\) and \\(\\nu\\). Show that the LRT of \\[ H_0:\\theta=1, \\nu \\text{ unknown} \\hspace{2em}\\text{v.s.}\\hspace{2em}H_1:\\theta\\neq1, \\nu \\text{ unknown} \\] has critical region of the form \\(\\{{\\boldsymbol x}\\mid T({\\boldsymbol x}) \\leq c_1 \\text{ or } T({\\boldsymbol x})\\geq c_2 \\}\\), where \\(0&lt;c_1&lt;c_2\\) and \\[ T = \\log \\left[ \\frac{\\prod_{i=1}^n X_i}{\\left\\{\\min_i X_i \\right\\}^n}\\right]. \\] We will derive the distribution of the test statistic for comparing two normal means from two independent samples with unknown variances. Let \\(X_1,\\dots,X_n \\,\\overset{\\text{iid}}{\\sim}\\,\\mathop{\\mathrm{N}}(\\mu_x,\\sigma^2)\\); and \\(Y_1,\\dots,Y_m\\,\\overset{\\text{iid}}{\\sim}\\,\\mathop{\\mathrm{N}}(\\mu_y,\\sigma^2)\\) independently of the \\(X\\)s. Denote by \\(\\bar X\\) and \\(\\bar Y\\) the sample mean of the \\(X\\)s and \\(Y\\)s respectively. Further let \\(S_x^2\\) and \\(S_y^2\\) denote the unbiased sample variance for \\(X\\) and \\(Y\\) respectively, and let \\[S^2 = \\frac{(n-1)S^2_x+(m-1)S^2_y}{n+m-2}\\] be the pooled sample variance of the data. Write down the distribution of \\(\\bar X - \\bar Y\\) \\((n-1)S^2_x/\\sigma^2 + (m-1)S^2_y/\\sigma^2\\) Based on your answers to part (a), derive the distribution of the statistic \\[ T = \\frac{(\\bar X - \\bar Y) - (\\mu_x - \\mu_y)}{S\\sqrt{\\frac{1}{n}+\\frac{1}{m}}} \\] Explain how you would use the above statistic to conduct a test of the hypothesis \\(H_0:\\mu_x=\\mu_y\\). Let \\(X_1,\\dots,X_n\\) be an independent random sample from \\(\\mathop{\\mathrm{N}}(\\mu,\\sigma^2)\\), where both the mean and variance parameters are unknown. We shall derive a statistical test for \\(\\sigma^2\\). Let \\(X_1,\\dots,X_n\\) be an independent random sample from \\(\\mathop{\\mathrm{N}}(\\mu,\\sigma^2)\\), where both the mean and variance parameters are unknown. We shall derive a statistical test for \\(\\sigma^2\\). For testing \\(H_0:\\sigma^2=\\sigma_0^2\\) against \\(H_1:\\sigma^2 \\neq \\sigma_0^2\\), write down the likelihood ratio test statistic and show that the rejection region of this test simplifies to, for some constant \\(k\\), \\[ R = \\left\\{{\\boldsymbol x}\\ \\bigg| \\left(\\frac{\\hat\\sigma^2}{\\sigma^2_0}\\right)^{n/2} \\exp\\left( -\\frac{n}{2}\\cdot\\frac{\\hat\\sigma^2}{\\sigma^2_0} \\right) \\leq k \\right\\}. \\] By considering the function \\(f(x)=x^ae^{-ax}\\), argue that the corresponding rejection regions are \\[ \\frac{\\hat\\sigma^2}{\\sigma^2_0} \\leq k_1 \\hspace{2em}\\text{or}\\hspace{2em} \\frac{\\hat\\sigma^2}{\\sigma^2_0} \\geq k_2 \\] for some constants \\(k_1\\) and \\(k_2\\). Find \\(k_1\\) and \\(k_2\\) that makes the size of test 0.05. Remark: this test is exact for normal samples and we do not require the use of asymptotics. Let \\(X_1,\\dots,X_n\\,\\overset{\\text{iid}}{\\sim}\\,\\mathop{\\mathrm{N}}(\\theta,1)\\). Consider testing \\[ H_0: \\theta = 0 \\hspace{2em}\\text{v.s.}\\hspace{2em} H_1:\\theta =1 \\] Let the rejection region be \\(R=\\{{\\boldsymbol x}\\mid T({\\boldsymbol x}) &gt; c \\}\\) where \\(T({\\boldsymbol x}) = n^{-1}\\sum_{i=1}^n X_i\\). Find \\(c\\) so that the test has size \\(\\alpha\\). Find the power under \\(H_1\\), i.e. find \\(B(1)\\). Show that \\(B(1)\\to 1\\) as \\(n\\to \\infty\\). Let \\(\\hat\\theta_n\\) be the MLE of a parameter \\(\\theta\\) and let \\(\\text{se}(\\hat\\theta_n)\\) be the standard error for the parameter \\(\\theta\\). Consider testing \\[ H_0: \\theta = \\theta_0 \\hspace{2em}\\text{v.s.}\\hspace{2em} H_1:\\theta \\neq\\theta_0. \\] Using the Wald test with rejection region \\(R= \\left\\{ {\\boldsymbol x}\\mid |Z_n| &gt; z(\\alpha/2) \\right\\}\\), where \\[ Z_n = \\frac{\\hat\\theta_n - \\theta_0}{\\text{se}(\\hat\\theta_n)}, \\] and \\(z(a)\\) is the top \\(a\\)-th point, \\(0\\leq a\\leq 1\\), of the standard normal distribution, show that the power of the test \\(B(\\theta)\\to 1\\) as \\(n\\to\\infty\\) for any value of \\(\\theta&gt;\\theta_0\\). A survey of the use a particular product was conducted in four areas, and a random sample of 200 potential users was interviewed in each area. In area \\(i\\), for \\(i = 1, 2, 3, 4\\), \\(X_i\\) of the 200 said that they used the product. Construct a likelihood ratio test to test whether the proportion of the population using the product is the same in each of the four areas. Carry out the test at 5% level for the case \\(X_1 = 76\\), \\(X_2 = 53\\), \\(X_3 = 59\\) and \\(X_4 = 48\\). In a given city it is assumed that the number of automobile accidents in a given year follows a Poisson distribution. In past years, the average number of accidents per year was 15, and this year it was 10. Is it justified to claim that the accident rate has dropped? Hint: Cast this into a statistical testing problem: Define the null hypothesis, calculate the \\(p\\)-value of observing the data under the null, and give your conclusion. The number of chocolate chips in a packet of Chipsmore cookies is well described by a Poisson distribution with mean 130 (chocolate chips per packet). Following the Kraft takeover of Cadbury (who produces Chipsmore cookies), the mean number of chocolate chips per packet reduced to 75. Fans of the beloved cookie bellowed in anger at the apparent evidence that Kraft has reduced the number of chocolate chips in Chipsmore cookies. Assess the apparent evidence and come to your own conclusion. Hint: Set up a hypothesis, calculate the likelihood ratio, and obtain an approximate \\(p\\)-value. In 1861, 10 essays appeared in the New Orleans Daily Crescent. They were signed “Quintus Curtius Snodgrass” and some people suspected they were actually written by Mark Twain. To investigate this, we will consider the proportion of three letter words found in an author’s work. From 8 of Twain’s essays, the proportions are: 0.225 0.262 0.217 0.240 0.230 0.229 0.235 0.217 From 10 Snodgrass essays, the proportions are: 0.209 0.205 0.196 0.210 0.202 0.207 0.224 0.223 0.220 0.201 Perform a Wald test for equality of the means. Report the \\(p\\)-value and a 95% confidence interval for the difference of means. What do you conclude? Hand-in questions Suppose that \\(X_1,\\dots,X_n\\) and \\(Y_1,\\dots,Y_m\\) are two independent random samples from two exponential distributions with mean \\(\\theta\\) and \\(\\mu\\) respectively. Find the LRT of \\(H_0:\\theta = \\mu\\) versus \\(H_1:\\theta\\neq\\mu\\). [4 marks] Show that the test in part (a) can be based on the statistic \\[ T = \\frac{\\sum_{i=1}^n X_i}{\\sum_{i=1}^n X_i + \\sum_{j=1}^m Y_j}. \\] [2 marks] Specify the asymptotic distribution of the LRT statistic under \\(H_0\\) found in a. [1 mark] Let \\(X_1,\\dots,X_n\\,\\overset{\\text{iid}}{\\sim}\\,\\mathop{\\mathrm{Unif}}(0,\\theta)\\) and let \\(Y=\\max\\{X_1,\\dots,X_n\\}\\). We want to test \\(H_0:\\theta = 1/2\\) versus \\(H_1:\\theta &gt; 1/2\\). Explain briefly why the Wald test would not be appropriate in this case. [1 mark] Suppose we decide to test this hypothesis by rejecting \\(H_0\\) when \\(Y &gt; c\\). Derive an expression for the power function \\(B(\\theta)\\). [2 marks] What choice of \\(c\\) will make the size of the test 0.05? [2 marks] In a sample of size \\(n=20\\) with \\(Y=0.48\\), what conclusion about \\(H_0\\) would you make? [2 marks] "],["interval-estimation.html", "Chapter 6 Interval estimation", " Chapter 6 Interval estimation Learning objectives By the end of this chapter, you will be able to: do this do that and this Readings Casella and Berger (2002) Chapter 9, sections 9.1, 9.2 (9.2.1 and 9.2.2 only), 9.3 (9.3.1 only) Chapter 10, section 10.4. Wasserman (2004) Chapter 6, section 6.3.2 All of Chapter 8 (Bootstrap) Topics not covered here: Bayesian intervals, pivots based on cdfs, test-related optimality, Bayesian optimality, loss function optimality, sinterval using core statistic "],["introduction-3.html", "6.1 Introduction", " 6.1 Introduction The task: to report a set \\(C\\subset \\Theta\\) of plausible values for the unknown parameter \\(\\theta\\), rather than a single point estimate. The set \\(C=C({\\boldsymbol x})\\) is a set determined by the value of the observed data \\({\\boldsymbol X}= {\\boldsymbol x}\\) (thus, the set is a random variable!); and will often be an interval in \\(\\mathbb{R}\\) (if \\(\\theta\\in\\mathbb{R}=:\\Theta\\))–hence ‘interval estimation’. Sometimes, the set of most plausible values may not be an interval. 6.1.1 Coverage probability We’ll start with some formal definitions. Let \\(C({\\boldsymbol X})\\) be a region of the parameter space \\(\\Theta\\), determined by the sample \\({\\boldsymbol X}\\). Definition 6.1 (Coverage probability) For any given value of \\(\\theta\\), the coverage probability of \\(C({\\boldsymbol X})\\) is \\[ \\Pr_\\theta \\left( \\theta \\in C({\\boldsymbol X}) \\right) =: c(\\theta) \\] In words: coverage is the proportion of times that the (random) interval \\(C({\\boldsymbol X})\\) contain the parameter value of interest \\(\\theta\\). Of course, we are interested how well the interval covers the true value of the parameter. 6.1.2 Confidence regions Since we do not know the true value of \\(\\theta\\), we can only guarantee a coverage probability equal to the infimum of \\(c(\\theta)\\) (called the confidence coefficient). We call such a set a confidence region. Definition 6.2 (Confidence region) The set \\(C({\\boldsymbol X})\\) is said to be a confidence region with confidence coefficient \\(c\\) if \\[ c = \\inf_\\theta c(\\theta). \\] In applications, \\(c\\) is typically fixed at some suitably large value such as 95% or 99%. That is, we want to “build” a confidence region that has a high chance of capturing the true value of \\(\\theta\\). Some remarks: The random variable here is the set \\(C({\\boldsymbol X})\\). The confidence coefficient is simply a statement about the repeated sampling properties of such a set. \\(C({\\boldsymbol X})\\) includes the true \\(\\theta\\) in at least 100\\(c\\)% of samples. In a frequentist setting, \\(\\Pr_\\theta \\left( \\theta \\in C({\\boldsymbol X}) \\right)\\) does not refer to ``the probability of \\(\\theta\\) being in \\(C\\)’’ (however, in the Bayesian setting it does). Rather, these probability statements refer to \\({\\boldsymbol X}\\) and its randomness, and not \\(\\theta\\). We have so far more generally described a \\(C({\\boldsymbol X})\\), but if it is a random interval, \\(C({\\boldsymbol X})=[L({\\boldsymbol X}), U({\\boldsymbol X})]\\) say, then \\(C({\\boldsymbol X})\\) is said to be a confidence interval with confidence coefficient \\(c\\). Estimating an unknown parameter \\(\\theta\\) with a set, rather than a point, seems imprecise. However, we gain some assurance that we capture the true value \\(\\theta\\) within the set. Example 6.1 For a sample \\(X_1,X_2,X_3,X_4\\,\\overset{\\text{iid}}{\\sim}\\,\\mathop{\\mathrm{N}}(\\mu,1)\\), an interval estimator of \\(\\mu\\) could be \\[ C({\\boldsymbol X}) = [\\bar X - 1, \\bar X + 1]. \\] That is, we assert that \\(\\mu\\) is within this interval. Realise that the probability that we are exactly correct when we estimate \\(\\mu\\) by \\(\\bar X\\) is \\(\\Pr(\\mu=\\bar X) = 0\\). On the other hand, \\[\\begin{align*} \\Pr\\left( \\mu \\in [\\bar X - 1, \\bar X + 1] \\right) &amp;= \\Pr(-1 \\leq \\bar X - \\mu \\leq 1) \\\\ &amp;= \\Pr\\left( -2 \\leq \\sqrt{4}(\\bar X - \\mu) \\leq 2 \\right) \\approx 0.95 \\end{align*}\\] Thus, we have a 95% chance of covering the unknown parameter with our interval estimator. Sacrificing precision in our estimate results in an increased confidence of a true assertion. Expanding on the previous example: Let \\(\\mu=0\\) be the true value. A random sample of size 4 is obtained as follows set.seed(123) (X &lt;- rnorm(4, mean = 0, sd = 1)) ## [1] -0.56047565 -0.23017749 1.55870831 0.07050839 mean(X) ## [1] 0.2096409 A 95% confidence interval based on the sample mean is c(mean(X) - 1, mean(X) + 1) ## [1] -0.7903591 1.2096409 In this case, the true value \\(\\mu=0\\) is indeed contained within the interval. If we repeated this experiment many times, what proportion of the intervals would contain \\(\\mu=0\\)? Here’s the R code: B &lt;- 10000 # number of replications res &lt;- data.frame(L = rep(NA, B), U = NA, contain = NA) # prepare the results data frame for (i in 1:B) { X &lt;- rnorm(4, mean = 0, sd = 1) L &lt;- mean(X) - 1 U &lt;- mean(X) + 1 contain &lt;- (L &lt;= 0) &amp; (0 &lt;= U) # is 0 contained? res[i, ] &lt;- c(L, U, contain) } mean(res$contain) # coverage rate ## [1] 0.9537 As expected, we get a ~95% coverage with the interval \\([\\bar X - 1, \\bar X + 1]\\). Graphically, we can see this below. Of the first 100 random replications and construction of confidence intervals, here exactly 5 do not contain the true value, whereas 95 confidence intervals contain the true value (95%). 6.1.3 Methods for obtaining confidence regions We will consider two general approaches: Use of a pivot Inversion of a hypothesis test As we shall see, the second is really just a special case of the first. In the same spirit, large-sample theory of maximum likelihood and of likelihood ratio tests will be found to deliver approximate confidence regions in situations where it is hard to evaluate coverage probabilities exactly. "],["pivots.html", "6.2 Pivots", " 6.2 Pivots Definition 6.3 (Pivot) Suppose that the distribution of \\({\\boldsymbol X}\\) is determined by an unknown parameter \\(\\theta\\). A pivotal quantity, or just pivot for short, is any function \\(Q({\\boldsymbol X},\\theta)\\) whose distribution is the same for all values of \\(\\theta\\). That is, the random variable \\(Q({\\boldsymbol X},\\theta)\\) is independent of all parameters \\(\\theta\\): The function \\(Q({\\boldsymbol X},\\theta)\\) will usually explicitly contain both parameters and statistics, but for any set \\({\\mathcal A}\\), \\(\\Pr_\\theta\\left( Q({\\boldsymbol X},\\theta) \\in {\\mathcal A}\\right)\\) cannot depend on \\(\\theta\\). From this, we can construct a confidence set for \\(\\theta\\) by \\[ \\{\\theta \\mid Q({\\boldsymbol x},\\theta) \\in {\\mathcal A}\\} \\] Let \\(X_1,\\dots,X_n\\,\\overset{\\text{iid}}{\\sim}\\,\\mathop{\\mathrm{N}}(\\mu,\\sigma^2)\\). Here, the three functions \\[ Q_1 = \\frac{\\bar X - \\mu}{\\sigma/\\sqrt n} \\hspace{2em} Q_2 = \\frac{\\bar X - \\mu}{S/\\sqrt n} \\hspace{2em} Q_3 = \\frac{(n-1)S^2}{\\sigma^2} \\] are all pivots. \\(Q_2\\) and \\(Q_3\\) may be used respectively for interval estimation of \\(\\mu\\) and \\(\\sigma\\). Let \\(X_1,\\dots,X_n\\,\\overset{\\text{iid}}{\\sim}\\,\\mathop{\\mathrm{Exp}}(\\lambda)\\). Here, \\(\\lambda\\) is a scale parameter, so each \\(X_i/\\lambda\\) is a pivot, and so is \\[Q({\\boldsymbol X})=\\bar X / \\lambda.\\] What are their distributions? Check that the distribution of \\(\\bar X / \\lambda\\) is \\(\\Gamma(n,1/n)\\). Hint: First check that \\(X_i/\\lambda \\sim \\mathop{\\mathrm{Exp}}(1)\\)! 6.2.1 From pivot to confidence interval Let \\(Q({\\boldsymbol X},\\theta)\\) be a pivot, and \\(c\\) a specified confidence coefficient (such as \\(c=0.95\\)). Proposition 6.1 (Pivotal confidence interval) Suppose we can find constants \\(a\\) and \\(b\\) such that \\[ \\Pr_\\theta\\left( a \\leq Q({\\boldsymbol X},\\theta) \\leq b \\right) = c. \\] Then, \\(C({\\boldsymbol X})=\\left\\{\\theta \\, \\big| \\, a \\leq Q({\\boldsymbol X},\\theta) \\leq b \\right\\}\\) is a 100\\(c\\)% confidence interval for \\(\\theta\\) Proof. This immediately follows from the definition. Example 6.2 Let \\(X_1,\\dots,X_n\\,\\overset{\\text{iid}}{\\sim}\\,\\mathop{\\mathrm{N}}(\\mu,\\sigma^2)\\). To construct a confidence interval for \\(\\mu\\), let’s use the pivot \\[ Q = \\frac{\\bar X - \\mu}{S/\\sqrt n} \\sim t_{n-1}. \\] So if \\(a\\) and \\(b\\) are such that \\[ \\Pr(Q \\leq a) = \\Pr(Q \\geq b) = (1-c)/2 \\] then \\(a=-b\\) and \\[ \\Pr_\\mu\\left(-b \\leq Q \\leq b \\right) = c \\Leftrightarrow \\Pr_\\mu \\left(\\bar X - bS/\\sqrt n \\leq \\mu \\leq \\bar X + bS/\\sqrt n \\right) = c. \\] Thus, the interval \\[ C({\\boldsymbol X}) = \\left[\\bar X - b \\frac{S}{\\sqrt n}, \\bar X + b \\frac{S}{\\sqrt n} \\right] \\] is a 100\\(c\\)% confidence interval for \\(\\mu\\). As an illustration, suppose that \\(n=20\\), \\(\\bar X=8.31\\) and \\(S = 1.97\\). Then, \\[ C({\\boldsymbol X}) = 8.31 \\pm 2.093 \\cdot \\frac{1.97}{\\sqrt{20}}= [7.38, 9.23]. \\] is a 95% confidence interval for \\(\\mu\\). Check, from the statistical tables, that \\(b=2.093\\) for \\(c=0.95\\) and \\(n=20\\). Example 6.3 Let \\(X_1,\\dots,X_n\\,\\overset{\\text{iid}}{\\sim}\\,\\mathop{\\mathrm{N}}(\\mu,\\sigma^2)\\). To construct a confidence interval for \\(\\sigma^2\\), let’s use the pivot \\[ Q = \\frac{(n-1)S^2}{\\sigma^2} \\sim \\chi^2_{n-1} \\] So if \\(a\\) and \\(b\\) are such that \\[ \\Pr(Q \\leq a) = \\Pr(Q \\geq b) = (1-c)/2 \\] then \\[ \\Pr_{\\sigma^2}\\left(a \\leq Q \\leq b \\right) = c \\Leftrightarrow \\Pr_{\\sigma^2} \\left((n-1)S^2/b \\leq \\sigma^2 \\leq (n-1)S^2/a \\right) = c. \\] Thus, the interval \\[ C({\\boldsymbol X}) = \\left[\\frac{(n-1)S^2}{b}, \\frac{(n-1)S^2}{a} \\right] \\] is a 100\\(c\\)% confidence interval for \\(\\sigma^2\\). As an illustration, suppose that \\(n=20\\), \\(S^2 =4.8\\). Then, \\[ C({\\boldsymbol X}) = \\left[\\frac{19 \\times 4.8}{32.85}, \\frac{19 \\times 4.8}{8.907} \\right] = [2.78, 10.24]. \\] is a 95% confidence interval for \\(\\sigma^2\\). Check, from the statistical tables, that \\(a=8.907\\) and \\(b=32.85\\) for \\(c=0.95\\) and \\(n=20\\) Notice how wide this interval is! I.e., the point estimate of \\(\\sigma^2\\) was \\(S^2=4.8\\), while the 95% confidence interval includes values more than twice that. Accurate estimation of a variance, in general, requires \\(n\\) to be fairly large. \\(n=20\\) is clearly not large enough to allow \\(\\sigma^2\\) to be determined very accurately. Consider \\(n=250\\). Then the lower and upper limits of the \\(\\chi^2_{249}\\) are 207.2 and 294.6 respectively. The confidence interval is then \\[ C({\\boldsymbol X}) = \\left[\\frac{249 \\times 4.8}{294.6}, \\frac{249 \\times 4.8}{207.2} \\right] = [4.06, 5.77]. \\] Example 6.4 Let \\(X_1,\\dots,X_n\\,\\overset{\\text{iid}}{\\sim}\\,\\mathop{\\mathrm{Exp}}(\\lambda)\\). To construct a confidence interval for \\(\\lambda\\), we could use \\(\\bar X / \\lambda \\sim \\Gamma(n,1/n)\\), but it’s much more convenient to use \\[ Q = \\frac{2n\\bar X}{\\lambda} \\sim \\Gamma(2n/2, 2) \\equiv \\chi^2_{2n}. \\] Here, we have used the fact that if \\(Y\\sim\\Gamma(\\alpha,\\beta)\\) with \\(\\alpha=k/2\\) and \\(\\beta=2\\), then \\(Y\\sim\\chi^2_k\\). So if \\(a\\) and \\(b\\) are such that \\(\\Pr(Q \\leq a) = \\Pr(Q \\geq b) = (1-c)/2\\), then \\[ \\Pr_{\\lambda}\\left(a \\leq Q \\leq b \\right) = c \\Leftrightarrow \\Pr_\\lambda \\left(2n\\bar X/b \\leq \\lambda \\leq 2n\\bar X/a \\right) = c. \\] Thus, the interval \\[ C({\\boldsymbol X}) = \\left[\\frac{2n\\bar X}{b}, \\frac{2n\\bar X}{a} \\right] \\] is a 100\\(c\\)% confidence interval for \\(\\lambda\\). As an illustration, suppose that \\(n=20\\) and \\(\\bar X =8.3\\). Then, \\[ C({\\boldsymbol X}) = \\left[\\frac{2\\times 20 \\times 8.3}{59.34}, \\frac{2\\times 20 \\times 8.3}{24.43} \\right] = [5.59, 13.59]. \\] is a 95% confidence interval for \\(\\mu\\). ::: {.mycheck} Check, from the statistical tables, that \\(a=24.43\\) and \\(b=59.34\\) for \\(c=0.95\\)::: "],["inverting-a-test-statistic.html", "6.3 Inverting a test statistic", " 6.3 Inverting a test statistic Suppose that \\(W_{\\theta_0}\\) is a test statistic measuring the evidence against \\(H_0:\\theta = \\theta_0\\). When \\(X\\) is continuous, we saw that the \\(p\\)-value \\(p_{W_{\\theta_0}}({\\boldsymbol X})\\) is distributed as \\(\\mathop{\\mathrm{Unif}}(0,1)\\) under \\(H_0\\). Hence, the \\(p\\)-value itself is a pivot since it is free of \\(\\theta\\)! Proposition 6.2 (Confidence interval from pivoting \\(p\\)-values) \\[ C({\\boldsymbol X}) = \\left\\{\\theta_0 \\ \\Big| \\ p_{W_{\\theta_0}}({\\boldsymbol X}) \\geq 1 - c \\right\\} \\] is a 100\\(c\\)% confidence region for \\(\\theta\\). Proof. \\[ \\Pr_\\theta \\left(\\theta \\in C({\\boldsymbol X}) \\right) = \\Pr_\\theta \\left(p_{W_{\\theta_0}}({\\boldsymbol X}) \\geq 1 - c \\right) = \\int_{1-c}^1 \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!k = c. \\] Let \\(A(\\theta)=\\{ {\\boldsymbol x}\\mid W_\\theta({\\boldsymbol x}) &lt; w\\} = R^c\\) for some constant \\(w\\) be the acceptance region of a hypothesis test, i.e. the set in the sample space such that \\(H_0\\) is “accepted”. For a confidence region \\(C({\\boldsymbol X})\\) with confidence coefficient \\(c\\), include all those \\(\\theta\\) values which, when tested, would result in a \\(p\\)-value of at least \\(1-c\\). That is, we want the set of \\({\\boldsymbol X}\\) such that \\[ p_W({\\boldsymbol X}) = \\Pr(W({\\boldsymbol X})\\in R) = 1-\\Pr(W({\\boldsymbol X}) \\in A) \\geq 1-c \\] For example, For a 95% confidence region, include in \\(C({\\boldsymbol X})\\) all those values of \\(\\theta_0\\) which are such that the \\(p\\)-value of evidence against \\(H_0\\) is at least 0.05. Or, in terms of the Neyman-Pearson approach: include in \\(C({\\boldsymbol X})\\) all those values of \\(\\theta\\) that would not be rejected by a test of size 0.05. We’ll look at a more concrete example next. Example 6.5 Let \\(X_1,\\dots,X_n\\,\\overset{\\text{iid}}{\\sim}\\,\\mathop{\\mathrm{N}}(\\mu,\\sigma^2)\\) with \\(\\sigma^2\\) known, and consider testing \\(H_0:\\mu=\\mu_0\\) versus \\(H_1:\\mu\\neq\\mu_0\\). Previously, we saw that for a fixed size \\(\\alpha\\), the rejection region is given by \\[ R = \\left\\{ {\\boldsymbol x}\\ \\bigg| \\ \\left| \\frac{\\bar x-\\mu_0}{\\sigma/\\sqrt n} \\right| \\geq z(\\alpha/2) \\right\\} \\] where \\(z(\\alpha)\\) is the top-\\(\\alpha\\) point of the standard normal distribution. The test does not reject \\(H_0\\) should the observed sample \\({\\boldsymbol X}={\\boldsymbol x}\\) fall in the region \\(\\{{\\boldsymbol x}\\mid |\\bar x - \\mu_0| \\leq z(\\alpha/2)\\sigma/\\sqrt n \\}\\). Those values of \\(\\mu\\) that would not be rejected fall in the region \\[ C({\\boldsymbol X}) = \\left[ \\bar x- z(\\alpha/2)\\frac{\\sigma}{\\sqrt n} \\ , \\ \\bar x + z(\\alpha/2)\\frac{\\sigma}{\\sqrt n} \\right], \\] which makes up a 100\\((1-\\alpha)\\)% confidence interval for \\(\\mu\\). Why? First note that \\(H_0\\) is ``accepted’’ for sample points in the acceptance region \\[ A = \\left\\{ {\\boldsymbol x}\\ \\bigg| \\ \\bar x- z(\\alpha/2)\\frac{\\sigma}{\\sqrt n} \\leq \\mu_0 \\leq \\bar x + z(\\alpha/2)\\frac{\\sigma}{\\sqrt n} \\right\\} = R^c. \\] Since the test has size \\(\\alpha\\), \\[ \\Pr\\!{}_{\\mu_0}(W({\\boldsymbol X}) \\in R) = \\alpha \\Leftrightarrow \\Pr\\!{}_{\\mu_0}(W({\\boldsymbol X}) \\in A) = 1 - \\alpha. \\] But this probability statement is true for every \\(\\mu_0\\). Thus, \\[ \\Pr(\\mu \\in C({\\boldsymbol X})) = \\Pr(W({\\boldsymbol X}) \\in A) = 1 - \\alpha =: c. \\] Some remarks. There is a correspondence between confidence sets and acceptance regions for a hypothesis test: A hypothesis test fixes the parameter value (under \\(H_0\\), \\(\\mu=\\mu_0\\) say) and asks what sample values are consistent with that fixed value, i.e. the test is accepted if it falls in \\[ A(\\mu_0) = \\left\\{ {\\boldsymbol x}\\ \\bigg| \\ \\mu_0- z(\\alpha/2)\\frac{\\sigma}{\\sqrt n} \\leq \\bar x \\leq \\mu_0 + z(\\alpha/2)\\frac{\\sigma}{\\sqrt n} \\right\\} \\] A confidence set fixes the sample value (say we observe \\({\\boldsymbol X}={\\boldsymbol x}^*\\)) and asks what parameter values make this sample value most plausible, i.e. the confidence set are the values of \\(\\mu\\) which fall within \\[ C({\\boldsymbol x}^*) = \\left\\{ \\mu \\ \\bigg| \\ \\bar x- z(\\alpha/2)\\frac{\\sigma}{\\sqrt n} \\leq \\mu \\leq \\bar x + z(\\alpha/2)\\frac{\\sigma}{\\sqrt n} \\right\\} \\] The two are connected by the tautology \\[ {\\boldsymbol x}\\in A(\\mu_0) \\Leftrightarrow \\mu \\in C({\\boldsymbol x}). \\] Example 6.6 In Ex. sheet 5, Q5 we looked at a hypothesis test for the variance of a normal distribution. Let \\(X_1,\\dots,X_n\\,\\overset{\\text{iid}}{\\sim}\\,\\mathop{\\mathrm{N}}(\\mu,\\sigma^2)\\) where both parameters are unknown. The LRT test rejects \\(H_0:\\sigma^2=\\sigma_0^2\\) for samples in the rejection region \\[ R = \\left\\{{\\boldsymbol x}\\ \\bigg| \\ \\frac{\\sum_{i=1}^n (x_i-\\bar x)^2}{n\\sigma_0^2} \\leq k_1 \\ \\text{ or } \\ \\frac{\\sum_{i=1}^n (x_i-\\bar x)^2}{n\\sigma_0^2} \\geq k_2\\right\\} \\] The acceptance region can be alternatively written as \\[ A = \\left\\{{\\boldsymbol x}\\ \\bigg| \\ a \\leq \\ \\frac{(n-1)s^2}{\\sigma_0^2} \\leq b\\right\\} \\] for some constants \\(a\\) and \\(b\\) based on the \\(\\chi^2_{n-1}\\) distribution. From this, we can see that we get the same confidence interval for \\(\\sigma^2\\) based on a pivotal quantity as in Example 6.3. 6.3.1 Discrete distributions When \\({\\boldsymbol X}\\) is discrete, the \\(p\\)-value is no longer uniform under \\(H_0\\). The \\(p\\)-value in that case is stochastically greater than a \\(\\mathop{\\mathrm{Unif}}(0,1)\\) r.v. in the sense that its cdf \\(F\\) satisfies \\(F(x)\\leq x\\), \\(\\forall x\\). Proof. For a continuous r.v. \\(T\\), we saw that \\(Y=F_T(T)\\sim\\mathop{\\mathrm{Unif}}(0,1)\\). However, if \\(T\\) is discrete23, the inverse \\(F_T^{-1}\\) is not defined, and \\[ F_Y(y) = \\Pr(Y \\leq y) = \\Pr(F_T(T)\\leq y) \\leq y. \\] Now for any data \\({\\boldsymbol x}\\), \\[ p_W({\\boldsymbol x}) = \\Pr_{\\theta_0}(W({\\boldsymbol X})\\geq W({\\boldsymbol x})) = \\Pr_{\\theta_0}(-W({\\boldsymbol X})\\leq -W({\\boldsymbol x}))=F_{-W}(-W). \\] Let \\(Y=-W\\) which is discrete, so by the above, the \\(p\\)-values are stochastically greater than a \\(\\mathop{\\mathrm{Unif}}(0,1)\\) r.v.. As a result, if \\({\\boldsymbol X}\\) is discrete, the construction of \\(C({\\boldsymbol X})\\) as in Proposition 6.2 above results in a conservative confidence region. A conservative confidence region allows for a large range with greater probability that the parameter falls in that range. Proposition 6.3 (\\(p\\)-value inversion gives a conservative confidence region) When \\({\\boldsymbol X}\\) is discrete, \\[ C({\\boldsymbol X}) = \\left\\{\\theta_0 \\ \\Big| \\ p_{W_{\\theta_0}}({\\boldsymbol X}) \\geq 1 - c \\right\\} \\] is a 100\\(c\\)% confidence region for \\(\\theta\\), but this confidence region is said to be conservative. Proof. \\[ \\Pr_\\theta \\left(\\theta \\in C({\\boldsymbol X}) \\right) = \\Pr_\\theta \\left(p_{W_{\\theta_0}}({\\boldsymbol X}) \\geq 1 - c \\right) \\geq 1 - (1-c) = c \\] Example 6.7 Suppose that \\(X\\) is a single binary random variable with \\[ \\Pr_\\theta(X = 1) = 1 - \\Pr_\\theta(X = 0) = \\theta, \\hspace{2em} 0&lt;\\theta &lt; 1. \\] Consider the LR test of \\(H_0:\\theta =\\theta_0\\). The MLE is \\(\\hat\\theta=X\\), so the LR statistic is \\[ W_{LR}(X) = \\frac{L(\\hat\\theta|X)}{L(\\theta_0|X)} = \\frac{\\hat\\theta^X(1-\\hat\\theta)^{1-X}}{\\theta_0^X(1-\\theta_0)^{1-X}} = \\begin{cases} \\frac{1}{\\theta_0} &amp;X =1 \\\\ \\frac{1}{1-\\theta_0} &amp; X=0. \\end{cases} \\] Thus, the \\(p\\)-value based on the observed data \\(X=x\\) is \\[ p_{W_{LR}}(x) = \\Pr_{\\theta_0}\\left(W_{LR}(X) \\geq W_{LR}(x) \\right) = \\begin{cases} \\theta_0 &amp;|x-\\theta_0|&gt;1/2 \\\\ 1&amp;|x-\\theta_0|\\leq 1/2 \\end{cases} \\] Suppose we set the confidence coefficient to be \\(c=0.95\\). Then, included in \\(C({\\boldsymbol X})\\) are all values of \\(\\theta_0\\) such that \\(p_{W_{LR}}(x)\\geq 0.05\\). If \\(x=1\\), this is the interval \\([0.05,1)\\); and by symmetry if \\(x=0\\) it is \\((0,0.95]\\). Coverage of such a confidence interval? \\[ c(\\theta) = \\Pr_\\theta(\\theta\\in C({\\boldsymbol X})) = \\begin{cases} 1- \\theta &amp;\\theta &lt; 0.05\\\\ 1 &amp;0.05 \\leq \\theta \\leq 0.95\\\\ \\theta &amp; \\theta &gt; 0.95 \\end{cases} \\] so we see \\(c(\\theta)&gt;0.95\\) for all \\(\\theta\\), so the confidence interval is indeed conservative. See here: https://stats.stackexchange.com/q/73778↩︎ "],["desirable-confidence-sets.html", "6.4 Desirable confidence sets", " 6.4 Desirable confidence sets We have seen two methods for deriving confidence sets (and there are others), and in fact different methods yield different confidence sets. Is there a best one? We desire a confidence set \\(C({\\boldsymbol X})\\) which has small size (for a confidence interval \\(C({\\boldsymbol X})=[L({\\boldsymbol X}), U({\\boldsymbol X})]\\), this means its length \\(U({\\boldsymbol X})-L({\\boldsymbol X})\\)); and large coverage probability \\(\\Pr(\\theta \\in C({\\boldsymbol X}))\\). Often hard to construct–clearly, to increase coverage we need only increase its size. In Example 6.5, we saw the use of the top and bottom \\(\\alpha/2\\) points of the standard normal being used. I.e., the size \\(\\alpha\\) was split equally among the two tails of the distribution. Is this necessary? Suppose \\(1-\\alpha =0.9\\). Then any of the following pairs give 90% intervals: It turns out the strategy of splitting \\(\\alpha\\) equally is optimal if the distribution is unimodal (note: it does not have to be symmetric!). Theorem 6.1 Let \\(f(x)\\) be a unimodal pdf. If the interval \\([a,b]\\) satisfies \\(\\int_a^b f(x) \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!x = c\\); \\(f(a)=f(b)&gt;0\\); and \\(a \\leq x^* \\leq b\\) where \\(x^*\\) is the mode of \\(f(x)\\), then \\([a,b]\\) is the shortest among all intervals that satisfy 1. The proof of this is omitted. See instead C&amp; B Thm 9.3.2. Example 6.8 Suppose \\(X\\sim\\Gamma(\\alpha,\\beta)\\). The quantity \\(Y=X/\\beta\\) is a pivot for \\(\\beta\\), with \\(Y\\sim\\Gamma(\\alpha,1)\\). We can get a confidence interval by finding \\(a\\) and \\(b\\) to satisfy \\[ \\Pr(a \\leq Y \\leq b) = c. \\] However, choosing \\(a\\) and \\(b\\) to satisfy \\(f_Y(a)=f_Y(b)\\) is not optimal, because the interval on \\(\\beta\\) is of the form \\[ C(X) = \\left\\{ x \\ \\bigg| \\ \\frac{x}{b} \\leq \\beta \\leq \\frac{x}{a} \\right\\}, \\] so the length of the interval is \\((1/a-1/b)x\\). That is, it is proportional to \\(1/a-1/b\\) and not \\(b-a\\). "],["intervals-based-on-ml-methods.html", "6.5 Intervals based on ML methods", " 6.5 Intervals based on ML methods Recall the following two asymptotics: Asymptotic efficiency of the MLE, \\[\\sqrt n (\\hat\\theta_n - \\theta) \\xrightarrow{\\text D} \\mathop{\\mathrm{N}}(0,{\\mathcal I}_1(\\theta)^{-1}).\\] Large sample distribution of \\(W_{LR}\\) for testing \\(H_0:\\theta=\\theta_0\\), \\[ -2\\log \\left[ \\frac{L(\\theta_0|{\\boldsymbol X})}{L(\\hat\\theta_n|{\\boldsymbol X})} \\right] \\xrightarrow{\\text D} \\chi^2_1 \\] (or the one based on Wilk’s theorem). These are two ‘automatic’ pivots based on the large sample distributions. So quite generally, an approximate confidence region can be based off maximum likelihood methods. Under certain regularity conditions the MLE is asymptotically normal, and we can make use of the fact that \\[ \\Pr\\left(-z(\\alpha/2) \\leq \\sqrt{{\\mathcal I}(\\theta)} (\\hat\\theta_n - \\theta) \\leq z(\\alpha/2) \\right) \\] to build a 100\\((1-\\alpha)\\)% confidence set for \\(\\theta\\). But this is hard to invert into an interval for \\(\\theta\\). So we simplify things by using the observed Fisher information instead. Definition 6.4 Let \\(X_1,\\dots,X_n\\,\\overset{\\text{iid}}{\\sim}\\,f(x|\\theta)\\), and \\(\\hat\\theta_n\\) be the MLE of \\(\\theta\\). The interval \\[ \\left[\\hat\\theta_n - z(\\alpha/2)\\cdot \\text{se}(\\hat\\theta_n) \\ , \\ \\hat\\theta_n + z(\\alpha/2)\\cdot \\text{se}(\\hat\\theta_n) \\right], \\] with \\(\\text{se}(\\hat\\theta_n) = 1 / \\sqrt{-l&#39;&#39;(\\hat\\theta_n)}\\), is an approximate 100\\((1-\\alpha)\\)% confidence interval for \\(\\theta\\). This is otherwise known as the Wald interval. Definition 6.5 Let \\(X_1,\\dots,X_n\\,\\overset{\\text{iid}}{\\sim}\\,f(x|\\theta)\\), and \\(\\hat\\theta_n\\) be the MLE of \\(\\theta\\). The set \\[ C({\\boldsymbol X}) = \\left\\{\\theta \\ \\bigg| \\ -2\\log \\left[ \\frac{L(\\theta|{\\boldsymbol X})}{L(\\hat\\theta_n|{\\boldsymbol X})} \\right] \\leq \\chi^2_1(\\alpha) \\right\\} \\] is an approximate 100\\((1-\\alpha)\\)% confidence interval. This is simply an inversion of the rejection region for the large-sample LRT test. The confidence region include values of \\(\\theta\\) such that \\(2\\log W_{LR}({\\boldsymbol X})\\sim\\chi^2_1\\) is small. For example, \\(\\Pr(2\\log W_{LR} \\leq 3.84) \\approx 0.95\\), so \\[ C({\\boldsymbol X}) = \\left\\{\\theta \\ \\bigg| \\ 2 l(\\theta |{\\boldsymbol X}) \\geq 2l(\\hat\\theta | {\\boldsymbol X}) - 3.84 \\right\\} \\] is an approximate 95% confidence region for \\(\\theta\\). Which to use? Undoubtedly, the Wald interval is simpler computationally. In comparison, the LRT interval demands the solution of a non-linear equation in order to find the end points. However, the Wald interval is not invariant to a change in parameter, e.g. \\(\\tau=g(\\theta)\\) (this is also a disadvantage of the Wald test), whereas the LRT interval is. The LRT interval approach works much better than the Wald interval when the likelihood is asymmetric or multi-modal. Example 6.9 Consider a single Poisson count, \\(Y\\sim\\mathop{\\mathrm{Poi}}(\\mu)\\). There is no exact pivot in this case, so we’ll build some approximate confidence sets. The log-likelihood gives \\[\\begin{align*} l(\\mu|y) &amp;= \\text{const.}- \\mu + y \\log\\mu \\\\ l&#39;(\\mu|y) &amp;= -1 +y/\\mu \\\\ l&#39;&#39;(\\mu|y) &amp;= -y/\\mu^2 \\end{align*}\\] From this, we have \\(\\hat\\mu=y\\), while \\(-l&#39;&#39;(\\hat\\mu)=1/y\\) (provided that \\(y&gt;0\\)–the method has problems if not!). Consider also the parameter transformation \\(\\tau = \\log \\mu\\), which is a fairly standard one to use in Poisson models24. Then \\[\\begin{align*} l(\\tau|y) &amp;= \\text{const.}- e^\\tau + y \\tau \\\\ l&#39;(\\tau|y) &amp;= - e^\\tau +y \\\\ l&#39;&#39;(\\tau|y) &amp;= - e^\\tau \\end{align*}\\] so (not surprising, since the MLE is invariant to continuous transformations) \\[ \\hat\\tau = \\begin{cases} \\log y &amp; y &gt; 0 \\\\ -\\infty &amp;y=0 \\end{cases} \\] and \\(-l&#39;&#39;(\\hat\\tau)=y\\) (notice that the standard error is not invariant). Approximate 95% confidence intervals for \\(\\mu\\)… based on the MLE \\(\\hat\\mu\\): \\[ [y-1.96\\sqrt y, y+1.96\\sqrt y] \\] based on the MLE \\(\\hat\\tau\\) (and converting it back via \\(\\mu = e^\\tau\\)): \\[ [e^{\\log y - 1.96 / \\sqrt y}, e^{\\log y + 1.96 / \\sqrt y}] \\] based on the LRT: \\[ \\left\\{ 2(-\\mu + y \\log\\mu) \\geq 2(-y +y\\log y) - 3.84 \\right\\} \\] Here are some results for \\(y=10\\) and \\(y=50\\) comparing the three kinds of confidence intervals. (a) (b) (c) \\(y=10\\) [3.8, 16.2] [5.4, 18.6] [5.0, 17.5] \\(y=50\\) [36.1, 63.9] [37.9, 66.0] [37.4, 65.2] If interested, check out Poisson regression (log-linear models)↩︎ "],["the-bootstrap-method.html", "6.6 The bootstrap method", " 6.6 The bootstrap method Bootstrap is a computational method for estimating standard errors and confidence intervals, especially when inference involves a statistic whose distribution is unknown. Suppose \\(X_1,\\dots,X_n\\,\\overset{\\text{iid}}{\\sim}\\,f(x|\\theta)\\), where both \\(f\\) and \\(\\theta\\) are unknown. We are interested to conduct inference about a statistic \\(T=T(X_1,\\dots,X_n)\\). If \\(T({\\boldsymbol X})=\\bar X_n\\), then the CLT applies as \\(n\\to\\infty\\) so we can know approximately its distribution and standard error. What about other statistics? E.g. Skewness \\(\\gamma=\\mathop{\\mathrm{E}}\\left[(X-\\mu)^3 \\right]/\\sigma^3\\) Kurtosis \\(\\kappa=\\mathop{\\mathrm{E}}\\left[(X-\\mu)^4 \\right]/\\sigma^4\\) Figure 6.1: Bootstrap. Main idea A point estimate \\(T({\\boldsymbol X})\\) is obtained using a sample from the population. This is all the data we have. We then draw a bootstrap sample \\(\\{X_1^*,\\dots,X_n^*\\}\\) from the sample and calculate the statistic \\(T({\\boldsymbol X}^*)\\). Repeat this many times to get an idea of the variability of the statistic. 6.6.1 Empirical distribution To see why this works, consider the empirical distribution function of a data set. Definition 6.6 (Empirical distribution) Let \\(X_1,\\dots,X_n\\) be iid with common cdf \\(F(x)\\). The empirical distribution function is defined as \\[ \\hat F_n(x) = \\frac{1}{n} \\sum_{i=1}^n \\mathop{\\mathrm{\\unicode{x1D7D9}}}[X_i\\leq x] \\] The empirical cdf just counts the number of elements in the sample less than a given value–it is literally doing what a cumulative frequency plot would do. Notice that For a fixed \\(x\\), the r.v. \\(\\mathop{\\mathrm{\\unicode{x1D7D9}}}[X_i\\leq x]\\) is Bernoulli with param. \\(p=\\Pr(X_i\\leq x)=F(x)\\). Hence, \\(n\\hat F_n(x)\\sim\\mathop{\\mathrm{Bin}}\\big(n,F(x)\\big)\\), and so we know the mean and variance. Importantly, \\(\\hat F_n(x)\\) is an unbiased estimator of \\(F(x)\\). It is also consistent: \\(\\hat F_n(x) \\xrightarrow{\\text P} F(x)\\) by the law of large numbers. 6.6.2 Bootstrap variance estimation GOAL: To estimate the variance of a statistic \\(T({\\boldsymbol X})\\) \\[ \\mathop{\\mathrm{Var}}_F(T)= \\int \\left\\{T({\\boldsymbol x}) - \\mathop{\\mathrm{E}}(T({\\boldsymbol x}))\\right\\}^2 \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!F({\\boldsymbol x}) \\] The bootstrap method has two steps: Estimate \\(\\mathop{\\mathrm{Var}}_F(T)\\) with \\(\\mathop{\\mathrm{Var}}_{\\hat F_n}(T)\\), i.e. using the empirical distribution. Approximate \\(\\mathop{\\mathrm{Var}}_{\\hat F_n}(T)\\) with \\(\\widehat{\\mathop{\\mathrm{Var}}}_{\\hat F_n}(T)\\) using simulation, i.e. bootstrap resampling. So actually there are two sources of error: \\[ \\mathop{\\mathrm{Var}}_F(T) \\overbrace{\\approx}^{\\text{estimation error}} \\mathop{\\mathrm{Var}}_{\\hat F_n}(T) \\overbrace{\\approx}^{\\text{simulation error}} \\widehat{\\mathop{\\mathrm{Var}}}_{\\hat F_n}(T) \\] As a remark, the estimation in Step 1 is typically consistent due to the LLN. Thus, the size of the error depends on the sample size. 6.6.2.1 Step 1: Bootstrap variance estimation Actually, Step 1 is what we have been doing so far. It simply uses the data to compute the variance of our statistic, assuming that the functional form of \\(F(x)\\) is known. Example 6.10 Suppose \\(T({\\boldsymbol X})=\\bar X_n\\). Then we know that \\(\\mathop{\\mathrm{Var}}_F(T)=\\sigma^2/n\\), where \\(\\sigma^2 = \\int (x-\\mu)^2 \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!F(x)\\) and \\(\\mu = \\int x \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!F(x)\\). Still, this involves and unknown quantity \\(\\sigma^2\\), so we use an estimate instead: \\[\\begin{align*} \\hat\\sigma^2 =\\frac{1}{n-1} \\sum_{i=1}^n (x_i-\\hat \\mu)^2 &amp;= \\frac{n}{n-1} \\cdot \\frac{1}{n} \\sum_{i=1}^n (x_i-\\hat \\mu)^2 \\mathop{\\mathrm{\\unicode{x1D7D9}}}[X_i\\leq x_i] \\\\ &amp;= \\frac{n}{n-1} \\int (x-\\hat\\mu)^2\\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!\\hat F_n(x), \\end{align*}\\] where \\(\\hat\\mu = \\int x \\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!\\hat F_n(x)= \\frac{1}{n} \\sum_{i=1}^n x_i \\mathop{\\mathrm{\\unicode{x1D7D9}}}[X_i\\leq x_i] = \\bar x\\). In the above example, Step 1 is sufficient. But when we cannot write down a simple formula for \\(\\mathop{\\mathrm{Var}}_{\\hat F_n}(T)\\) we need to do bootstrap. 6.6.2.2 Step 2: Bootstrap variance estimation Recap the problem again: From our sample \\(\\{X_1,\\dots,X_n\\}\\) we compute \\(T=T({\\boldsymbol X})\\), and we want to estimate \\(\\mathop{\\mathrm{Var}}_{\\hat F_n}(T)\\) (variance of \\(T\\) using the empirical cdf of \\({\\boldsymbol X}\\), e.g. think \\(\\hat\\sigma^2/n\\)) but we are unable to, for whatever reason. Hypothetically if we had a “random sample” of our test statistic \\(\\{T_1^*,\\dots,T_B^*\\}\\), where each \\(T_k^*\\) is computed from a new sample \\(\\{X_1^*,\\dots,X_n^*\\}\\) obtained from the empirical cdf, then \\[ \\widehat{\\mathop{\\mathrm{Var}}}_{\\hat F_n}(T) = \\frac{1}{B} \\sum_{k=1}^B (T_k^* - \\bar T_B )^2 \\xrightarrow{\\text P} \\mathop{\\mathrm{E}}_{\\hat F_n}((T-\\mathop{\\mathrm{E}}T)^2) = \\mathop{\\mathrm{Var}}_{\\hat F_n}(T) \\] as \\(B\\to\\infty\\), so we have found a consistent estimator for \\(\\mathop{\\mathrm{Var}}_{\\hat F_n}(T)\\). The question is, how do we sample from the empirical cdf? Suppose we observe \\({\\boldsymbol X}= \\{x_1,\\dots,x_n\\}\\). Order these to create \\[ x_{(1)}, x_{(2)}, \\dots, x_{(n)} \\] where \\(x_{(1)}=\\min_i x_i\\) and \\(x_{(n)}=\\max_i x_i\\) and \\(x_{(k)} \\leq x_{(k+1)}\\). By definition of the empirical cdf, \\[ \\hat F_n(x_{(k)}) = \\frac{1}{n} \\sum_{i=1}^n \\mathop{\\mathrm{\\unicode{x1D7D9}}}[X \\leq x_{(k)}] = \\frac{k}{n}. \\] Evidently, the empirical cdf assigns mass \\(1/n\\) on each data point \\(x_i\\). Therefore, to simulate \\(\\{X_1^*,\\dots,X_n^*\\}\\sim \\hat F_n(x)\\), it suffices to draw \\(n\\) observations with replacement from \\(\\{X_1,\\dots,X_n\\}\\). 6.6.2.3 Summary of bootstrap procedure Using the bootstrap procedure below, we may obtain an estimator \\(v_{boot}\\) for \\(\\mathop{\\mathrm{Var}}(T)\\), the variance of a statistic of interest. Draw \\(\\{X_1^*,\\dots,X_n^*\\}\\sim \\hat F_n(x)\\) by sampling with replacement from the set \\(\\{X_1,\\dots,X_n\\}\\). Compute \\(T^*=T(X_1^*,\\dots,X_n^*)\\). Repeat steps 1 and 2 \\(B\\) number of times to obtain \\(\\{T^*_1,\\dots,T^*_B\\}\\). Compute \\[ v_{boot} := \\widehat{\\mathop{\\mathrm{Var}}}_{\\hat F_n}(T) = \\frac{1}{B} \\sum_{k=1}^B (T_k^* - \\bar T_B )^2 \\] where \\(\\bar T_B = B^{-1}\\sum_{k=1}^B T_k^*\\). The above steps are what is used to calculate the variance of the estimator in practice in a variety of problems where the variance of the estimator would be unobtainable otherwise. Depending on the actual function of the statistic \\(T\\), the above bootstrap procedure is quite simple to implement, and does not require too much computational power. Example 6.11 We’ll inspect the daily returns of the Shanghai Stock Exchange Composite Index in December 1994. An inspection of plots below all indicate non-normality (positive skew). The ``tailed-ness’’ of a distribution is measured by the kurtosis \\(\\kappa=\\mathop{\\mathrm{E}}\\left[(X-\\mu)^4 \\right]/\\sigma^4\\) and we may use the plug-in estimator below to estimate \\(\\kappa\\): \\[ \\hat\\kappa = \\frac{1}{nS^4} \\sum_{i=1}^n (X_i-\\bar X)^4 \\] The estimate kurtosis is \\(\\hat\\kappa=7.84\\), indicating daily returns are heavy-tailed. In comparison, the kurtosis of any univariate normal distribution is 3. How accurate is this estimate? Use bootstrap to compute the standard errors. mean((x - mean(x)) ^ 4) / sd(x) ^ 4 # estimate of kurtosis ## [1] 7.840612 n &lt;- length(x) B &lt;- 1000 res &lt;- rep(NA, B) # vector to hold results for (k in 1:B) { xstar &lt;- sample(x = x, size = n, replace = TRUE) res[k] &lt;- mean((xstar - mean(xstar)) ^ 4) / sd(xstar) ^ 4 } head(res) # this is T* ## [1] 10.661504 7.766468 4.223420 7.587617 3.980878 7.679705 sd(res) # bootstrap standard error ## [1] 3.136696 "],["bootstrap-confidence-intervals.html", "6.7 Bootstrap confidence intervals", " 6.7 Bootstrap confidence intervals Now that we’ve seen how to compute the bootstrap standard error, we can build confidence intervals using it There are three kinds of bootstrap cis: Normal bootstrap interval Pivotal bootstrap interval Percentile bootstrap interval Let \\(X_1,\\dots,X_n\\,\\overset{\\text{iid}}{\\sim}\\,f(x|\\theta)\\) whose distribution is unknown, and we are interested in constructing a ci for the parameter \\(\\theta\\). For each of the cis, we need to obtain bootstrap samples \\(\\{\\hat\\theta^*_1,\\dots,\\hat\\theta^*_B\\}\\) of the estimator \\(\\hat\\theta=\\theta(X_1,\\dots,X_n)\\) using the procedure in Definition ??. 6.7.1 Normal bootstrap interval From the bootstrap samples obtain \\[ \\text{se}_{boot}(\\hat\\theta) = \\sqrt{\\frac{1}{B} \\sum_{i=1}^B \\left(\\hat\\theta^*_i - \\frac{1}{B}\\sum_{i=1}^B \\hat\\theta^*_i \\right)^2}. \\] Definition 6.7 (Normal bootstrap interval) Suppose the estimator \\(\\hat\\theta\\) for \\(\\theta\\) is asymptotically normal. The interval \\[ \\left[\\hat\\theta - z(\\alpha/2)\\cdot \\text{se}_{boot}(\\hat\\theta) \\ , \\ \\hat\\theta + z(\\alpha/2)\\cdot \\text{se}_{boot}(\\hat\\theta) \\right], \\] is an approximate 100\\((1-\\alpha)\\)% confidence interval for \\(\\theta\\). The idea is to replace \\(\\text{se}(\\hat\\theta_n)\\) in the Wald interval with the bootstrap se. Note that this interval is not very accurate unless the distribution of \\(\\hat\\theta\\) is close to normal. 6.7.2 Bootstrap percentile interval Arrange the bootstrapped quantities \\(\\hat\\theta_i^*\\) in ascending order to obtain the ordered quantities \\[ \\hat\\theta_{(1)}^*, \\hat\\theta_{(2)}^*, \\dots, \\hat\\theta_{(B)}^*. \\] Let \\(\\hat\\theta^*_{(\\alpha)}\\) be the \\(\\lfloor B\\alpha \\rfloor\\)-th smallest value among the \\(\\hat\\theta_i^*\\). In other words, 100\\(\\alpha\\)% of the ordered \\(\\hat\\theta_{(i)}^*\\) are smaller than \\(\\hat\\theta^*_{(\\alpha)}\\). Definition 6.8 (Bootstrap percentile interval) An approximate 100\\((1-\\alpha)\\)% confidence interval based on the bootstrap percentiles is given by \\[ \\left[\\hat\\theta^*_{(\\alpha/2)} \\ , \\ \\hat\\theta^*_{(1-\\alpha/2)}\\right] \\] The logic here is that the bootstrap method suggests that the true parameter value for \\(\\hat F_n(x)\\) will lie in this interval about 100\\((1-\\alpha)\\)% of the time. Hopefully, the ci for \\(\\theta\\) based on \\(\\hat F_n(x)\\) will converge to the ci for \\(\\theta\\) based on \\(F(x)\\). 6.7.3 Bootstrap pivotal interval Define the pivotal quantity \\(Q=\\hat\\theta - \\theta\\), and denote the cdf of \\(Q\\) by \\(G(r) = \\Pr(\\hat\\theta - \\theta \\leq r)\\). Define further the top \\(\\alpha\\) point of the distribution of this pivot by \\(r(\\alpha)\\) s.t. \\(G\\big(r(\\alpha)\\big)=1-\\alpha\\). The fact that \\[\\begin{align*} 1-\\alpha &amp;= \\Pr\\left(r(1-\\alpha/2) \\leq \\hat\\theta - \\theta \\leq r(\\alpha/2) \\right) \\\\ &amp;= \\Pr\\left(\\hat\\theta - r(\\alpha/2) \\leq \\theta \\leq \\hat\\theta - r(1-\\alpha/2) \\right), \\end{align*}\\] this gives an exact 100\\((1-\\alpha)\\)% confidence interval for \\(\\theta\\) of the form \\[ \\left[ \\hat\\theta - r(\\alpha/2), \\hat\\theta - r(1-\\alpha/2) \\right]. \\] Of course, this is a valid interval if the pivot \\(Q\\) is free of \\(\\theta\\), which unfortunately it is not (since its distribution \\(G\\) depends on \\(\\theta\\)). However, in the bootstrap approach we need not care about this! The argument is that the behaviour of \\(Q=\\hat\\theta - \\theta\\) is not far off from \\(\\hat Q = \\hat\\theta^* - \\hat\\theta\\), in which case we make use of the estimate of \\(G(r)\\) given by \\[ \\hat G(r) = \\frac{1}{B} \\sum_{k=1}^B \\mathop{\\mathrm{\\unicode{x1D7D9}}}[\\hat\\theta^*_k - \\hat\\theta \\leq r], \\] the empirical distribution using the bootstrap samples \\(\\hat\\theta^*_k\\). We replace \\(r(\\alpha/2)\\) and \\(r(1-\\alpha/2)\\) by their bootstrap counterparts \\(r^*(\\alpha/2)\\) and \\(r^*(1-\\alpha/2)\\) s.t. \\(\\hat G\\big(r^*(\\alpha)\\big)=1-\\alpha\\). Then, \\[\\begin{align*} 1-\\alpha &amp;= \\Pr\\left(r^*(1-\\alpha/2) \\leq \\hat\\theta^* - \\hat\\theta \\leq r^*(\\alpha/2) \\right) \\\\ &amp;\\approx \\Pr\\left(r^*(1-\\alpha/2) \\leq \\hat\\theta - \\theta \\leq r^*(\\alpha/2) \\right) \\\\ &amp;= \\Pr\\left(\\hat\\theta - r^*(\\alpha/2) \\leq \\theta \\leq \\hat\\theta - r^*(1-\\alpha/2) \\right), \\end{align*}\\] so we can build a ci based off of this fact. In practice however, it’s easier to use the bootstrap percentiles, since \\[ r^*(\\alpha) = \\hat\\theta^*_{(1-\\alpha)} - \\hat\\theta \\] by definition. It follows that \\[ \\Pr\\left(\\hat\\theta - r^*(\\alpha/2) \\leq \\theta \\leq \\hat\\theta - r^*(1-\\alpha/2) \\right) = \\Pr\\left(2\\hat\\theta - \\hat\\theta^*_{(1-\\alpha/2)} \\leq \\theta \\leq 2\\hat\\theta - \\hat\\theta^*_{(\\alpha/2)} \\right) \\approx 1-\\alpha. \\] Definition 6.9 (Bootstrap pivotal interval) An approximate 100\\((1-\\alpha)\\)% confidence interval based on the bootstrap pivotal quantity is \\[ \\left[ 2\\hat\\theta - \\hat\\theta^*_{(1-\\alpha/2)} \\ , \\ 2\\hat\\theta - \\hat\\theta^*_{(\\alpha/2)} \\right], \\] where \\(\\hat\\theta^*_{(\\alpha)}\\) denotes the 100\\(\\alpha\\)-th perncetile of the ordered bootstrap estimates \\(\\hat\\theta_i^*\\)s. 6.7.4 Which one to use? In general, all three methods give similar performance, provided that the (empirical) distribution of \\(\\hat\\theta\\) is roughly “nice”, i.e. unimodal, symmetric, not skewed, unbiased. the empirical distribution \\(F_n(x)\\) of the data represents the population distribution \\(F(x)\\) well. If it doesn’t, then no bootstrapping method will be reliable25. In all cases, these confidence intervals are , i.e. the coverage probability \\(\\Pr(\\theta \\in C({\\boldsymbol X}))\\) is not exactly \\(1-\\alpha\\). More accurate methods exist but are not discussed here. Example 6.12 This example was used by Bradley Efron, the inventor of the bootstrap. The data are LSAT scores (for entrance to law school) and GPA. \\(i\\) LSAT GPA 1 576 3.39 2 635 3.30 3 558 2.81 4 578 3.03 5 666 3.44 6 580 3.07 7 555 3.00 8 661 3.43 9 651 3.36 10 605 3.13 11 653 3.12 12 575 2.74 13 545 2.76 14 572 2.88 15 594 2.96 Each data point is of the form \\(X_i=(Y_i, Z_i)\\), where \\(Y_i = \\text{LSAT}_i\\) and \\(Z_i = \\text{GPA}_i\\). The law school is interested in the correlation coefficient \\[ \\rho = \\frac{\\iint (y-\\mu_y)(z-\\mu_z)\\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!F(y,z)}{\\sqrt{\\int(y-\\mu_y)^2\\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!F(y) \\int (z-\\mu_z)^2\\mathop{\\mathrm{d}}\\hspace{0.5pt}\\!F(z)}}. \\] The plug-in estimate is the sample correlation \\[ \\hat\\rho = \\frac{\\sum_{i=1}^n (Y_i-\\bar Y)(Z_i-\\bar Z)}{\\sqrt {\\sum_{i=1}^n (Y - \\bar Y)^2\\sum_{i=1}^n (Z_i-\\bar Z)^2}}. \\] The estimated correlation is \\(\\hat\\rho=0.776\\). Note that \\(\\hat\\rho\\in[0,1]\\) and it is not entirely obvious what its distribution might be. Several choices do exist for distributions within the unit interval of course, for instance \\(\\mathop{\\mathrm{Unif}}(0,1)\\) or the Beta distribution–but are these good distributions to impose on our statistic? Let’s use bootstrap to estimate the 95% ci for \\(\\rho\\). (rho &lt;- cor(y, z)) # &#39;law&#39; data frame in R package &#39;bootstrap&#39; ## [1] 0.7763745 B &lt;- 1000 rhostar &lt;- rep(NA, B) for (i in 1:B) { samp &lt;- sample(1:15, size = 15, replace = TRUE) rhostar[i] &lt;- cor(y[samp], z[samp]) } round(head(rhostar), 3) ## [1] 0.684 0.898 0.955 0.675 0.910 0.864 (bootse &lt;- sd(rhostar)) # bootstrap se ## [1] 0.1269466 Now, compute the three kinds of intervals. # normal interval c(rho - qnorm(0.975) * bootse, min(rho + qnorm(0.975) * bootse, 1)) ## [1] 0.5275637 1.0000000 # percentile interval a &lt;- as.numeric(quantile(rhostar, probs = 0.025)) b &lt;- as.numeric(quantile(rhostar, probs = 0.975)) c(a, b) ## [1] 0.4904234 0.9568777 # pivotal interval c(2 * rho - b, min(2 * rho - a, 1)) ## [1] 0.5958713 1.0000000 The three methods are not too far off each other, but with a larger sample size they may show closer agreement. The plot below shows the distribution of \\(\\hat\\rho^*\\) (a bit skewed). https://stats.stackexchange.com/a/357498↩︎ "],["exercises-5.html", "6.8 Exercises", " 6.8 Exercises Let \\(X_1,\\dots,X_n\\) be iid \\(N(\\theta,1)\\). A 95% confidence interval for \\(\\theta\\) is \\(\\bar x\\pm 1.96/\\sqrt n\\). Let \\(p\\) denote the probability that an additional independent observation, \\(X_{n+1}\\), will fall in this interval. Is \\(p\\) greater than, less than, or equal to 0.95? Prove your answer. The length (in millimetres) \\(X_i\\) of cuckoos’ eggs found in hedge sparrow nests can be modelled with the distribution\\[\\Pr(X_i\\leq x\\mid \\alpha,\\beta) = \\begin{cases}0 &amp;x &lt;0 \\\\(x/\\beta)^\\alpha &amp;0\\leq x \\leq \\beta \\\\1&amp;x &gt; \\beta\\end{cases}\\] Find a two-dimensional sufficient statistic for \\((\\alpha,\\beta)\\). The following data for the length of cuckoo’s eggs were collected: 22.0 23.9 20.9 23.8 25.0 24.0 21.7 23.8 22.8 23.1 23.1 23.5 23.0 23.0 Find the MLEs of \\(\\alpha\\) and \\(\\beta\\). Construct 95% confidence interval estimate for \\(\\beta\\) based on this data set, assuming that \\(\\alpha\\) is known and equal to its MLE. Hint: The parameter \\(\\beta\\) is a scale parameter, so each \\(X_i/\\beta\\) is a pivot. Derive a confidence interval for a binomial \\(p\\) by inverting the LRT of \\(H_0:p=p_0\\) versus \\(H_1:p\\neq p_0\\). Let \\(X_1,\\dots,X_n\\) be iid \\(\\mathop{\\mathrm{N}}(\\theta,\\sigma^2)\\) where \\(\\sigma^2\\) is known. For each of the following hypotheses, write out the acceptance region of a level \\(\\alpha\\) test, and the \\(1-\\alpha\\) confidence interval that results from inverting the test. \\(H_0:\\theta=\\theta_0\\) v.s. \\(H_1:\\theta \\neq \\theta_0\\). \\(H_0:\\theta\\geq\\theta_0\\) v.s. \\(H_1:\\theta &lt; \\theta_0\\). (c)\\(H_0:\\theta\\leq\\theta_0\\) v.s. \\(H_1:\\theta &gt; \\theta_0\\). Suppose that \\(X_1,\\dots,X_{30}\\) is a random sample from \\(\\mathop{\\mathrm{N}}(\\mu,\\sigma^2)\\) where both parameters are unknown. If the observed values of \\(\\bar X\\) and \\(S^2\\) are respectively \\(\\bar x = 12.9\\) and \\(s^2=4.6\\), calculate a 99% confidence interval for \\(\\mu\\) and \\(\\sigma\\). We saw previously (Example 6.3) that a 95% confidence interval for the normal variance is rather wide when \\(n=20\\)–the ratio of the upper to lower limits was \\(10.24/2.78=3.7\\). How large a value of \\(n\\) would be needed in order for the interval \\([L({\\boldsymbol X}),U({\\boldsymbol X})]\\) to be short enough that \\((U-L)/L\\leq 0.2\\)? I.e., roughly, short enough that the interval puts bounds \\(\\pm 10\\)% on the point estimate \\(S^2\\). Suppose that \\(Y_1,\\dots,Y_{6}\\) are iid from the gamma distribution whose pdf is \\[ f(y|\\alpha,\\beta)\\propto \\frac{1}{\\beta^\\alpha}y^{\\alpha-1}e^{-y/\\beta} \\hspace{2em}y,\\alpha,\\beta&gt;0, \\] with known shape \\(\\alpha=6\\) and unknown scale parameter \\(\\beta\\). Find a pivot based on \\(Y_1,\\dots,Y_6\\). If the observed value of \\(\\sum_{i=1}^6 Y_i\\) is \\(\\sum_{i=1}^6 y_i=6.6\\), calculate a 99% confidence interval for \\(\\mathop{\\mathrm{E}}(Y_i)\\). Hint: Using the properties of the gamma distribution would really help out here. If \\(X_1,\\dots,X_n\\) are iid exponential random variable with mean \\(\\mu\\), show that \\(Y = \\min(X_1,\\dots,X_n)\\) is also exponentially distributed. Compare the lengths of 95% confidence intervals for \\(\\mu\\) based on the two different pivots \\(\\bar X/\\mu\\) and \\(Y/\\mu\\) when \\(n=3\\) and \\(n=10\\). Suppose that \\(Y_1,\\dots,Y_{12}\\) are monthly counts of insurance claims, assumed independently Poisson distributed with (monthly) mean \\(\\mu\\). Show that the annual count \\(T=Y_1+\\cdots + Y_{12}\\) is sufficient for \\(\\mu\\). If the observed value of \\(T\\) is \\(t=96\\), calculate two approximate 99% confidence intervals for \\(\\mu\\) based on the MLE. the likelihood ratio. Let \\(X_1,\\dots,X_n\\) be distinct observations. Let \\(X_1^*,\\dots,X_n^*\\) denote a bootstrap sample, and let \\(\\bar X_n^*=\\frac{1}{n}\\sum_{i=1}^n X_i^*.\\) Find \\(\\mathop{\\mathrm{E}}(\\bar X_n^* \\mid X_1,\\dots,X_n)\\). \\(\\mathop{\\mathrm{Var}}(\\bar X_n^* \\mid X_1,\\dots,X_n)\\). \\(\\mathop{\\mathrm{Var}}(\\bar X_n^* )\\). Let \\(X_1,\\dots,X_n\\) be a random sample from \\(\\mathop{\\mathrm{Bern}}(p)\\) where \\(p\\in(0,1)\\) is unknown. Let \\(\\theta=p^2\\). Show that the MLE \\(\\hat\\theta\\) for the parameter \\(\\theta\\) is biased. Outline a bootstrap procedure for estimating the bias of \\(\\hat\\theta\\). Hand-in questions Find a \\(1-\\alpha\\) confidence interval for \\(\\theta\\), given \\(X_1,\\dots,X_n\\) iid with pdf \\(f(x|\\theta)=1\\) for \\(x\\in(\\theta-1/2,\\theta+1/2)\\). [3 marks] Let \\(X_1,\\dots,X_n\\) be a sample from \\(\\mathop{\\mathrm{Bern}}(p)\\), and \\(Y_1,\\dots,Y_m\\) a sample from \\(\\mathop{\\mathrm{Bern}}(q)\\). The two samples are independent of each other. [3 marks] Find an approximate 95% confidence interval for \\(p-q\\) when both \\(n\\) and \\(m\\) are large. [2 marks] 100 people are given a standard antibiotic to treat an infection and another 100 are given a new antibiotic. In the first group, 90 people recover; in the second group, 85 people recover. Let \\(p\\) be the probability of recovery with the standard antibiotic and \\(q\\) the probability of recovery with the new antibiotic. Provide an 80% confidence interval for the difference \\(\\theta = p-q\\). [3 marks] Let \\(X_1,\\dots,X_n\\) be a sample from \\(\\mathop{\\mathrm{N}}(\\mu,1)\\). Let \\(\\theta = e^\\mu\\), and we estimate \\(\\theta\\) by \\(\\hat\\theta=e^{\\bar X}\\), where \\(\\bar X = n^{-1}\\sum_{i=1}^n X_i\\). Describe how to obtain a bootstrap estimator \\(v_{boot}=\\widehat{\\mathop{\\mathrm{Var}}}(\\hat{\\theta})\\) for \\(\\mathop{\\mathrm{Var}}(\\hat{\\theta})\\). [3 marks] "],["exam-tips.html", "A Exam tips", " A Exam tips "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
