# Interval estimation

Often times, we would want to give a set of plausible values for the estimate of an unknown parameter value $\theta$, rather than just a single point estimate.
Such sets are called as confidence sets, and that is the focus of this chapter.
While estimating $\theta$ with a set, rather than a point, seems imprecise, what we gain by using interval estimators is some assurances that the interval will contain the true value.

The two methods for finding interval estimators are by making use of pivotal quantities and through the inversion of hypothesis tests.
Once again, the maximum likelihood method, by way of the likelihood ratio test, gives a 'free' way of finding interval estimators.

To establish coverage probabilities of interval estimators, it is pertinent to know what the distribution of the pivotal quantities used are. 
This is not easy to do in all instances, so approximations can be employed here.
Besides the usual asymptotics, we will also learn about the Bootstrap method.


### Learning objectives {-}

::: {.learningobjectives}
By the end of this chapter, you will be able to:

- Understand the concept of coverage probabilities of interval estimators
- Construct interval estimators by way of pivotal quantities and inverting test statistics
- Use the asymptotic test statistics to construct approximate confidence intervals
- Use the bootstrap method to compute standard errors as well as to compute three kinds of bootstrap intervals
:::

### Readings {-}

- Casella and Berger (2002)
    - Chapter 9, sections 9.1, 9.2 (9.2.1 and 9.2.2 only), 9.3 (9.3.1 only)
    - Chapter 10, section 10.4.
- Wasserman (2004)
    - Chapter 6, section 6.3.2
    - All of Chapter 8 (Bootstrap) 
- Topics not covered here: Bayesian intervals, pivots based on cdfs, test-related optimality, Bayesian optimality, loss function optimality, sinterval using core statistic

## Introduction

The task: to report a *set* $C\subset \Theta$ of plausible values for the unknown parameter $\theta$, rather than a single point estimate.
The set $C=C(\bx)$ is

- a set determined by the value of the observed data $\bX = \bx$ (thus, the set is a random variable!); and
- will often be an *interval* in $\bbR$ (if $\theta\in\bbR=:\Theta$)--hence 'interval estimation'.



Sometimes, the set of most plausible values may not be an interval.



```{r bimodal_confint, echo = FALSE, fig.height = 2.2}
x <- seq(-6, 7, length = 1000)
fx1 <- 0.5 * dnorm(x, mean = -2, sd = 1) + 0.5 * dnorm(x, mean = 3, sd = 1)
plot.df <- data.frame(x = x, y = fx1)
Lthetahat <- 0.5 * dnorm(3, mean = 3, sd = 1)

ggplot(plot.df, aes(x, y)) +
  geom_segment(aes(x = 3, xend = -7, y = Lthetahat, yend = Lthetahat),
               linetype = "dashed", col = "grey") +
  geom_line() +
  geom_ribbon(data = subset(plot.df, x < -1.5 & x > -2.5), aes(x, ymax = y), 
              ymin = 0, alpha = 0.4) +
  geom_ribbon(data = subset(plot.df, x < 3.5 & x > 2.5), aes(x, ymax = y), 
              ymin = 0, alpha = 0.4) +  

  labs(y = expression(L(theta)), x = expression(theta), 
       title = "Bimodal likelihood") +
  coord_cartesian(xlim = c(-6, 7)) +
  scale_y_continuous(breaks = Lthetahat, labels = expression(hat(theta))) +
  theme(axis.ticks.x = element_blank(), axis.text.x = element_blank())
```

### Coverage probability

We'll start with some formal definitions.
Let $C(\bX)$ be a region of the parameter space $\Theta$, determined by the sample $\bX$.  

::: {.definition name="Coverage probability"}
For any given value of $\theta$, the coverage probability of $C(\bX)$ is 
$$
\Pr_\theta \left( \theta \in C(\bX) \right) =: c(\theta)
$$
:::

In words: coverage is the proportion of times that the (random) interval $C(\bX)$ contain the parameter value of interest $\theta$.
Of course, we are interested how well the interval covers the **true value** of the parameter.

### Confidence regions

Since we do not know the true value of $\theta$, we can only guarantee a coverage probability equal to the infimum of $c(\theta)$ (called the *confidence coefficient*). 
We call such a set a *confidence region*.

::: {.definition name="Confidence region"}
The set $C(\bX)$ is said to be a confidence region with confidence coefficient $c$ if 
$$
c = \inf_\theta c(\theta).
$$
:::

In applications, $c$ is typically *fixed* at some suitably large value such as 95% or 99%. 
That is, we want to "build" a confidence region that has a high chance of capturing the true value of $\theta$.

Some remarks:

1. The random variable here is the set $C(\bX)$. The confidence coefficient is simply a statement about the repeated sampling properties of such a set.

   > $C(\bX)$ includes the true $\theta$ in at least 100$c$\% of samples.

::: {.myalert}
In a frequentist setting, $\Pr_\theta \left( \theta \in C(\bX) \right)$ does not  refer to ``the probability of $\theta$ being in $C$'' (however, in the Bayesian setting it does). 
Rather, these probability statements refer to $\bX$ and its randomness, and not $\theta$.
:::


2. We have so far more generally described a \underline{set} $C(\bX)$, but if it is a random *interval*, $C(\bX)=[L(\bX), U(\bX)]$ say, then $C(\bX)$ is said to be a *confidence interval* with confidence coefficient $c$.

3. Estimating an unknown parameter $\theta$ with a set, rather than a point, seems imprecise. However, we gain some assurance that we capture the true value $\theta$ within the set.

::: {.example}
For a sample $X_1,X_2,X_3,X_4\iid\N(\mu,1)$, an interval estimator of $\mu$ could be
$$
C(\bX) = [\bar X - 1, \bar X + 1].
$$
That is, we assert that $\mu$ is within this interval. 
Realise that the probability that we are exactly correct when we estimate $\mu$ by $\bar X$ is $\Pr(\mu=\bar X) = 0$.
On the other hand, 
\begin{align*}
\Pr\left( \mu \in [\bar X - 1, \bar X + 1] \right)
&= \Pr(-1 \leq \bar X - \mu \leq 1) \\
&= \Pr\left( -2 \leq \sqrt{4}(\bar X - \mu) \leq 2 \right) \approx 0.95
\end{align*}
Thus, we have a 95\% chance of covering the unknown parameter with our interval estimator. 
Sacrificing precision in our estimate results in an increased confidence of a true assertion.
:::

Expanding on the previous example: Let $\mu=0$ be the true value. 
A random sample of size 4 is obtained as follows

```{r ci_sim1}
set.seed(123)
(X <- rnorm(4, mean = 0, sd = 1))
mean(X)
```

A 95% confidence interval based on the sample mean is

```{r ci_sim2}
c(mean(X) - 1, mean(X) + 1)
```

In this case, the true value $\mu=0$ is indeed contained within the interval. 
If we repeated this experiment many times, what proportion of the intervals would contain $\mu=0$?


Here's the `R` code:

```{r ci_sim3, echo = -1}
set.seed(29986)
B <- 10000  # number of replications
res <- data.frame(L = rep(NA, B), U = NA, contain = NA)  # prepare the results data frame

for (i in 1:B) {
  X <- rnorm(4, mean = 0, sd = 1)
  L <- mean(X) - 1
  U <- mean(X) + 1
  contain <- (L <= 0) & (0 <= U)  # is 0 contained?
  res[i, ] <- c(L, U, contain)
}
mean(res$contain)  # coverage rate
```

As expected, we get a ~95% coverage with the interval $[\bar X - 1, \bar X + 1]$.
Graphically, we can see this below. 
Of the first 100 random replications and construction of confidence intervals, here exactly 5 do not contain the true value, whereas 95 confidence intervals contain the true value (95%).

```{r ci_sim4, echo = FALSE}
res$contain <- factor(res$contain, labels = c("No", "Yes"))
res %>%
  mutate(i = row_number()) %>%
  subset(i <= 100) %>%
  ggplot() +
  geom_segment(aes(x = i, xend = i, y = L, yend = U, col = contain)) +
  geom_hline(yintercept = 0, linetype = "dashed", col = "grey50") +
  # scale_x_continuous(breaks = c(1, 25, 50, 75, 100)) +
  labs(col = expression("Contains " * mu), y = expression(mu),
       x = "Replication number", title = "First 100 replications of the confidence interval")
```

### Methods for obtaining confidence regions

We will consider two general approaches:

1. Use of a *pivot*
2. Inversion of a hypothesis test

As we shall see, the second is really just a special case of the first.

In the same spirit, large-sample theory of maximum likelihood and of likelihood ratio tests will be found to deliver approximate confidence regions in situations where it is hard to evaluate coverage probabilities exactly.























## Pivots


::: {.definition name="Pivot"}
Suppose that the distribution of $\bX$ is determined by an unknown parameter $\theta$. A *pivotal quantity*, or just pivot for short, is any function $Q(\bX,\theta)$ whose distribution is the same for all values of $\theta$.
:::

That is, the random variable $Q(\bX,\theta)$ is independent of all parameters $\theta$: The function $Q(\bX,\theta)$ will usually explicitly contain both parameters and statistics, but for any set $\cA$, $\Pr_\theta\left( Q(\bX,\theta) \in \cA \right)$ [cannot]{.ul} depend on $\theta$.
From this, we can construct a confidence set for $\theta$ by
$$
\{\theta \mid Q(\bx,\theta) \in \cA \}
$$


- Let $X_1,\dots,X_n\iid \N(\mu,\sigma^2)$. Here, the three functions
$$
Q_1 = \frac{\bar X - \mu}{\sigma/\sqrt n} \hspace{2em}
Q_2 = \frac{\bar X - \mu}{S/\sqrt n} \hspace{2em}
Q_3 = \frac{(n-1)S^2}{\sigma^2}
$$
   are all pivots. $Q_2$ and $Q_3$ may be used respectively for interval estimation of $\mu$ and $\sigma$.
   
- Let $X_1,\dots,X_n\iid\Exp(\lambda)$. Here, $\lambda$ is a scale parameter, so each $X_i/\lambda$ is a pivot, and so is $$Q(\bX)=\bar X / \lambda.$$

::: {.mycheck}
What are their distributions? 
Check that the distribution of $\bar X / \lambda$ is $\Gamma(n,1/n)$. Hint: First check that $X_i/\lambda \sim \Exp(1)$!
:::

### From pivot to confidence interval

Let $Q(\bX,\theta)$ be a pivot, and $c$ a specified confidence coefficient (such as $c=0.95$).

::: {.proposition name="Pivotal confidence interval"}
Suppose we can find constants $a$ and $b$ such that
$$
\Pr_\theta\left( a \leq Q(\bX,\theta) \leq b \right) = c.
$$
Then, $C(\bX)=\left\{\theta \, \big| \, a \leq Q(\bX,\theta) \leq b  \right\}$ is a 100$c$\% confidence interval for $\theta$
:::

::: {.proof}
This immediately follows from the definition.
:::


::: {.example}
Let $X_1,\dots,X_n\iid \N(\mu,\sigma^2)$. To construct a confidence interval for $\mu$, let's use the pivot
$$
Q = \frac{\bar X - \mu}{S/\sqrt n} \sim t_{n-1}.
$$

So if $a$ and $b$ are such that 
$$
\Pr(Q \leq a) = \Pr(Q \geq b) = (1-c)/2
$$
then $a=-b$  and
$$
\Pr_\mu\left(-b \leq Q \leq b \right) = c \Leftrightarrow \Pr_\mu \left(\bar X - bS/\sqrt n \leq \mu \leq \bar X + bS/\sqrt n \right) = c.
$$

Thus, the interval
$$
C(\bX) = \left[\bar X - b \frac{S}{\sqrt n}, \bar X + b \frac{S}{\sqrt n} \right]
$$
is a 100$c$\% confidence interval for $\mu$.


As an illustration, suppose that $n=20$, $\bar X=8.31$ and $S = 1.97$. Then,
$$
C(\bX) = 8.31 \pm 2.093 \cdot \frac{1.97}{\sqrt{20}}= [7.38, 9.23].
$$
is a 95\% confidence interval for $\mu$.
:::

::: {.mycheck}
Check, from the statistical tables, that  $b=2.093$ for  $c=0.95$ and  $n=20$.
:::


::: {.example #normvar}
Let $X_1,\dots,X_n\iid \N(\mu,\sigma^2)$. To construct a confidence interval for $\sigma^2$, let's use the pivot
$$
Q =  \frac{(n-1)S^2}{\sigma^2} \sim \chi^2_{n-1}
$$

So if $a$ and $b$ are such that 
$$
\Pr(Q \leq a) = \Pr(Q \geq b) = (1-c)/2
$$
then 
$$
\Pr_{\sigma^2}\left(a \leq Q \leq b \right) = c \Leftrightarrow \Pr_{\sigma^2} \left((n-1)S^2/b \leq \sigma^2 \leq (n-1)S^2/a  \right) = c.
$$

Thus, the interval
$$
C(\bX) = \left[\frac{(n-1)S^2}{b}, \frac{(n-1)S^2}{a} \right]
$$
is a 100$c$\% confidence interval for $\sigma^2$.


As an illustration, suppose that $n=20$, $S^2 =4.8$. Then, 
$$
C(\bX) = \left[\frac{19 \times 4.8}{32.85}, \frac{19 \times 4.8}{8.907} \right] = [2.78, 10.24].
$$
is a 95\% confidence interval for $\sigma^2$.

:::

::: {.mycheck}
Check, from the statistical tables, that  $a=8.907$ and $b=32.85$ for  $c=0.95$ and  $n=20$
:::

Notice how wide this interval is! I.e., the point estimate of $\sigma^2$ was $S^2=4.8$, while the 95% confidence interval includes values more than twice that.


Accurate estimation of a variance, in general, requires $n$ to be fairly large.
$n=20$ is clearly not large enough to allow $\sigma^2$ to be determined very accurately.


Consider $n=250$. Then the lower and upper limits of the $\chi^2_{249}$ are `r round(qchisq(0.025, 249), 1)` and  `r round(qchisq(0.975, 249), 1)` respectively.
The confidence interval is then
$$
C(\bX) = \left[\frac{249 \times 4.8}{294.6}, \frac{249 \times 4.8}{207.2} \right] = [4.06, 5.77].
$$


::: {.example}
Let $X_1,\dots,X_n\iid \Exp(\lambda)$. To construct a confidence interval for $\lambda$, we could use $\bar X / \lambda \sim \Gamma(n,1/n)$, but it's much more convenient to use
$$
Q = \frac{2n\bar X}{\lambda} \sim \Gamma(2n/2, 2) \equiv \chi^2_{2n}.
$$
Here, we have used the fact that if $Y\sim\Gamma(\alpha,\beta)$ with $\alpha=k/2$ and $\beta=2$, then $Y\sim\chi^2_k$.

So if $a$ and $b$ are such that $\Pr(Q \leq a) = \Pr(Q \geq b) = (1-c)/2$, then
$$
\Pr_{\lambda}\left(a \leq Q \leq b \right) = c \Leftrightarrow \Pr_\lambda \left(2n\bar X/b \leq \lambda \leq 2n\bar X/a  \right) = c.
$$

Thus, the interval
$$
C(\bX) = \left[\frac{2n\bar X}{b}, \frac{2n\bar X}{a} \right]
$$
is a 100$c$\% confidence interval for $\lambda$.


As an illustration, suppose that $n=20$ and $\bar X =8.3$. Then,
$$
C(\bX) = \left[\frac{2\times 20 \times 8.3}{59.34}, \frac{2\times 20 \times 8.3}{24.43} \right] = [5.59, 13.59].
$$
is a 95\% confidence interval for $\mu$.
:::

::: {.mycheck}
Check, from the statistical tables, that  $a=24.43$ and $b=59.34$ for  $c=0.95$:::

## Inverting a test statistic

Suppose that $W_{\theta_0}$ is a test statistic measuring the evidence against $H_0:\theta = \theta_0$. When $X$ is continuous, we saw that the $p$-value $p_{W_{\theta_0}}(\bX)$ is distributed as $\Unif(0,1)$ under $H_0$. Hence, the $p$-value itself is a pivot since it is free of $\theta$!

::: {.proposition #pvalci name="Confidence interval from pivoting \\(p\\)-values"}
$$
C(\bX) = \left\{\theta_0 \ \Big| \ p_{W_{\theta_0}}(\bX) \geq 1 - c \right\}
$$
is a 100$c$\% confidence region for $\theta$.
:::

::: {.proof}
$$
\Pr_\theta \left(\theta \in C(\bX) \right) = \Pr_\theta \left(p_{W_{\theta_0}}(\bX) \geq 1 - c \right) = \int_{1-c}^1 \dint k = c.
$$
:::


Let $A(\theta)=\{ \bx \mid W_\theta(\bx) < w\} = R^c$ for some constant $w$ be the acceptance region of a hypothesis test, i.e. the set in the sample space such that $H_0$ is "accepted". 
For a confidence region $C(\bX)$ with confidence coefficient $c$, include all those $\theta$ values which, when tested, would result in a $p$-value of at least $1-c$.
That is, we want the set of $\bX$ such that
$$
p_W(\bX) = \Pr(W(\bX)\in R) = 1-\Pr(W(\bX) \in A) \geq 1-c
$$
For example,

- For a 95\% confidence region, include in $C(\bX)$ all those values of $\theta_0$ which are such that the $p$-value of evidence against $H_0$ is at least 0.05.

- Or, in terms of the Neyman-Pearson approach: include in $C(\bX)$ all those values of $\theta$ that would not be rejected by a test of size 0.05.

We'll look at a more concrete example next.

::: {.example #invertnormtest}
Let $X_1,\dots,X_n\iid\N(\mu,\sigma^2)$ with $\sigma^2$ known, and consider testing $H_0:\mu=\mu_0$ versus $H_1:\mu\neq\mu_0$. 
Previously, we saw that for a fixed size $\alpha$, the rejection region is given by
$$
R = \left\{ \bx \ \bigg| \ \left| \frac{\bar x-\mu_0}{\sigma/\sqrt n} \right| \geq z(\alpha/2)  \right\}
$$
where $z(\alpha)$ is the top-$\alpha$ point of the standard normal distribution.
The test does not reject $H_0$ should the observed sample $\bX=\bx$ fall in the region  $\{\bx \mid |\bar x - \mu_0| \leq z(\alpha/2)\sigma/\sqrt n \}$. 
Those values of $\mu$ that would not be rejected fall in the region 
$$
C(\bX) = \left[ \bar x- z(\alpha/2)\frac{\sigma}{\sqrt n} \ , \   \bar x + z(\alpha/2)\frac{\sigma}{\sqrt n} \right],
$$
which makes up a 100$(1-\alpha)$\% confidence interval for $\mu$.

Why? First note that $H_0$ is ``accepted'' for sample points in the acceptance region 
$$
A = \left\{ \bx \ \bigg| \ \bar x- z(\alpha/2)\frac{\sigma}{\sqrt n} \leq \mu_0 \leq  \bar x + z(\alpha/2)\frac{\sigma}{\sqrt n} \right\} = R^c.
$$
Since the test has size $\alpha$,
$$
\Pr\!{}_{\mu_0}(W(\bX) \in R) = \alpha \Leftrightarrow \Pr\!{}_{\mu_0}(W(\bX) \in A) = 1 - \alpha.
$$
But this probability statement is true for every $\mu_0$.
Thus,
$$
\Pr(\mu \in C(\bX)) = \Pr(W(\bX) \in A) = 1 - \alpha =: c.
$$

:::

Some remarks.

There is a correspondence between confidence sets and acceptance regions for a hypothesis test:

- A hypothesis test [fixes the parameter value]{.ul} (under $H_0$, $\mu=\mu_0$ say) and asks *what sample values* are consistent with that fixed value, i.e. the test is accepted if it falls in
$$
A(\mu_0) = \left\{ \bx \ \bigg| \ \mu_0- z(\alpha/2)\frac{\sigma}{\sqrt n} \leq \bar x \leq  \mu_0 + z(\alpha/2)\frac{\sigma}{\sqrt n} \right\}
$$

- A confidence set [fixes the sample value]{.ul} (say we observe $\bX =\bx^*$) and asks *what parameter values* make this sample value most plausible, i.e. the confidence set are the values of $\mu$ which fall within
$$
C(\bx^*) =  \left\{ \mu \ \bigg| \ \bar x- z(\alpha/2)\frac{\sigma}{\sqrt n} \leq \mu \leq  \bar x + z(\alpha/2)\frac{\sigma}{\sqrt n} \right\}
$$

The two are connected by the tautology
$$
\bx \in A(\mu_0) \Leftrightarrow \mu \in C(\bx).
$$



```{r invert_test_taut, echo = FALSE, warning = FALSE, message = FALSE}
# devtools::install_github("solatar/ggbrace")
library(ggbrace)
ggplot() +
  geom_abline(slope = 1, intercept = -0.5) +
  geom_abline(slope = 1, intercept = 1) +
  annotate("text", x = 1.7, y = 0.5, size = 5,
           label = expression(bar(x)*" = "*mu-z[alpha/2]*sigma*"/"*sqrt(n))) +
  annotate("text", x = -0.45, y = 1.2, size = 5,
           label = expression(bar(x)*" = "*mu+z[alpha/2]*sigma*"/"*sqrt(n))) +  
  # acceptance region
  geom_segment(aes(x = -100, xend = -0.5, y = -1, yend = -1), 
               linetype = "dotted") +
  geom_segment(aes(x = -100, xend = -0.5, y = 0.5, yend = 0.5), 
               linetype = "dotted") +
  geom_segment(aes(x = -0.5, xend = -0.5, y = -100, yend = 0.5), 
               linetype = "dotted") +  
  geom_brace(xstart = -2.2, xend = -2, ystart = -1, yend = 0.5, 
             pointing = "side", col = iprior::gg_col_hue(2)[1]) +
  annotate("text", x = -1.8, y = -0.25, label = expression("A("*mu[0]*")"),
           col = iprior::gg_col_hue(2)[1], size = 5) +  
  # acceptance region
  geom_segment(aes(x = -100, xend = 2.3, y = 1.8, yend = 1.8), 
               linetype = "dotted") +
  geom_segment(aes(x = 2.3, xend = 2.3, y = 1.8, yend = -100), 
               linetype = "dotted") +  
  geom_segment(aes(x = 0.8, xend = 0.8, y = 1.8, yend = -100), 
               linetype = "dotted") +    
  geom_brace(xstart = 0.8, xend = 2.3, ystart = -1.4, yend = -1.1, 
             col = iprior::gg_col_hue(2)[2]) +  
  annotate("text", x = 1.55, y = -0.95, label = expression("C("*bar(x)*"*)"),
           col = iprior::gg_col_hue(2)[2], size = 5) +
  coord_cartesian(xlim = c(-2, 3), ylim = c(-1.3, 2)) +
  scale_x_continuous(breaks = -0.5, labels = expression(mu[0])) +
  scale_y_continuous(breaks = 1.8, labels = expression(bar(x)*"*")) +
  labs(x = expression(mu), y = expression(bar(x))) 
```

::: {.example}
In Ex. sheet 5, Q5 we looked at a hypothesis test for the variance of a normal distribution.
Let $X_1,\dots,X_n\iid\N(\mu,\sigma^2)$ where both parameters are unknown. 
The LRT test rejects $H_0:\sigma^2=\sigma_0^2$ for samples in the rejection region
$$
R = \left\{\bx \ \bigg| \ \frac{\sum_{i=1}^n (x_i-\bar x)^2}{n\sigma_0^2} \leq k_1 \ \text{ or } \ \frac{\sum_{i=1}^n (x_i-\bar x)^2}{n\sigma_0^2} \geq k_2\right\}
$$
The acceptance region can be alternatively written as
$$
A = \left\{\bx \ \bigg| \ a \leq \ \frac{(n-1)s^2}{\sigma_0^2} \leq b\right\}
$$
for some constants $a$ and $b$ based on the $\chi^2_{n-1}$ distribution. From this, we can see that we get the same confidence interval for $\sigma^2$ based on a pivotal quantity as in Example \@ref(exm:normvar).
:::



```{r sketch_rej_conf_reg, echo = FALSE}
x <- seq(0, 15, length = 100)
fx <- dchisq(x, df = 5)
plot.df <- rbind(
  data.frame(x = x, y = fx, region = "Rejection"),
  data.frame(x = x, y = fx, region = "Confidence")
)
plot.df$region <- factor(plot.df$region, levels = c("Rejection", "Confidence"))
a <- qchisq(0.025, 5)
b <- qchisq(0.975, 5)

plot.df %>%
  subset(region == "Rejection") %>%
  subset(x <= a) -> rejplot.df1
plot.df %>%
  subset(region == "Rejection") %>%
  subset(x >= b) -> rejplot.df2
plot.df %>%
  subset(region == "Confidence") %>%
  subset(x >= a & x <= b) -> conplot.df

ggplot(plot.df, aes(x, y)) +
  geom_line() +
  geom_ribbon(data = rejplot.df1,  aes(x, ymin = 0, ymax = y),
              fill = iprior::gg_col_hue(2)[1], alpha = 0.5) +
  geom_ribbon(data = rejplot.df2,  aes(x, ymin = 0, ymax = y),
              fill = iprior::gg_col_hue(2)[1], alpha = 0.5) +
  geom_ribbon(data = conplot.df,  aes(x, ymin = 0, ymax = y),
              fill = iprior::gg_col_hue(2)[2], alpha = 0.5) +
  scale_x_continuous(breaks = c(a, b), labels = c("a", "b")) +
  facet_grid(region ~ .) +
  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank()) +
  labs(y = expression(f(x)))
```

### Discrete distributions

When $\bX$ is discrete, the $p$-value is no longer uniform under $H_0$.
The $p$-value in that case is *stochastically greater* than a $\Unif(0,1)$ r.v. in the sense that its cdf $F$ satisfies $F(x)\leq x$, $\forall x$.

::: {.proof}
For a continuous r.v. $T$, we saw that $Y=F_T(T)\sim\Unif(0,1)$. However, if $T$ is discrete^[See here: https://stats.stackexchange.com/q/73778], the inverse $F_T^{-1}$ is not defined, and
$$
F_Y(y) = \Pr(Y \leq y) = \Pr(F_T(T)\leq y) \leq y.
$$
Now for any data $\bx$,
$$
p_W(\bx) = \Pr_{\theta_0}(W(\bX)\geq W(\bx)) = \Pr_{\theta_0}(-W(\bX)\leq -W(\bx))=F_{-W}(-W).
$$
Let $Y=-W$ which is discrete, so by the above, the $p$-values are *stochastically greater* than a $\Unif(0,1)$ r.v..
:::

As a result, if $\bX$ is discrete, the construction of $C(\bX)$ as in Proposition \@ref(prp:pvalci) above results in a *conservative* confidence region.
A conservative confidence region allows for a large range with greater probability that the parameter falls in that range.

::: {.proposition #pvalci2 name="\\(p\\)-value inversion gives a conservative confidence region"}
When $\bX$ is discrete,
$$
C(\bX) = \left\{\theta_0 \ \Big| \ p_{W_{\theta_0}}(\bX) \geq 1 - c \right\}
$$
is a 100$c$\% confidence region for $\theta$, but this confidence region is said to be *conservative*.
:::

::: {.proof}
$$
\Pr_\theta \left(\theta \in C(\bX) \right) = \Pr_\theta \left(p_{W_{\theta_0}}(\bX) \geq 1 - c \right) \geq 1 - (1-c) = c
$$
:::

::: {.example}
Suppose that $X$ is a single binary random variable with
$$
\Pr_\theta(X = 1) = 1 - \Pr_\theta(X = 0) = \theta, \hspace{2em} 0<\theta < 1.
$$
Consider the LR test of $H_0:\theta =\theta_0$. The MLE is $\hat\theta=X$, so the LR statistic is
$$
W_{LR}(X) = \frac{L(\hat\theta|X)}{L(\theta_0|X)} = \frac{\hat\theta^X(1-\hat\theta)^{1-X}}{\theta_0^X(1-\theta_0)^{1-X}}  = \begin{cases}
\frac{1}{\theta_0} &X =1 \\
\frac{1}{1-\theta_0} & X=0.
\end{cases}
$$
Thus, the $p$-value based on the observed data $X=x$ is

$$
p_{W_{LR}}(x) = \Pr_{\theta_0}\left(W_{LR}(X) \geq W_{LR}(x) \right) = \begin{cases}
\theta_0 &|x-\theta_0|>1/2 \\
1&|x-\theta_0|\leq 1/2
\end{cases}
$$

Suppose we set the confidence coefficient to be $c=0.95$. Then, included in $C(\bX)$ are all values of $\theta_0$ such that $p_{W_{LR}}(x)\geq 0.05$.
If $x=1$, this is the interval $[0.05,1)$; and by symmetry if $x=0$ it is $(0,0.95]$.

Coverage of such a confidence interval?
$$
c(\theta) = \Pr_\theta(\theta\in C(\bX)) = \begin{cases}
1- \theta  &\theta < 0.05\\
1 &0.05 \leq \theta \leq 0.95\\
\theta & \theta > 0.95
\end{cases}
$$
so we see $c(\theta)>0.95$ for all $\theta$, so the confidence interval is indeed conservative.
:::

## Desirable confidence sets

We have seen two methods for deriving confidence sets (and there are others), and in fact different methods yield different confidence sets.
Is there a best one?

We desire a confidence set $C(\bX)$ which has

- small size (for a confidence interval $C(\bX)=[L(\bX), U(\bX)]$, this means its length $U(\bX)-L(\bX)$); and

- large coverage probability $\Pr(\theta \in C(\bX))$.

Often hard to construct--clearly, to increase coverage we need only increase its size.

In Example \@ref(exm:invertnormtest), we saw the use of the top and bottom $\alpha/2$ points of the standard normal being used. I.e., the size $\alpha$ was split equally among the two tails of the distribution. Is this necessary?

Suppose $1-\alpha =0.9$. Then any of the following pairs give 90% intervals:

```{r opt_length, echo = FALSE, fig.height = 3 * 3, out.width = "80%"}
x <- seq(-3, 3, length = 1000)
fx <- dnorm(x)

a1 <- qnorm(0.099); b1 <- qnorm(0.1 - 0.099, lower.tail = FALSE)
a2 <- qnorm(0.075); b2 <- qnorm(0.1 - 0.075, lower.tail = FALSE)
a3 <- qnorm(0.05); b3 <- qnorm(0.1 - 0.05, lower.tail = FALSE)

plot.df <- data.frame(x = x, y = fx)

# plot.df$type <- factor(plot.df$type, labels = c(
#   paste0("a = ", round(a1, 2), ", b = ", round(b1, 2)),
#   paste0("a = ", round(a2, 2), ", b = ", round(b2, 2)),
#   paste0("a = ", round(a3, 2), ", b = ", round(b3, 2))
# ))

ggplot(plot.df, aes(x, y)) +
  geom_line() +
  geom_ribbon(data = subset(plot.df, x >= a1 & x <= b1), 
              aes(x, ymin = 0, ymax = y), 
              fill = iprior::gg_col_hue(3)[1], alpha = 0.5) +
  geom_bracket(xmin = a1, xmax = b1, y.position = max(fx) * 1.05,
               label = paste0("b - a = ", iprior::dec_plac(b1 - a1, 2))) +
  scale_x_continuous(breaks = round(c(a1, b1), 2)) +
  labs(x = "z", y = expression(phi(z)),
       title = paste0("a = ", round(a1, 2), ", b = ", round(b1, 2))) +
  coord_cartesian(ylim = c(0, max(fx) * 1.1)) +
  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank()) -> p1

ggplot(plot.df, aes(x, y)) +
  geom_line() +
  geom_ribbon(data = subset(plot.df, x >= a2 & x <= b2), 
              aes(x, ymin = 0, ymax = y), 
              fill = iprior::gg_col_hue(3)[2], alpha = 0.5) +
  geom_bracket(xmin = a2, xmax = b2, y.position = max(fx) * 1.05,
               label = paste0("b - a = ", iprior::dec_plac(b2 - a2, 2))) +
  scale_x_continuous(breaks = round(c(a2, b2), 2)) +
  labs(x = "z", y = expression(phi(z)),
       title = paste0("a = ", round(a2, 2), ", b = ", round(b2, 2))) +
  coord_cartesian(ylim = c(0, max(fx) * 1.1)) +
  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank()) -> p2

ggplot(plot.df, aes(x, y)) +
  geom_line() +
  geom_ribbon(data = subset(plot.df, x >= a3 & x <= b3), 
              aes(x, ymin = 0, ymax = y), 
              fill = iprior::gg_col_hue(3)[3], alpha = 0.5) +
  geom_bracket(xmin = a3, xmax = b3, y.position = max(fx) * 1.05,
               label = paste0("b - a = ", iprior::dec_plac(b3 - a3, 2))) +
  scale_x_continuous(breaks = round(c(a3, b3), 2)) +
  labs(x = "z", y = expression(phi(z)),
       title = paste0("a = ", round(a3, 2), ", b = ", round(b3, 2))) +
  coord_cartesian(ylim = c(0, max(fx) * 1.1)) +
  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank()) -> p3

cowplot::plot_grid(p1, p2, p3, ncol = 1)
```

It turns out the strategy of splitting $\alpha$ **equally** is optimal if the distribution is unimodal (note: it does not have to be symmetric!).

::: {.theorem}
Let $f(x)$ be a unimodal pdf. If the interval $[a,b]$ satisfies

- $\int_a^b f(x) \dint x = c$;
- $f(a)=f(b)>0$; and
- $a \leq x^* \leq b$ where $x^*$ is the mode of $f(x)$,

then $[a,b]$ is the shortest among all intervals that satisfy 1.
:::

The proof of this is omitted. See instead C\& B Thm 9.3.2.


::: {.example}
Suppose $X\sim\Gamma(\alpha,\beta)$. The quantity $Y=X/\beta$ is a pivot for $\beta$, with $Y\sim\Gamma(\alpha,1)$.
We can get a confidence interval by finding $a$ and $b$ to satisfy
$$
\Pr(a \leq Y \leq b) = c.
$$

However, choosing $a$ and $b$ to satisfy $f_Y(a)=f_Y(b)$ is not optimal, because the interval on $\beta$ is of the form
$$
C(X) = \left\{ x \ \bigg| \ \frac{x}{b} \leq \beta \leq \frac{x}{a}  \right\},
$$
so the length of the interval is $(1/a-1/b)x$. That is, it is proportional to $1/a-1/b$ and not $b-a$.

:::



## Intervals based on ML methods

Recall the following two asymptotics:

1. Asymptotic efficiency of the MLE,
$$\sqrt n (\hat\theta_n - \theta) \xrightarrow{\text D} \N(0,\cI_1(\theta)^{-1}).$$

2. Large sample distribution of $W_{LR}$ for testing $H_0:\theta=\theta_0$,
$$
-2\log \left[ \frac{L(\theta_0|\bX)}{L(\hat\theta_n|\bX)} \right]  \xrightarrow{\text D}  \chi^2_1
$$
    (or the one based on Wilk's theorem).

    
These are two 'automatic' pivots based on the large sample distributions. So quite generally, an approximate confidence region can be based off maximum likelihood methods.

Under certain regularity conditions the MLE is asymptotically normal, and we can make use of the fact that
$$
\Pr\left(-z(\alpha/2) \leq \sqrt{\cI(\theta)} (\hat\theta_n - \theta) \leq z(\alpha/2) \right)
$$
to build a 100$(1-\alpha)$% confidence set for $\theta$. But this is hard to invert into an interval for $\theta$. So we simplify things by using the observed Fisher information instead.

::: {.definition}
Let $X_1,\dots,X_n\iid f(x|\theta)$, and $\hat\theta_n$ be the MLE of $\theta$.
The interval
$$
\left[\hat\theta_n - z(\alpha/2)\cdot \text{se}(\hat\theta_n) \ , \  \hat\theta_n + z(\alpha/2)\cdot \text{se}(\hat\theta_n) \right],
$$
with $\text{se}(\hat\theta_n) = 1 / \sqrt{-l''(\hat\theta_n)}$, is an approximate 100$(1-\alpha)$\% confidence interval for $\theta$.
:::

This is otherwise known as the Wald interval.

    

::: {.definition}
Let $X_1,\dots,X_n\iid f(x|\theta)$, and $\hat\theta_n$ be the MLE of $\theta$.
The set
$$
C(\bX) = \left\{\theta \ \bigg| \ -2\log \left[ \frac{L(\theta|\bX)}{L(\hat\theta_n|\bX)} \right]  \leq \chi^2_1(\alpha)  \right\}
$$
is an approximate 100$(1-\alpha)$\% confidence interval.
:::

This is simply an inversion of the rejection region for the large-sample LRT test.
The confidence region include values of $\theta$ such that $2\log W_{LR}(\bX)\sim\chi^2_1$ is small. For example, $\Pr(2\log W_{LR} \leq 3.84) \approx 0.95$, so
$$
C(\bX) = \left\{\theta \ \bigg| \ 2 l(\theta |\bX) \geq  2l(\hat\theta | \bX) - 3.84  \right\}
$$
is an approximate 95% confidence region for $\theta$.

Which to use?

- Undoubtedly, the Wald interval is simpler computationally. In comparison, the LRT interval demands the solution of a non-linear equation in order to find the end points.

- However, the Wald interval is **not** invariant to a change in parameter, e.g. $\tau=g(\theta)$ (this is also a disadvantage of the Wald test), whereas the LRT interval is.

- The LRT interval approach works much better than the Wald interval when the likelihood is asymmetric or multi-modal.

::: {.example
}
Consider a single Poisson count, $Y\sim\Pois(\mu)$. There is no exact pivot in this case, so we'll build some approximate confidence sets.

The log-likelihood gives
\begin{align*}
l(\mu|y) &= \const - \mu + y \log\mu \\
l'(\mu|y) &= -1 +y/\mu \\
l''(\mu|y) &= -y/\mu^2 
\end{align*}

From this, we have $\hat\mu=y$, while $-l''(\hat\mu)=1/y$ (provided that $y>0$--the method has problems if not!).

Consider also the parameter transformation  $\tau = \log \mu$, which is a fairly standard one to use in Poisson models^[If interested, check out Poisson regression (log-linear models)]. Then
\begin{align*}
l(\tau|y) &= \const - e^\tau + y \tau \\
l'(\tau|y) &= - e^\tau  +y \\
l''(\tau|y) &= - e^\tau
\end{align*}
so (not surprising, since the MLE is invariant to continuous transformations)
$$
\hat\tau = \begin{cases}
\log y & y > 0 \\
-\infty &y=0
\end{cases}
$$
and $-l''(\hat\tau)=y$ (notice that the standard error is not invariant).

Approximate 95\% confidence intervals for $\mu$...

(a) based on the MLE $\hat\mu$:
$$
[y-1.96\sqrt y, y+1.96\sqrt y]
$$
 based on the MLE $\hat\tau$ (and converting it back via $\mu = e^\tau$):
$$
[e^{\log y - 1.96 / \sqrt y}, e^{\log y + 1.96 / \sqrt y}]
$$
(c) based on the LRT:
$$
\left\{ 2(-\mu + y \log\mu) \geq 2(-y +y\log y) - 3.84 \right\}
$$

:::



Here are some results for $y=10$ and $y=50$ comparing the three kinds of confidence intervals.

|        | (a)          | (b)          | (c)          |
|--------|--------------|--------------|--------------|
| $y=10$ | [3.8, 16.2]  | [5.4, 18.6]  | [5.0, 17.5]  |
| $y=50$ | [36.1, 63.9] | [37.9, 66.0] | [37.4, 65.2] |

## The bootstrap method

Bootstrap is a computational method for estimating standard errors and confidence intervals, especially when inference involves a statistic whose distribution is unknown. 


Suppose $X_1,\dots,X_n\iid f(x|\theta)$, where both $f$ and $\theta$ are unknown. We are interested to conduct inference about a statistic $T=T(X_1,\dots,X_n)$.

- If $T(\bX)=\bar X_n$, then the CLT applies as $n\to\infty$ so we can know approximately its distribution and standard error.

- What about other statistics? E.g.
    
    - Skewness $\gamma=\E\left[(X-\mu)^3 \right]/\sigma^3$
    - Kurtosis $\kappa=\E\left[(X-\mu)^4 \right]/\sigma^4$ 



```{r bootstrap, echo = FALSE, fig.align = "center", out.width = "90%", fig.cap = "Bootstrap."}
knitr::include_graphics("figure/bootstrap.pdf")
```

::: {.mycheck}
**Main idea**

A point estimate $T(\bX)$ is obtained using a sample from the population. This is all the data we have. We then draw a *bootstrap sample* $\{X_1^*,\dots,X_n^*\}$ from the sample and calculate the statistic $T(\bX^*)$. Repeat this many times to get an idea of the *variability* of the statistic.
:::


### Empirical distribution

To see why this works, consider the *empirical distribution function* of a data set.

::: {.definition name="Empirical distribution"}
Let $X_1,\dots,X_n$ be iid with common cdf $F(x)$. The empirical distribution function is defined as
$$
\hat F_n(x) = \frac{1}{n} \sum_{i=1}^n \ind[X_i\leq x]
$$
:::

The empirical cdf just counts the number of elements in the sample less than a given value--it is literally doing what a cumulative frequency plot would do. Notice that

- For a fixed $x$, the r.v. $\ind[X_i\leq x]$ is Bernoulli with param. $p=\Pr(X_i\leq x)=F(x)$.
- Hence, $n\hat F_n(x)\sim\Bin\big(n,F(x)\big)$, and so we know the mean and variance.
- Importantly, $\hat F_n(x)$ is an *unbiased* estimator of $F(x)$. 
- It is also consistent: $\hat F_n(x) \xrightarrow{\text P} F(x)$ by the law of large numbers.

### Bootstrap variance estimation

GOAL: To estimate the variance of a statistic $T(\bX)$
$$
\Var_F(T)= \int \left\{T(\bx) - \E(T(\bx))\right\}^2 \dint F(\bx)
$$
The bootstrap method has two steps:

1. Estimate $\Var_F(T)$ with $\Var_{\hat F_n}(T)$, i.e. using the empirical distribution.

2. Approximate $\Var_{\hat F_n}(T)$ with  $\widehat{\Var}_{\hat F_n}(T)$ using simulation, i.e. bootstrap resampling.


So actually there are two sources of error:
$$
\Var_F(T) \overbrace{\approx}^{\text{estimation error}} \Var_{\hat F_n}(T) \overbrace{\approx}^{\text{simulation error}} \widehat{\Var}_{\hat F_n}(T)
$$

::: {.mycheck}
As a remark, the estimation in Step 1 is typically consistent due to the LLN. Thus, the size of the error depends on the sample size.
:::

#### Step 1: Bootstrap variance estimation

Actually, Step 1 is what we have been doing so far. It simply uses the data to compute the variance of our statistic, assuming that the functional form of $F(x)$ is known.

::: {.example}
Suppose $T(\bX)=\bar X_n$. Then we know that $\Var_F(T)=\sigma^2/n$, where $\sigma^2 = \int (x-\mu)^2 \dint F(x)$ and $\mu = \int x \dint F(x)$. Still, this involves and unknown quantity $\sigma^2$, so we use an estimate instead:

\begin{align*}
\hat\sigma^2
=\frac{1}{n-1} \sum_{i=1}^n (x_i-\hat \mu)^2 
&= \frac{n}{n-1} \cdot \frac{1}{n}  \sum_{i=1}^n (x_i-\hat \mu)^2 \ind [X_i\leq x_i] \\
&= \frac{n}{n-1}  \int (x-\hat\mu)^2\dint \hat F_n(x),
\end{align*}
where $\hat\mu = \int x \dint \hat F_n(x)= \frac{1}{n}  \sum_{i=1}^n x_i \ind [X_i\leq x_i] = \bar x$.
:::

In the above example, Step 1 is sufficient. But when we cannot write down a simple formula for $\Var_{\hat F_n}(T)$ we need to do bootstrap.

#### Step 2: Bootstrap variance estimation

Recap the problem again: From our sample $\{X_1,\dots,X_n\}$ we compute $T=T(\bX)$, and we want to estimate $\Var_{\hat F_n}(T)$ (variance of $T$ using the empirical cdf of $\bX$, e.g. think $\hat\sigma^2/n$) but we are unable to, for whatever reason.

Hypothetically if we had a "random sample" of our test statistic $\{T_1^*,\dots,T_B^*\}$, where each $T_k^*$ is computed from a new sample $\{X_1^*,\dots,X_n^*\}$ obtained from the empirical cdf, then
$$
\widehat{\Var}_{\hat F_n}(T) = \frac{1}{B} \sum_{k=1}^B (T_k^* - \bar T_B )^2 \xrightarrow{\text P}  \E_{\hat F_n}((T-\E T)^2) = \Var_{\hat F_n}(T)
$$
as $B\to\infty$, so we have found a consistent estimator for $\Var_{\hat F_n}(T)$.

The question is, how do we sample from the empirical cdf?

Suppose we observe $\bX = \{x_1,\dots,x_n\}$. Order these to create
$$
x_{(1)}, x_{(2)}, \dots, x_{(n)}
$$
where $x_{(1)}=\min_i x_i$ and $x_{(n)}=\max_i x_i$ and $x_{(k)} \leq x_{(k+1)}$. 
By definition of the empirical cdf,
$$
\hat F_n(x_{(k)}) = \frac{1}{n} \sum_{i=1}^n \ind [X \leq x_{(k)}] = \frac{k}{n}.
$$
Evidently, the empirical cdf assigns mass $1/n$ on each data point $x_i$. Therefore, to simulate $\{X_1^*,\dots,X_n^*\}\sim \hat F_n(x)$, it suffices to draw $n$ observations *with replacement* from $\{X_1,\dots,X_n\}$.

#### Summary of bootstrap procedure

Using the bootstrap procedure below, we may obtain an estimator $v_{boot}$ for $\Var(T)$, the variance of a statistic of interest.

::: {.definition #bootstrap name="Bootstrap variance estimation"}

- Draw $\{X_1^*,\dots,X_n^*\}\sim \hat F_n(x)$ by sampling with replacement from the set $\{X_1,\dots,X_n\}$.

- Compute $T^*=T(X_1^*,\dots,X_n^*)$.

- Repeat steps 1 and 2 $B$ number of times to obtain $\{T^*_1,\dots,T^*_B\}$.

- Compute
$$
v_{boot} := \widehat{\Var}_{\hat F_n}(T) = \frac{1}{B} \sum_{k=1}^B (T_k^* - \bar T_B )^2
$$
where $\bar T_B = B^{-1}\sum_{k=1}^B T_k^*$.
:::

The above steps are what is used to calculate the variance of the estimator in practice in a variety of problems where the variance of the estimator would be unobtainable otherwise.
Depending on the actual function of the statistic $T$, the above bootstrap procedure is quite simple to implement, and does not require too much computational power.

::: {.example}
We'll inspect the daily returns of the Shanghai Stock Exchange Composite Index in December 1994. An inspection of plots below all indicate non-normality (positive skew).


The ``tailed-ness'' of a distribution is measured by the kurtosis $\kappa=\E\left[(X-\mu)^4 \right]/\sigma^4$ and we may use the plug-in estimator below to estimate $\kappa$:

$$
\hat\kappa = \frac{1}{nS^4} \sum_{i=1}^n (X_i-\bar X)^4 
$$


```{r kurtosis_eg1, echo = FALSE, fig.height = 1.7}
set.seed(19716)
y <- rt(3839, df = 3)
y[y < -15] <- abs(y[y < -15])
x <- rchisq(3839, df = 10) - 10 + y

ggplot() +
  geom_histogram(aes(x = x, y = ..density..), binwidth = 1, fill = "grey90", 
                 col = "grey30") +
  labs(y = "Density", x = "Returns", title = "Histogram of daily returns") +
  geom_density(aes(x = x), col = iprior::gg_col_hue(2)[2]) -> p1

ggplot(data.frame(x = x), aes(sample = x)) +
  stat_qq(shape = 1) + 
  stat_qq_line() +
  labs(x = "Normal quantiles", y = "Quantiles of returns",
       title = "QQ-plot of daily returns") -> p2

cowplot::plot_grid(p1, p2)
```



The estimate kurtosis is $\hat\kappa=7.84$, indicating daily returns are heavy-tailed. In comparison, the kurtosis of any univariate normal distribution is 3. How accurate is this estimate? Use bootstrap to compute the standard errors.


```{r kurtosis_eg2}
mean((x - mean(x)) ^ 4) / sd(x) ^ 4  # estimate of kurtosis
```



```{r kurtosis_eg3}
n <- length(x)
B <- 1000
res <- rep(NA, B)  # vector to hold results
for (k in 1:B) {
  xstar <- sample(x = x, size = n, replace = TRUE)
  res[k] <- mean((xstar - mean(xstar)) ^ 4) / sd(xstar) ^ 4
}
head(res)  # this is T*
sd(res)  # bootstrap standard error
```

:::

## Bootstrap confidence intervals

Now that we've seen how to compute the bootstrap standard error, we can build confidence intervals using it There are three kinds of bootstrap cis:

1. Normal bootstrap interval
2. Pivotal bootstrap interval
3. Percentile bootstrap interval


Let $X_1,\dots,X_n\iid f(x|\theta)$ whose distribution is unknown, and we are interested in constructing a ci for the parameter $\theta$.
For each of the cis, we need to obtain bootstrap samples $\{\hat\theta^*_1,\dots,\hat\theta^*_B\}$ of the estimator $\hat\theta=\theta(X_1,\dots,X_n)$ using the procedure in Definition \@ref(def:bootstrap).

### Normal bootstrap interval

From the bootstrap samples obtain
$$
\text{se}_{boot}(\hat\theta) = \sqrt{\frac{1}{B} \sum_{i=1}^B \left(\hat\theta^*_i - \frac{1}{B}\sum_{i=1}^B \hat\theta^*_i \right)^2}.
$$

::: {.definition name="Normal bootstrap interval"}
Suppose the estimator $\hat\theta$ for $\theta$ is asymptotically normal.
The interval
$$
\left[\hat\theta - z(\alpha/2)\cdot \text{se}_{boot}(\hat\theta) \ , \  \hat\theta + z(\alpha/2)\cdot  \text{se}_{boot}(\hat\theta) \right],
$$
is an approximate 100$(1-\alpha)$\% confidence interval for $\theta$.
:::

The idea is to replace $\text{se}(\hat\theta_n)$ in the Wald interval with the bootstrap se. Note that this interval is not very accurate unless the distribution of $\hat\theta$ is close to normal.

### Bootstrap percentile interval

Arrange the bootstrapped quantities $\hat\theta_i^*$ in ascending order to obtain the ordered quantities
$$
\hat\theta_{(1)}^*, \hat\theta_{(2)}^*, \dots, \hat\theta_{(B)}^*.
$$

Let $\hat\theta^*_{(\alpha)}$ be the $\lfloor B\alpha \rfloor$-th smallest value among the $\hat\theta_i^*$. In other words, 100$\alpha$% of the ordered $\hat\theta_{(i)}^*$ are smaller than $\hat\theta^*_{(\alpha)}$.


::: {.definition name="Bootstrap percentile interval"}
An approximate 100$(1-\alpha)$\% confidence interval based on the bootstrap percentiles is given by
$$
\left[\hat\theta^*_{(\alpha/2)} \ , \ \hat\theta^*_{(1-\alpha/2)}\right]
$$
:::

The logic here is that the bootstrap method suggests that the true parameter value for $\hat F_n(x)$ will lie in this interval about 100$(1-\alpha)$\% of the time.
Hopefully, the ci for $\theta$ based on $\hat F_n(x)$ will converge to the ci for $\theta$ based on $F(x)$.


```{r boot_perc_int, echo = FALSE}

plot.df <- data.frame(x = rchisq(100, 5))
alpha <- 0.2
xx <- quantile(plot.df$x, probs = c(alpha / 2, 1 - alpha / 2))
ggplot(plot.df, aes(x)) + 
  stat_ecdf(geom = "step") +
  geom_segment(aes(x = -10, xend = xx[1], y = alpha / 2, yend = alpha / 2),
               linetype = "dashed", col = "grey50") +
  geom_segment(aes(x = xx[1], xend = xx[1], y = alpha / 2, yend = -10),
               linetype = "dashed", col = "grey50") +  
  geom_segment(aes(x = -10, xend = xx[2], y = 1 - alpha / 2, yend = 1 - alpha / 2),
               linetype = "dashed", col = "grey50") +
  geom_segment(aes(x = xx[2], xend = xx[2], y = 1 - alpha / 2, yend = -10),
               linetype = "dashed", col = "grey50") +    
  scale_x_continuous(breaks = xx, labels = c(expression(hat(theta^"*")[(alpha/2)]),
                                             expression(hat(theta^"*")[(1-alpha/2)]))) +
  scale_y_continuous(breaks = c(0, alpha / 2, 1 - alpha / 2, 1), 
                     labels = c(0, expression(alpha/2),  expression(1-alpha/2), 1)) +
  labs(x = "Bootstrap values", y = "Percentile",
       title = expression("Empirical cdf for the bootstrapped "*theta*" values")) +
  coord_cartesian(xlim = c(min(plot.df$x), max(plot.df$x)), ylim = c(0, 1)) 
  # geom_brace(xstart = min(plot.df$x) - 0.6, xend = min(plot.df$x),
  #            ystart = alpha / 2, yend = 1 - alpha / 2, pointing = "side")
```

### Bootstrap pivotal interval

Define the pivotal quantity $Q=\hat\theta - \theta$, and denote the cdf of $Q$ by $G(r) = \Pr(\hat\theta - \theta \leq r)$. Define further the top $\alpha$ point of the distribution of this pivot by $r(\alpha)$ s.t. $G\big(r(\alpha)\big)=1-\alpha$.
The fact that
\begin{align*}
1-\alpha &= \Pr\left(r(1-\alpha/2) \leq \hat\theta - \theta \leq r(\alpha/2) \right) \\
&= \Pr\left(\hat\theta - r(\alpha/2) \leq \theta \leq \hat\theta - r(1-\alpha/2) \right),
\end{align*}
this gives an exact 100$(1-\alpha)$\% confidence interval for $\theta$ of the form
$$
\left[ \hat\theta - r(\alpha/2), \hat\theta - r(1-\alpha/2) \right].
$$
Of course, this is a valid interval if the pivot $Q$ is free of $\theta$, which unfortunately it is not (since its distribution $G$ depends on $\theta$). However, in the bootstrap approach we need not care about this!


The argument is that the behaviour of $Q=\hat\theta - \theta$ is not far off from $\hat Q = \hat\theta^* - \hat\theta$, in which case we make use of the estimate of $G(r)$  given by
$$
\hat G(r) = \frac{1}{B} \sum_{k=1}^B \ind[\hat\theta^*_k - \hat\theta \leq r],
$$
the empirical distribution using the bootstrap samples $\hat\theta^*_k$. We replace $r(\alpha/2)$ and $r(1-\alpha/2)$ by their bootstrap counterparts $r^*(\alpha/2)$ and $r^*(1-\alpha/2)$ s.t. $\hat G\big(r^*(\alpha)\big)=1-\alpha$. Then,
\begin{align*}
1-\alpha 
&= \Pr\left(r^*(1-\alpha/2) \leq \hat\theta^* - \hat\theta \leq r^*(\alpha/2) \right) \\
&\approx \Pr\left(r^*(1-\alpha/2) \leq \hat\theta - \theta \leq r^*(\alpha/2) \right) \\
&= \Pr\left(\hat\theta - r^*(\alpha/2) \leq \theta \leq \hat\theta - r^*(1-\alpha/2) \right),
\end{align*}
so we can build a ci based off of this fact.

In practice however, it's easier to use the bootstrap percentiles, since
$$
r^*(\alpha) = \hat\theta^*_{(1-\alpha)} - \hat\theta
$$
by definition. It follows that
$$
\Pr\left(\hat\theta - r^*(\alpha/2) \leq \theta \leq \hat\theta - r^*(1-\alpha/2) \right) = \Pr\left(2\hat\theta - \hat\theta^*_{(1-\alpha/2)} \leq \theta \leq 2\hat\theta - \hat\theta^*_{(\alpha/2)} \right) \approx 1-\alpha.
$$

::: {.definition name="Bootstrap pivotal interval"}
An approximate 100$(1-\alpha)$\% confidence interval based on the bootstrap pivotal quantity is
$$
\left[ 2\hat\theta - \hat\theta^*_{(1-\alpha/2)} \ , \ 2\hat\theta - \hat\theta^*_{(\alpha/2)} \right],
$$
where $\hat\theta^*_{(\alpha)}$ denotes the 100$\alpha$-th perncetile of the ordered bootstrap estimates $\hat\theta_i^*$s.
:::

### Which one to use?

In general, all three methods give similar performance, provided that

- the (empirical) distribution of $\hat\theta$ is roughly "nice", i.e. unimodal, symmetric, not skewed, unbiased.

- the empirical distribution $F_n(x)$ of the data represents the population distribution $F(x)$ well. If it doesn't, then no bootstrapping method will be reliable^[https://stats.stackexchange.com/a/357498].

In all cases, these confidence intervals are \underline{approximate}, i.e. the coverage probability $\Pr(\theta \in C(\bX))$ is not exactly $1-\alpha$. More accurate methods exist but are not discussed here.


::: {.example}
This example was used by Bradley Efron, the inventor of the bootstrap. The data are LSAT scores (for entrance to law school) and GPA.

```{r bootstrapEfronData, echo = FALSE}
y <- c(576, 635, 558, 578, 666, 580, 555, 661, 651, 605, 653, 575, 545, 572, 594)
z <- c(3.39, 3.30, 2.81, 3.03, 3.44, 3.07, 3.00, 3.43, 3.36, 3.13, 3.12, 2.74, 2.76, 2.88, 2.96)
tibble(
  `$i$` = 1:15,
  LSAT = y,
  GPA = z
) %>% knitr::kable(align = "c")
```


Each data point is of the form $X_i=(Y_i, Z_i)$, where $Y_i = \text{LSAT}_i$  and $Z_i = \text{GPA}_i$.

The law school is interested in the correlation coefficient
$$
\rho = \frac{\iint (y-\mu_y)(z-\mu_z)\dint F(y,z)}{\sqrt{\int(y-\mu_y)^2\dint F(y) \int (z-\mu_z)^2\dint F(z)}}.
$$
The plug-in estimate is the sample correlation
$$
\hat\rho = \frac{\sum_{i=1}^n (Y_i-\bar Y)(Z_i-\bar Z)}{\sqrt {\sum_{i=1}^n (Y - \bar Y)^2\sum_{i=1}^n (Z_i-\bar Z)^2}}.
$$
The estimated correlation is `r paste0("$\\hat\\rho=", round(cor(y, z), 3), "$")`. Note that $\hat\rho\in[0,1]$ and it is not entirely obvious what its distribution might be. 
Several choices do exist for distributions within the unit interval of course, for instance $\Unif(0,1)$ or the Beta distribution--but are these good distributions to impose on our statistic? 
Let's use bootstrap to estimate the 95\% ci for $\rho$.


```{r bootstrap_ci_eg_Efron}
(rho <- cor(y, z))  # 'law' data frame in R package 'bootstrap'
B <- 1000
rhostar <- rep(NA, B)
for (i in 1:B) {
  samp <- sample(1:15, size = 15, replace = TRUE)
  rhostar[i] <- cor(y[samp], z[samp])
}
round(head(rhostar), 3)
(bootse <- sd(rhostar))  # bootstrap se
```

Now, compute the three kinds of intervals.

```{r bootstrap_ci_eg_Efron2}
# normal interval
c(rho - qnorm(0.975) * bootse, min(rho + qnorm(0.975) * bootse, 1))
# percentile interval
a <- as.numeric(quantile(rhostar, probs = 0.025))
b <- as.numeric(quantile(rhostar, probs = 0.975))
c(a, b)
# pivotal interval
c(2 * rho - b, min(2 * rho - a, 1))
```


The three methods are not too far off each other, but with a larger sample size they may show closer agreement. The plot below shows the distribution of $\hat\rho^*$ (a bit skewed).

```{r bootstrap_ci_eg_Efron3, echo = FALSE, fig.height = 3.2}
ggplot() +
  geom_histogram(aes(x = rhostar, y = ..density..), binwidth = 0.05, 
                 fill = "grey90", col = "grey30", breaks = seq(0, 1, by = 0.05)) +
  labs(y = "Density", x = "Bootstrap samples") +
  geom_density(aes(x = rhostar), col = iprior::gg_col_hue(2)[1])
```

:::

## Exercises

1. Let $X_1,\dots,X_n$ be iid $N(\theta,1)$. A 95\% confidence interval for $\theta$ is $\bar x\pm 1.96/\sqrt n$. Let $p$ denote the probability that an additional independent observation, $X_{n+1}$, will fall in this interval. Is $p$ greater than, less than, or equal to 0.95? Prove your answer. \textit{Hint: Consider the distribution of $X_{n-1}-\bar X$.}

2. The length (in millimetres) $X_i$ of cuckoos' eggs found in hedge sparrow nests can be modelled with the distribution$$\Pr(X_i\leq x\mid \alpha,\beta) = \begin{cases}0 &x <0 \\(x/\beta)^\alpha &0\leq x \leq \beta \\1&x > \beta\end{cases}$$ 

   (a) Find a two-dimensional sufficient statistic for $(\alpha,\beta)$.
   
   (b) The following data for the length of cuckoo's eggs were collected:
   
   |      |      |      |      |      |      |      |
   |------|------|------|------|------|------|------|
   | 22.0 | 23.9 | 20.9 | 23.8 | 25.0 | 24.0 | 21.7 |
   | 23.8 | 22.8 | 23.1 | 23.1 | 23.5 | 23.0 | 23.0 |
    
      Find the MLEs of $\alpha$ and $\beta$.
    
   (c) Construct 95\% confidence interval estimate for $\beta$ based on this data set, assuming that $\alpha$ is known and equal to its MLE. *Hint: The parameter $\beta$ is a scale parameter, so each $X_i/\beta$ is a pivot.*


3. Derive a confidence interval for a binomial $p$ by inverting the LRT of $H_0:p=p_0$ versus $H_1:p\neq p_0$.

4. Let $X_1,\dots,X_n$ be iid $\N(\theta,\sigma^2)$ where $\sigma^2$ is known. For each of the following hypotheses, write out the acceptance region of a level $\alpha$ test, and the $1-\alpha$ confidence interval that results from inverting the test.

   (a) $H_0:\theta=\theta_0$ v.s. $H_1:\theta \neq \theta_0$.	
   
   (b) $H_0:\theta\geq\theta_0$ v.s. $H_1:\theta < \theta_0$.	
   
   (c)$H_0:\theta\leq\theta_0$ v.s. $H_1:\theta > \theta_0$.	

5. Suppose that $X_1,\dots,X_{30}$ is a random sample from $\N(\mu,\sigma^2)$ where both parameters are unknown. If the observed values of $\bar X$ and $S^2$ are respectively $\bar x = 12.9$ and $s^2=4.6$, calculate a 99\% confidence interval for $\mu$ and $\sigma$.

6. We saw previously (Example \@ref(exm:normvar)) that a 95\% confidence interval for the normal variance is rather wide when $n=20$--the ratio of the upper to lower limits was $10.24/2.78=3.7$. How large a value of $n$ would be needed in order for the interval $[L(\bX),U(\bX)]$ to be short enough that $(U-L)/L\leq 0.2$? I.e., roughly, short enough that the interval puts bounds $\pm 10$\% on the point estimate $S^2$.

7. Suppose that $Y_1,\dots,Y_{6}$ are iid from the gamma distribution whose pdf is
  $$
  f(y|\alpha,\beta)\propto \frac{1}{\beta^\alpha}y^{\alpha-1}e^{-y/\beta} \hspace{2em}y,\alpha,\beta>0,
  $$ 
  with known shape $\alpha=6$ and unknown scale parameter $\beta$. Find a pivot based on $Y_1,\dots,Y_6$. If the observed value of $\sum_{i=1}^6 Y_i$ is $\sum_{i=1}^6 y_i=6.6$, calculate a 99\% confidence interval for $\E(Y_i)$. *Hint: Using the properties of the gamma distribution would really help out here.*

8. (a) If $X_1,\dots,X_n$ are iid exponential random variable with mean $\mu$, show that $Y = \min(X_1,\dots,X_n)$ is also exponentially distributed.

   (b) Compare the lengths of 95\% confidence intervals for $\mu$ based on the two different pivots $\bar X/\mu$ and $Y/\mu$ when $n=3$ and $n=10$.
   
9. Suppose that $Y_1,\dots,Y_{12}$ are monthly counts of insurance claims, assumed independently Poisson distributed with (monthly) mean $\mu$. 

   (a) Show that the annual count $T=Y_1+\cdots + Y_{12}$ is sufficient for $\mu$.
   
   (b) If the observed value of $T$ is $t=96$, calculate two approximate 99\% confidence intervals for $\mu$ based on
   
      i. the MLE.
      ii.  the likelihood ratio.
      
10. Let $X_1,\dots,X_n$ be distinct observations. Let $X_1^*,\dots,X_n^*$ denote a bootstrap sample, and let $\bar X_n^*=\frac{1}{n}\sum_{i=1}^n X_i^*.$ Find 

    (a) $\E(\bar X_n^* \mid X_1,\dots,X_n)$.
    (b) $\Var(\bar X_n^* \mid X_1,\dots,X_n)$.
    (c) $\Var(\bar X_n^* )$. 

11. Let $X_1,\dots,X_n$ be a random sample from $\Bern(p)$ where $p\in(0,1)$ is unknown. Let $\theta=p^2$. 

   (a) Show that the MLE $\hat\theta$ for the parameter $\theta$ is biased.

   (b) Outline a bootstrap procedure for estimating the bias of $\hat\theta$.
	

### Hand-in questions {-}

1. Find a $1-\alpha$ confidence interval for $\theta$, given $X_1,\dots,X_n$ iid with pdf $f(x|\theta)=1$ for $x\in(\theta-1/2,\theta+1/2)$. **[3 marks]**

2. (a) Let $X_1,\dots,X_n$ be a sample from $\Bern(p)$, and $Y_1,\dots,Y_m$ a sample from $\Bern(q)$. The two samples are independent of each other. **[3 marks]**

   (b) Find an approximate 95\% confidence interval for $p-q$ when both $n$ and $m$ are large. **[2 marks]**

   (c) 100 people are given a standard antibiotic to treat an infection and another 100 are given a new antibiotic. In the first group, 90 people recover; in the second group, 85 people recover. Let $p$ be the probability of recovery with the standard antibiotic and $q$ the probability of recovery with the new antibiotic. Provide an 80\% confidence interval for the difference $\theta = p-q$. **[3 marks]**

3. Let $X_1,\dots,X_n$ be a sample from $\N(\mu,1)$. Let $\theta = e^\mu$, and we estimate $\theta$ by $\hat\theta=e^{\bar X}$, where $\bar X = n^{-1}\sum_{i=1}^n X_i$. 

   (a) Describe how to obtain a bootstrap estimator $v_{boot}=\widehat{\Var}(\hat{\theta})$ for $\Var(\hat{\theta})$. **[4 marks]**
   
   (b) What is a 90% normal bootstrap interval for $\theta$? **[2 marks]**


















