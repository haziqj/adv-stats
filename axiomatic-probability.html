<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>1.2 Axiomatic probability | SM-4331 Advanced Statistics</title>
  <meta name="description" content="Course notes for SM-4331 Advanced Statistics (UBD)." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="1.2 Axiomatic probability | SM-4331 Advanced Statistics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Course notes for SM-4331 Advanced Statistics (UBD)." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1.2 Axiomatic probability | SM-4331 Advanced Statistics" />
  
  <meta name="twitter:description" content="Course notes for SM-4331 Advanced Statistics (UBD)." />
  

<meta name="author" content="Dr Haziq Jamil" />


<meta name="date" content="2022-03-09" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="elementary-set-theory.html"/>
<link rel="next" href="conditioning-and-independence.html"/>
<script src="libs/header-attrs-2.12/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.10.0/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.5.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.5.1/plotly-latest.min.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="mystyle.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">SM-4331 Advanced Statistics</a></li>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
TeX: {extensions: ["cancel.js"]}
});
</script>

<li class="divider"></li>
<li><a href="index.html#about">About<span></span></a></li>
<li class="part"><span><b>I Introduction<span></span></b></span></li>
<li><a href="what-is-statistics.html#what-is-statistics">What is statistics?<span></span></a>
<ul>
<li><a href="learning-statistics.html#learning-statistics">Learning statistics<span></span></a></li>
<li><a href="population-sample-and-parametric-models.html#population-sample-and-parametric-models">Population, sample and parametric models<span></span></a>
<ul>
<li><a href="population-sample-and-parametric-models.html#population-vs-sample">Population vs sample<span></span></a></li>
<li><a href="population-sample-and-parametric-models.html#parametric-models">Parametric models<span></span></a></li>
<li><a href="population-sample-and-parametric-models.html#a-sample-a-set-of-data-or-random-variablesa-duality">A sample: a set of data or random variables?–A duality<span></span></a></li>
<li><a href="population-sample-and-parametric-models.html#variability-of-estimates">Variability of estimates<span></span></a></li>
</ul></li>
<li><a href="probability-and-statistics.html#probability-and-statistics">Probability and statistics<span></span></a></li>
</ul></li>
<li class="part"><span><b>II Prepare<span></span></b></span></li>
<li class="chapter" data-level="1" data-path="probability-theory-primer.html"><a href="probability-theory-primer.html"><i class="fa fa-check"></i><b>1</b> Probability theory primer<span></span></a>
<ul>
<li><a href="probability-theory-primer.html#learning-objectives">Learning objectives<span></span></a></li>
<li><a href="probability-theory-primer.html#readings">Readings<span></span></a></li>
<li class="chapter" data-level="1.1" data-path="elementary-set-theory.html"><a href="elementary-set-theory.html"><i class="fa fa-check"></i><b>1.1</b> Elementary set theory<span></span></a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="elementary-set-theory.html"><a href="elementary-set-theory.html#set-operations"><i class="fa fa-check"></i><b>1.1.1</b> Set operations<span></span></a></li>
<li class="chapter" data-level="1.1.2" data-path="elementary-set-theory.html"><a href="elementary-set-theory.html#partitions"><i class="fa fa-check"></i><b>1.1.2</b> Partitions<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="axiomatic-probability.html"><a href="axiomatic-probability.html"><i class="fa fa-check"></i><b>1.2</b> Axiomatic probability<span></span></a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="axiomatic-probability.html"><a href="axiomatic-probability.html#probability-as-a-measure"><i class="fa fa-check"></i><b>1.2.1</b> Probability as a measure<span></span></a></li>
<li class="chapter" data-level="1.2.2" data-path="axiomatic-probability.html"><a href="axiomatic-probability.html#axioms-of-probability"><i class="fa fa-check"></i><b>1.2.2</b> Axioms of probability<span></span></a></li>
<li class="chapter" data-level="1.2.3" data-path="axiomatic-probability.html"><a href="axiomatic-probability.html#derived-probability-results"><i class="fa fa-check"></i><b>1.2.3</b> Derived probability results<span></span></a></li>
<li class="chapter" data-level="1.2.4" data-path="axiomatic-probability.html"><a href="axiomatic-probability.html#why-measure-theory"><i class="fa fa-check"></i><b>1.2.4</b> Why measure theory?<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="conditioning-and-independence.html"><a href="conditioning-and-independence.html"><i class="fa fa-check"></i><b>1.3</b> Conditioning and independence<span></span></a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="conditioning-and-independence.html"><a href="conditioning-and-independence.html#bayes-theorem"><i class="fa fa-check"></i><b>1.3.1</b> Bayes Theorem<span></span></a></li>
<li class="chapter" data-level="1.3.2" data-path="conditioning-and-independence.html"><a href="conditioning-and-independence.html#independence"><i class="fa fa-check"></i><b>1.3.2</b> Independence<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="random-variables.html"><a href="random-variables.html"><i class="fa fa-check"></i><b>1.4</b> Random variables<span></span></a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="random-variables.html"><a href="random-variables.html#distribution-functions"><i class="fa fa-check"></i><b>1.4.1</b> Distribution functions<span></span></a></li>
<li class="chapter" data-level="1.4.2" data-path="random-variables.html"><a href="random-variables.html#identically-distributed-r.v."><i class="fa fa-check"></i><b>1.4.2</b> Identically distributed r.v.<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="probability-functions.html"><a href="probability-functions.html"><i class="fa fa-check"></i><b>1.5</b> Probability functions<span></span></a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="probability-functions.html"><a href="probability-functions.html#probability-mass-function"><i class="fa fa-check"></i><b>1.5.1</b> Probability mass function<span></span></a></li>
<li class="chapter" data-level="1.5.2" data-path="probability-functions.html"><a href="probability-functions.html#probability-density-functions"><i class="fa fa-check"></i><b>1.5.2</b> Probability density functions<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="transformations.html"><a href="transformations.html"><i class="fa fa-check"></i><b>1.6</b> Transformations<span></span></a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="transformations.html"><a href="transformations.html#probability-integral-transform"><i class="fa fa-check"></i><b>1.6.1</b> Probability integral transform<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="multiple-random-variables.html"><a href="multiple-random-variables.html"><i class="fa fa-check"></i><b>1.7</b> Multiple random variables<span></span></a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="multiple-random-variables.html"><a href="multiple-random-variables.html#bivariate-distributions"><i class="fa fa-check"></i><b>1.7.1</b> Bivariate distributions<span></span></a></li>
<li class="chapter" data-level="1.7.2" data-path="multiple-random-variables.html"><a href="multiple-random-variables.html#marginal-distributions"><i class="fa fa-check"></i><b>1.7.2</b> Marginal distributions<span></span></a></li>
<li class="chapter" data-level="1.7.3" data-path="multiple-random-variables.html"><a href="multiple-random-variables.html#conditional-distributions"><i class="fa fa-check"></i><b>1.7.3</b> Conditional distributions<span></span></a></li>
<li class="chapter" data-level="1.7.4" data-path="multiple-random-variables.html"><a href="multiple-random-variables.html#independent-random-variables"><i class="fa fa-check"></i><b>1.7.4</b> Independent random variables<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="expectations.html"><a href="expectations.html"><i class="fa fa-check"></i><b>1.8</b> Expectations<span></span></a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="expectations.html"><a href="expectations.html#expectations-of-functions-of-r.v."><i class="fa fa-check"></i><b>1.8.1</b> Expectations of functions of r.v.<span></span></a></li>
<li class="chapter" data-level="1.8.2" data-path="expectations.html"><a href="expectations.html#properties-of-expectations"><i class="fa fa-check"></i><b>1.8.2</b> Properties of expectations<span></span></a></li>
<li class="chapter" data-level="1.8.3" data-path="expectations.html"><a href="expectations.html#variance"><i class="fa fa-check"></i><b>1.8.3</b> Variance<span></span></a></li>
<li class="chapter" data-level="1.8.4" data-path="expectations.html"><a href="expectations.html#covariance-and-correlation"><i class="fa fa-check"></i><b>1.8.4</b> Covariance and correlation<span></span></a></li>
<li class="chapter" data-level="1.8.5" data-path="expectations.html"><a href="expectations.html#properties-of-variances-and-covariances"><i class="fa fa-check"></i><b>1.8.5</b> Properties of variances and covariances<span></span></a></li>
<li class="chapter" data-level="1.8.6" data-path="expectations.html"><a href="expectations.html#multivariate-means-and-covariances"><i class="fa fa-check"></i><b>1.8.6</b> Multivariate means and covariances<span></span></a></li>
<li class="chapter" data-level="1.8.7" data-path="expectations.html"><a href="expectations.html#conditional-expectations-and-variance"><i class="fa fa-check"></i><b>1.8.7</b> Conditional expectations and variance<span></span></a></li>
<li class="chapter" data-level="1.8.8" data-path="expectations.html"><a href="expectations.html#additional-explainers"><i class="fa fa-check"></i><b>1.8.8</b> Additional explainers<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="moment-generating-functions.html"><a href="moment-generating-functions.html"><i class="fa fa-check"></i><b>1.9</b> Moment generating functions<span></span></a>
<ul>
<li class="chapter" data-level="1.9.1" data-path="moment-generating-functions.html"><a href="moment-generating-functions.html#moment-generating-functions-1"><i class="fa fa-check"></i><b>1.9.1</b> Moment generating functions<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.10" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>1.10</b> Exercises<span></span></a>
<ul>
<li><a href="exercises.html#hand-in-questions">Hand-in questions<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="commonly-used-probability-models.html"><a href="commonly-used-probability-models.html"><i class="fa fa-check"></i><b>2</b> Commonly-used probability models<span></span></a>
<ul>
<li><a href="commonly-used-probability-models.html#learning-objectives-1">Learning objectives<span></span></a></li>
<li><a href="commonly-used-probability-models.html#readings-1">Readings<span></span></a></li>
<li class="chapter" data-level="2.1" data-path="discrete-models.html"><a href="discrete-models.html"><i class="fa fa-check"></i><b>2.1</b> Discrete models<span></span></a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="discrete-models.html"><a href="discrete-models.html#point-mass-distribution"><i class="fa fa-check"></i><b>2.1.1</b> Point mass distribution<span></span></a></li>
<li class="chapter" data-level="2.1.2" data-path="discrete-models.html"><a href="discrete-models.html#uniform-distribution"><i class="fa fa-check"></i><b>2.1.2</b> Uniform distribution<span></span></a></li>
<li class="chapter" data-level="2.1.3" data-path="discrete-models.html"><a href="discrete-models.html#bernoulli-distribution"><i class="fa fa-check"></i><b>2.1.3</b> Bernoulli distribution<span></span></a></li>
<li class="chapter" data-level="2.1.4" data-path="discrete-models.html"><a href="discrete-models.html#binomial-distribution"><i class="fa fa-check"></i><b>2.1.4</b> Binomial distribution<span></span></a></li>
<li class="chapter" data-level="2.1.5" data-path="discrete-models.html"><a href="discrete-models.html#geometric-distribution"><i class="fa fa-check"></i><b>2.1.5</b> Geometric distribution<span></span></a></li>
<li class="chapter" data-level="2.1.6" data-path="discrete-models.html"><a href="discrete-models.html#negative-binomial"><i class="fa fa-check"></i><b>2.1.6</b> Negative binomial<span></span></a></li>
<li class="chapter" data-level="2.1.7" data-path="discrete-models.html"><a href="discrete-models.html#poisson-distribution"><i class="fa fa-check"></i><b>2.1.7</b> Poisson distribution<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="continuous-models.html"><a href="continuous-models.html"><i class="fa fa-check"></i><b>2.2</b> Continuous models<span></span></a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="continuous-models.html"><a href="continuous-models.html#continuous-uniform-distribution"><i class="fa fa-check"></i><b>2.2.1</b> Continuous uniform distribution<span></span></a></li>
<li class="chapter" data-level="2.2.2" data-path="continuous-models.html"><a href="continuous-models.html#exponential-distribution"><i class="fa fa-check"></i><b>2.2.2</b> Exponential distribution<span></span></a></li>
<li class="chapter" data-level="2.2.3" data-path="continuous-models.html"><a href="continuous-models.html#gamma-distribution"><i class="fa fa-check"></i><b>2.2.3</b> Gamma distribution<span></span></a></li>
<li class="chapter" data-level="2.2.4" data-path="continuous-models.html"><a href="continuous-models.html#beta-distribution"><i class="fa fa-check"></i><b>2.2.4</b> Beta distribution<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="normal-distribution.html"><a href="normal-distribution.html"><i class="fa fa-check"></i><b>2.3</b> Normal distribution<span></span></a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="normal-distribution.html"><a href="normal-distribution.html#location-and-scale-parameter"><i class="fa fa-check"></i><b>2.3.1</b> Location and scale parameter<span></span></a></li>
<li class="chapter" data-level="2.3.2" data-path="normal-distribution.html"><a href="normal-distribution.html#linear-transformations-of-normal-random-variables"><i class="fa fa-check"></i><b>2.3.2</b> Linear transformations of normal random variables<span></span></a></li>
<li class="chapter" data-level="2.3.3" data-path="normal-distribution.html"><a href="normal-distribution.html#the-normal-cdf"><i class="fa fa-check"></i><b>2.3.3</b> The normal cdf<span></span></a></li>
<li class="chapter" data-level="2.3.4" data-path="normal-distribution.html"><a href="normal-distribution.html#rule"><i class="fa fa-check"></i><b>2.3.4</b> 68–95–99.7 Rule<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="some-relationships.html"><a href="some-relationships.html"><i class="fa fa-check"></i><b>2.4</b> Some relationships<span></span></a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="some-relationships.html"><a href="some-relationships.html#poisson-binomial-relationship"><i class="fa fa-check"></i><b>2.4.1</b> Poisson-Binomial relationship<span></span></a></li>
<li class="chapter" data-level="2.4.2" data-path="some-relationships.html"><a href="some-relationships.html#poisson-exponential"><i class="fa fa-check"></i><b>2.4.2</b> Poisson-Exponential<span></span></a></li>
<li class="chapter" data-level="2.4.3" data-path="some-relationships.html"><a href="some-relationships.html#poisson-gamma"><i class="fa fa-check"></i><b>2.4.3</b> Poisson-Gamma<span></span></a></li>
<li class="chapter" data-level="2.4.4" data-path="some-relationships.html"><a href="some-relationships.html#normal-approximations"><i class="fa fa-check"></i><b>2.4.4</b> Normal approximations<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="exercises-1.html"><a href="exercises-1.html"><i class="fa fa-check"></i><b>2.5</b> Exercises<span></span></a>
<ul>
<li><a href="exercises-1.html#hand-in-questions-1">Hand-in questions<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="inequalities-convergences-and-normal-random-samples.html"><a href="inequalities-convergences-and-normal-random-samples.html"><i class="fa fa-check"></i><b>3</b> Inequalities, convergences, and normal random samples<span></span></a>
<ul>
<li><a href="inequalities-convergences-and-normal-random-samples.html#learning-objectives-2">Learning objectives<span></span></a></li>
<li><a href="inequalities-convergences-and-normal-random-samples.html#readings-2">Readings<span></span></a></li>
<li class="chapter" data-level="3.1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>3.1</b> Introduction<span></span></a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="introduction.html"><a href="introduction.html#independent-and-identical-random-variable"><i class="fa fa-check"></i><b>3.1.1</b> Independent and identical random variable<span></span></a></li>
<li class="chapter" data-level="3.1.2" data-path="introduction.html"><a href="introduction.html#statistic"><i class="fa fa-check"></i><b>3.1.2</b> Statistic<span></span></a></li>
<li class="chapter" data-level="3.1.3" data-path="introduction.html"><a href="introduction.html#sampling-distribution"><i class="fa fa-check"></i><b>3.1.3</b> Sampling distribution<span></span></a></li>
<li class="chapter" data-level="3.1.4" data-path="introduction.html"><a href="introduction.html#large-sample-approximation"><i class="fa fa-check"></i><b>3.1.4</b> Large-sample approximation<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="inequalities.html"><a href="inequalities.html"><i class="fa fa-check"></i><b>3.2</b> Inequalities<span></span></a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="inequalities.html"><a href="inequalities.html#markovs-inequality"><i class="fa fa-check"></i><b>3.2.1</b> Markov’s inequality<span></span></a></li>
<li class="chapter" data-level="3.2.2" data-path="inequalities.html"><a href="inequalities.html#chebyshevs-inequality"><i class="fa fa-check"></i><b>3.2.2</b> Chebyshev’s inequality<span></span></a></li>
<li class="chapter" data-level="3.2.3" data-path="inequalities.html"><a href="inequalities.html#cauchy-schwartz-inequality"><i class="fa fa-check"></i><b>3.2.3</b> Cauchy-Schwartz inequality<span></span></a></li>
<li class="chapter" data-level="3.2.4" data-path="inequalities.html"><a href="inequalities.html#jensens-inequality"><i class="fa fa-check"></i><b>3.2.4</b> Jensen’s inequality<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="convergence-of-random-variables.html"><a href="convergence-of-random-variables.html"><i class="fa fa-check"></i><b>3.3</b> Convergence of random variables<span></span></a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="convergence-of-random-variables.html"><a href="convergence-of-random-variables.html#convergence-in-probability"><i class="fa fa-check"></i><b>3.3.1</b> Convergence in probability<span></span></a></li>
<li class="chapter" data-level="3.3.2" data-path="convergence-of-random-variables.html"><a href="convergence-of-random-variables.html#convergence-in-distribution"><i class="fa fa-check"></i><b>3.3.2</b> Convergence in distribution<span></span></a></li>
<li class="chapter" data-level="3.3.3" data-path="convergence-of-random-variables.html"><a href="convergence-of-random-variables.html#mean-square-convergence"><i class="fa fa-check"></i><b>3.3.3</b> Mean-square convergence<span></span></a></li>
<li class="chapter" data-level="3.3.4" data-path="convergence-of-random-variables.html"><a href="convergence-of-random-variables.html#relationship-between-convergences"><i class="fa fa-check"></i><b>3.3.4</b> Relationship between convergences<span></span></a></li>
<li class="chapter" data-level="3.3.5" data-path="convergence-of-random-variables.html"><a href="convergence-of-random-variables.html#slutzkys-theorem"><i class="fa fa-check"></i><b>3.3.5</b> Slutzky’s Theorem<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="limit-theorems.html"><a href="limit-theorems.html"><i class="fa fa-check"></i><b>3.4</b> Limit theorems<span></span></a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="limit-theorems.html"><a href="limit-theorems.html#the-weak-law-of-large-numbers"><i class="fa fa-check"></i><b>3.4.1</b> The (weak) Law of Large Numbers<span></span></a></li>
<li class="chapter" data-level="3.4.2" data-path="limit-theorems.html"><a href="limit-theorems.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>3.4.2</b> The Central Limit Theorem<span></span></a></li>
<li class="chapter" data-level="3.4.3" data-path="limit-theorems.html"><a href="limit-theorems.html#gauging-the-error-of-sample-mean-estimator"><i class="fa fa-check"></i><b>3.4.3</b> Gauging the error of sample mean estimator<span></span></a></li>
<li class="chapter" data-level="3.4.4" data-path="limit-theorems.html"><a href="limit-theorems.html#clt-with-sigma2-unknown"><i class="fa fa-check"></i><b>3.4.4</b> CLT with <span class="math inline">\(\sigma^2\)</span> unknown<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="delta-method.html"><a href="delta-method.html"><i class="fa fa-check"></i><b>3.5</b> Delta method<span></span></a></li>
<li class="chapter" data-level="3.6" data-path="normal-random-samples.html"><a href="normal-random-samples.html"><i class="fa fa-check"></i><b>3.6</b> Normal random samples<span></span></a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="normal-random-samples.html"><a href="normal-random-samples.html#chi2-distribution"><i class="fa fa-check"></i><b>3.6.1</b> <span class="math inline">\(\chi^2\)</span>-distribution<span></span></a></li>
<li class="chapter" data-level="3.6.2" data-path="normal-random-samples.html"><a href="normal-random-samples.html#students-t-distribution"><i class="fa fa-check"></i><b>3.6.2</b> Student’s <span class="math inline">\(t\)</span>-distribution<span></span></a></li>
<li class="chapter" data-level="3.6.3" data-path="normal-random-samples.html"><a href="normal-random-samples.html#proof-of-theorem-refthmpropertynormalsamp"><i class="fa fa-check"></i><b>3.6.3</b> Proof of Theorem @ref(thm:propertynormalsamp)<span></span></a></li>
<li class="chapter" data-level="3.6.4" data-path="normal-random-samples.html"><a href="normal-random-samples.html#f-distribution"><i class="fa fa-check"></i><b>3.6.4</b> <span class="math inline">\(F\)</span>-distribution<span></span></a></li>
<li class="chapter" data-level="3.6.5" data-path="normal-random-samples.html"><a href="normal-random-samples.html#the-analysis-of-variance"><i class="fa fa-check"></i><b>3.6.5</b> The analysis of variance<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="exercises-2.html"><a href="exercises-2.html"><i class="fa fa-check"></i><b>3.7</b> Exercises<span></span></a>
<ul>
<li><a href="exercises-2.html#hand-in-questions-2">Hand-in questions<span></span></a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Inference<span></span></b></span></li>
<li class="chapter" data-level="4" data-path="point-estimation.html"><a href="point-estimation.html"><i class="fa fa-check"></i><b>4</b> Point estimation<span></span></a>
<ul>
<li><a href="point-estimation.html#learning-objectives-3">Learning objectives<span></span></a></li>
<li><a href="point-estimation.html#readings-3">Readings<span></span></a></li>
<li class="chapter" data-level="4.1" data-path="the-likelihood.html"><a href="the-likelihood.html"><i class="fa fa-check"></i><b>4.1</b> The likelihood<span></span></a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="the-likelihood.html"><a href="the-likelihood.html#calculating-the-likelihood"><i class="fa fa-check"></i><b>4.1.1</b> Calculating the likelihood<span></span></a></li>
<li class="chapter" data-level="4.1.2" data-path="the-likelihood.html"><a href="the-likelihood.html#likelihood-ratio"><i class="fa fa-check"></i><b>4.1.2</b> Likelihood ratio<span></span></a></li>
<li class="chapter" data-level="4.1.3" data-path="the-likelihood.html"><a href="the-likelihood.html#log-likelihood"><i class="fa fa-check"></i><b>4.1.3</b> Log likelihood<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="sufficiency.html"><a href="sufficiency.html"><i class="fa fa-check"></i><b>4.2</b> Sufficiency<span></span></a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="sufficiency.html"><a href="sufficiency.html#the-factorisation-theorem"><i class="fa fa-check"></i><b>4.2.1</b> The factorisation theorem<span></span></a></li>
<li class="chapter" data-level="4.2.2" data-path="sufficiency.html"><a href="sufficiency.html#minimal-sufficient-statistic"><i class="fa fa-check"></i><b>4.2.2</b> Minimal sufficient statistic<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="point-estimators.html"><a href="point-estimators.html"><i class="fa fa-check"></i><b>4.3</b> Point estimators<span></span></a></li>
<li class="chapter" data-level="4.4" data-path="method-of-moments.html"><a href="method-of-moments.html"><i class="fa fa-check"></i><b>4.4</b> Method of moments<span></span></a></li>
<li class="chapter" data-level="4.5" data-path="method-of-maximum-likelihood.html"><a href="method-of-maximum-likelihood.html"><i class="fa fa-check"></i><b>4.5</b> Method of maximum likelihood<span></span></a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="method-of-maximum-likelihood.html"><a href="method-of-maximum-likelihood.html#finding-the-mle"><i class="fa fa-check"></i><b>4.5.1</b> Finding the MLE<span></span></a></li>
<li class="chapter" data-level="4.5.2" data-path="method-of-maximum-likelihood.html"><a href="method-of-maximum-likelihood.html#invariance-of-mle"><i class="fa fa-check"></i><b>4.5.2</b> Invariance of MLE<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="evaluating-estimators.html"><a href="evaluating-estimators.html"><i class="fa fa-check"></i><b>4.6</b> Evaluating estimators<span></span></a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="evaluating-estimators.html"><a href="evaluating-estimators.html#bias"><i class="fa fa-check"></i><b>4.6.1</b> Bias<span></span></a></li>
<li class="chapter" data-level="4.6.2" data-path="evaluating-estimators.html"><a href="evaluating-estimators.html#variance-and-standard-error"><i class="fa fa-check"></i><b>4.6.2</b> Variance and standard error<span></span></a></li>
<li class="chapter" data-level="4.6.3" data-path="evaluating-estimators.html"><a href="evaluating-estimators.html#mean-squared-error"><i class="fa fa-check"></i><b>4.6.3</b> Mean squared error<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="cramér-rao-lower-bound-crlb.html"><a href="cramér-rao-lower-bound-crlb.html"><i class="fa fa-check"></i><b>4.7</b> Cramér-Rao lower bound (CRLB)<span></span></a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="cramér-rao-lower-bound-crlb.html"><a href="cramér-rao-lower-bound-crlb.html#fisher-information"><i class="fa fa-check"></i><b>4.7.1</b> Fisher information<span></span></a></li>
<li class="chapter" data-level="4.7.2" data-path="cramér-rao-lower-bound-crlb.html"><a href="cramér-rao-lower-bound-crlb.html#variance-reduction-rao-blackwellisation"><i class="fa fa-check"></i><b>4.7.2</b> Variance reduction: Rao-Blackwellisation<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="large-sample-properties-of-estimators.html"><a href="large-sample-properties-of-estimators.html"><i class="fa fa-check"></i><b>4.8</b> Large sample properties of estimators<span></span></a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="large-sample-properties-of-estimators.html"><a href="large-sample-properties-of-estimators.html#consistency"><i class="fa fa-check"></i><b>4.8.1</b> Consistency<span></span></a></li>
<li class="chapter" data-level="4.8.2" data-path="large-sample-properties-of-estimators.html"><a href="large-sample-properties-of-estimators.html#consistency-vs-unbiasedness"><i class="fa fa-check"></i><b>4.8.2</b> Consistency vs unbiasedness<span></span></a></li>
<li class="chapter" data-level="4.8.3" data-path="large-sample-properties-of-estimators.html"><a href="large-sample-properties-of-estimators.html#consistency-of-mles"><i class="fa fa-check"></i><b>4.8.3</b> Consistency of MLEs<span></span></a></li>
<li class="chapter" data-level="4.8.4" data-path="large-sample-properties-of-estimators.html"><a href="large-sample-properties-of-estimators.html#efficiency"><i class="fa fa-check"></i><b>4.8.4</b> Efficiency<span></span></a></li>
<li class="chapter" data-level="4.8.5" data-path="large-sample-properties-of-estimators.html"><a href="large-sample-properties-of-estimators.html#asymptotic-normality-and-consistency"><i class="fa fa-check"></i><b>4.8.5</b> Asymptotic normality and consistency<span></span></a></li>
<li class="chapter" data-level="4.8.6" data-path="large-sample-properties-of-estimators.html"><a href="large-sample-properties-of-estimators.html#efficiency-of-mle"><i class="fa fa-check"></i><b>4.8.6</b> Efficiency of MLE<span></span></a></li>
<li class="chapter" data-level="4.8.7" data-path="large-sample-properties-of-estimators.html"><a href="large-sample-properties-of-estimators.html#efficiency-of-transformations-of-mle"><i class="fa fa-check"></i><b>4.8.7</b> Efficiency of transformations of MLE<span></span></a></li>
<li class="chapter" data-level="4.8.8" data-path="large-sample-properties-of-estimators.html"><a href="large-sample-properties-of-estimators.html#application-of-asymptotic-normality"><i class="fa fa-check"></i><b>4.8.8</b> Application of asymptotic normality<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="exercises-3.html"><a href="exercises-3.html"><i class="fa fa-check"></i><b>4.9</b> Exercises<span></span></a>
<ul>
<li><a href="exercises-3.html#hand-in-questions-3">Hand-in questions<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html"><i class="fa fa-check"></i><b>5</b> Hypothesis testing<span></span></a>
<ul>
<li><a href="hypothesis-testing.html#learning-objectives-4">Learning objectives<span></span></a></li>
<li><a href="hypothesis-testing.html#readings-4">Readings<span></span></a></li>
<li class="chapter" data-level="5.1" data-path="introduction-1.html"><a href="introduction-1.html"><i class="fa fa-check"></i><b>5.1</b> Introduction<span></span></a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="introduction-1.html"><a href="introduction-1.html#a-general-paradigm"><i class="fa fa-check"></i><b>5.1.1</b> A general paradigm<span></span></a></li>
<li class="chapter" data-level="5.1.2" data-path="introduction-1.html"><a href="introduction-1.html#p-values"><i class="fa fa-check"></i><b>5.1.2</b> <span class="math inline">\(p\)</span>-values<span></span></a></li>
<li class="chapter" data-level="5.1.3" data-path="introduction-1.html"><a href="introduction-1.html#accept-h_0"><i class="fa fa-check"></i><b>5.1.3</b> Accept <span class="math inline">\(H_0\)</span>?<span></span></a></li>
<li class="chapter" data-level="5.1.4" data-path="introduction-1.html"><a href="introduction-1.html#uniformity-of-p-values"><i class="fa fa-check"></i><b>5.1.4</b> Uniformity of <span class="math inline">\(p\)</span>-values<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="likelihood-ratio-test.html"><a href="likelihood-ratio-test.html"><i class="fa fa-check"></i><b>5.2</b> Likelihood ratio test<span></span></a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="likelihood-ratio-test.html"><a href="likelihood-ratio-test.html#log-likelihood-ratio-test-statistic"><i class="fa fa-check"></i><b>5.2.1</b> Log likelihood ratio test statistic<span></span></a></li>
<li class="chapter" data-level="5.2.2" data-path="likelihood-ratio-test.html"><a href="likelihood-ratio-test.html#example-normal-with-known-variance"><i class="fa fa-check"></i><b>5.2.2</b> Example: Normal with known variance<span></span></a></li>
<li class="chapter" data-level="5.2.3" data-path="likelihood-ratio-test.html"><a href="likelihood-ratio-test.html#example-normal-with-unknown-variance-t-test"><i class="fa fa-check"></i><b>5.2.3</b> Example: Normal with unknown variance (<span class="math inline">\(t\)</span>-test)<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="the-neyman-pearson-approach.html"><a href="the-neyman-pearson-approach.html"><i class="fa fa-check"></i><b>5.3</b> The Neyman-Pearson approach<span></span></a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="the-neyman-pearson-approach.html"><a href="the-neyman-pearson-approach.html#performance-of-a-test"><i class="fa fa-check"></i><b>5.3.1</b> Performance of a test<span></span></a></li>
<li class="chapter" data-level="5.3.2" data-path="the-neyman-pearson-approach.html"><a href="the-neyman-pearson-approach.html#relation-to-p-values"><i class="fa fa-check"></i><b>5.3.2</b> Relation to <span class="math inline">\(p\)</span>-values<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="type-i-and-ii-errors.html"><a href="type-i-and-ii-errors.html"><i class="fa fa-check"></i><b>5.4</b> Type I and II errors<span></span></a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="type-i-and-ii-errors.html"><a href="type-i-and-ii-errors.html#minimising-errors"><i class="fa fa-check"></i><b>5.4.1</b> Minimising errors<span></span></a></li>
<li class="chapter" data-level="5.4.2" data-path="type-i-and-ii-errors.html"><a href="type-i-and-ii-errors.html#optimality-of-the-lr-test"><i class="fa fa-check"></i><b>5.4.2</b> Optimality of the LR test<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="one-sided-tests.html"><a href="one-sided-tests.html"><i class="fa fa-check"></i><b>5.5</b> One-sided tests<span></span></a></li>
<li class="chapter" data-level="5.6" data-path="approximate-tests.html"><a href="approximate-tests.html"><i class="fa fa-check"></i><b>5.6</b> Approximate tests<span></span></a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="approximate-tests.html"><a href="approximate-tests.html#asymptotic-distribution-of-lrts"><i class="fa fa-check"></i><b>5.6.1</b> Asymptotic distribution of LRTs<span></span></a></li>
<li class="chapter" data-level="5.6.2" data-path="approximate-tests.html"><a href="approximate-tests.html#wilks-theorem"><i class="fa fa-check"></i><b>5.6.2</b> Wilk’s theorem<span></span></a></li>
<li class="chapter" data-level="5.6.3" data-path="approximate-tests.html"><a href="approximate-tests.html#the-wald-test"><i class="fa fa-check"></i><b>5.6.3</b> The Wald test<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="exercises-4.html"><a href="exercises-4.html"><i class="fa fa-check"></i><b>5.7</b> Exercises<span></span></a>
<ul>
<li><a href="exercises-4.html#hand-in-questions-4">Hand-in questions<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="interval-estimation.html"><a href="interval-estimation.html"><i class="fa fa-check"></i><b>6</b> Interval estimation<span></span></a>
<ul>
<li><a href="interval-estimation.html#learning-objectives-5">Learning objectives<span></span></a></li>
<li><a href="interval-estimation.html#readings-5">Readings<span></span></a></li>
<li class="chapter" data-level="6.1" data-path="introduction-2.html"><a href="introduction-2.html"><i class="fa fa-check"></i><b>6.1</b> Introduction<span></span></a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="introduction-2.html"><a href="introduction-2.html#coverage-probability"><i class="fa fa-check"></i><b>6.1.1</b> Coverage probability<span></span></a></li>
<li class="chapter" data-level="6.1.2" data-path="introduction-2.html"><a href="introduction-2.html#confidence-regions"><i class="fa fa-check"></i><b>6.1.2</b> Confidence regions<span></span></a></li>
<li class="chapter" data-level="6.1.3" data-path="introduction-2.html"><a href="introduction-2.html#methods-for-obtaining-confidence-regions"><i class="fa fa-check"></i><b>6.1.3</b> Methods for obtaining confidence regions<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="pivots.html"><a href="pivots.html"><i class="fa fa-check"></i><b>6.2</b> Pivots<span></span></a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="pivots.html"><a href="pivots.html#from-pivot-to-confidence-interval"><i class="fa fa-check"></i><b>6.2.1</b> From pivot to confidence interval<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="inverting-a-test-statistic.html"><a href="inverting-a-test-statistic.html"><i class="fa fa-check"></i><b>6.3</b> Inverting a test statistic<span></span></a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="inverting-a-test-statistic.html"><a href="inverting-a-test-statistic.html#discrete-distributions"><i class="fa fa-check"></i><b>6.3.1</b> Discrete distributions<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="desirable-confidence-sets.html"><a href="desirable-confidence-sets.html"><i class="fa fa-check"></i><b>6.4</b> Desirable confidence sets<span></span></a></li>
<li class="chapter" data-level="6.5" data-path="intervals-based-on-ml-methods.html"><a href="intervals-based-on-ml-methods.html"><i class="fa fa-check"></i><b>6.5</b> Intervals based on ML methods<span></span></a></li>
<li class="chapter" data-level="6.6" data-path="the-bootstrap-method.html"><a href="the-bootstrap-method.html"><i class="fa fa-check"></i><b>6.6</b> The bootstrap method<span></span></a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="the-bootstrap-method.html"><a href="the-bootstrap-method.html#empirical-distribution"><i class="fa fa-check"></i><b>6.6.1</b> Empirical distribution<span></span></a></li>
<li class="chapter" data-level="6.6.2" data-path="the-bootstrap-method.html"><a href="the-bootstrap-method.html#bootstrap-variance-estimation"><i class="fa fa-check"></i><b>6.6.2</b> Bootstrap variance estimation<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="bootstrap-confidence-intervals.html"><a href="bootstrap-confidence-intervals.html"><i class="fa fa-check"></i><b>6.7</b> Bootstrap confidence intervals<span></span></a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="bootstrap-confidence-intervals.html"><a href="bootstrap-confidence-intervals.html#normal-bootstrap-interval"><i class="fa fa-check"></i><b>6.7.1</b> Normal bootstrap interval<span></span></a></li>
<li class="chapter" data-level="6.7.2" data-path="bootstrap-confidence-intervals.html"><a href="bootstrap-confidence-intervals.html#bootstrap-percentile-interval"><i class="fa fa-check"></i><b>6.7.2</b> Bootstrap percentile interval<span></span></a></li>
<li class="chapter" data-level="6.7.3" data-path="bootstrap-confidence-intervals.html"><a href="bootstrap-confidence-intervals.html#bootstrap-pivotal-interval"><i class="fa fa-check"></i><b>6.7.3</b> Bootstrap pivotal interval<span></span></a></li>
<li class="chapter" data-level="6.7.4" data-path="bootstrap-confidence-intervals.html"><a href="bootstrap-confidence-intervals.html#which-one-to-use"><i class="fa fa-check"></i><b>6.7.4</b> Which one to use?<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="exercises-5.html"><a href="exercises-5.html"><i class="fa fa-check"></i><b>6.8</b> Exercises<span></span></a>
<ul>
<li><a href="exercises-5.html#hand-in-questions-5">Hand-in questions<span></span></a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix<span></span></b></span></li>
<li class="chapter" data-level="A" data-path="exam-tips.html"><a href="exam-tips.html"><i class="fa fa-check"></i><b>A</b> Exam tips<span></span></a></li>
<li><a href="references.html#references">References<span></span></a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">SM-4331 Advanced Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="axiomatic-probability" class="section level2 hasAnchor" number="1.2">
<h2><span class="header-section-number">1.2</span> Axiomatic probability<a href="axiomatic-probability.html#axiomatic-probability" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In principle, we can understand and easily grasp the notion of probability as the “frequency of an event occurring”.
But how do we operationalise this concept? That is, by what rules and mechanisms are we allowed to assign probabilities to events?
If we can overcome this task and are able to assign probabilities to (random) events in an experiment, then we can start to analyse them statistically!</p>
<div id="probability-as-a-measure" class="section level3 hasAnchor" number="1.2.1">
<h3><span class="header-section-number">1.2.1</span> Probability as a measure<a href="axiomatic-probability.html#probability-as-a-measure" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let us take a measure-theoretic approach to defining probabilities.
We will dive straight into the rigors of definitions before providing a somewhat apologetic rationale as to why such mathematical difficulties are required for probability theory.</p>
<p>As the name implies, measure theory is the theory about how we measure things (duh!).
Measure itself is a fundamental concept in mathematics, and it would be useful to come up with a mathematical framework for how we deal with everyday concepts like length, mass, area, volume, and so on.
Importantly, such a framework allow us to reliably measure in even higher dimensions or onto more abstract constructs not yet imaginable.</p>
<p>Intuitively, a measure is simply a function whose input is the thing we want to measure (let’s call it a set), and whose output is a non-negative number.
Don’t worry, a formal definition will follow, but for now, call this function <span class="math inline">\(\mu\)</span>.
It would be fair to expect a measure <span class="math inline">\(\mu\)</span> to satisfy</p>
<ul>
<li><span class="math inline">\(A \subseteq B \Rightarrow \mu(A) \leq \mu(B)\)</span></li>
<li><span class="math inline">\(A \subseteq B \Rightarrow \mu(B-A)= \mu(B) - \mu(A)\)</span></li>
<li>If <span class="math inline">\(\{A_1,A_2,\dots\}\)</span> are mutually exclusive sets (disjoint), then <span class="math inline">\(\mu\left(\cup_{i=1}^\infty A_i \right) = \sum_{i=1}^\infty \mu(A_i)\)</span></li>
</ul>
<p>The first property simply says that if <span class="math inline">\(A\)</span> is a subset of <span class="math inline">\(B\)</span>, then the measure of <span class="math inline">\(A\)</span> is at most the measure of <span class="math inline">\(B\)</span>.
The second property follows this up by saying that the measure of the set <span class="math inline">\(B-A\)</span>, that is, the set that is obtained by starting with <span class="math inline">\(B\)</span> and taking away the parts that is contained in <span class="math inline">\(A\)</span>, then the measure of this created set is the difference between the measures of <span class="math inline">\(B\)</span> and <span class="math inline">\(A\)</span>.
Finally, the third property, also known as <em>countable additivity</em>, simply states that the measure of the whole is equal to the sum of the parts.
It turns out that the first and second properties follow from the third (and the fact that a measure cannot be negative)–see Definition <a href="axiomatic-probability.html#def:measure">1.2</a>.</p>
<p>So we have this intuition about what the measure should be, but what about the stuff we want to measure?
For our purposes, we are interested in measuring subsets of <span class="math inline">\(\Omega\)</span>.
We ask, are we able to measure all possible subsets of <span class="math inline">\(\Omega\)</span>?
At a glance, perhaps if <span class="math inline">\(\Omega\)</span> is countable (e.g. <span class="math inline">\(\Omega=\{1,2,3\}\)</span>), it is easy to describe the subsets of <span class="math inline">\(\Omega\)</span> through the power set<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a> <span class="math inline">\({\mathcal P}(\Omega)\)</span>, which is the set of all possible subsets of <span class="math inline">\(\Omega\)</span>, but what about when <span class="math inline">\(\Omega\)</span> is uncountable (e.g. an interval <span class="math inline">\(\Omega=[0,1]\in\mathbb{R}\)</span>).
Given a sample space <span class="math inline">\(\Omega\)</span>, we need to define the largest possible collection of subsets of <span class="math inline">\(\Omega\)</span> that can be observed and on which we can assign valid measure.</p>
<div class="definition">
<p><span id="def:sigmaalgebra" class="definition"><strong>Definition 1.1  (\(\sigma\)-algebra) </strong></span>A collection <span class="math inline">\({\mathcal F}\)</span> of subsets of a set <span class="math inline">\(\Omega\)</span> is called a <strong><span class="math inline">\(\sigma\)</span>-algebra</strong> if it satisfies the following conditions:</p>
<ol style="list-style-type: lower-roman">
<li>If <span class="math inline">\(A \in {\mathcal F}\)</span>, then <span class="math inline">\(A^c \in cF\)</span> <em>[closed under complementation]</em>.</li>
<li>If <span class="math inline">\(A_1,A_2,\cdots \in {\mathcal F}\)</span>, then <span class="math inline">\(\cup_{i=1}^\infty A_i \in {\mathcal F}\)</span> <em>[closed under countable unions]</em>.</li>
<li><span class="math inline">\(\{\} \in {\mathcal F}\)</span> <em>[contains the empty set]</em>.</li>
</ol>
</div>
<p>As a remark, condition iii. can be replaced with <span class="math inline">\(\Omega\in{\mathcal F}\)</span> by virtue of condition i..
The <span class="math inline">\(\sigma\)</span>-algebra is a collection of events or subsets of the sample space <span class="math inline">\(\Omega\)</span>, including <span class="math inline">\(\Omega\)</span> itself and the empty set <span class="math inline">\(\{\}\)</span>, which is closed under countable applications of set operations.
This is because DeMorgan’s Law allows us to write the countable union property in iii. also as <em>countable intersections</em>: If <span class="math inline">\(A_1,A_2,\cdots \in {\mathcal F}\)</span>, then by i. <span class="math inline">\(A_1^c,A_2^c,\cdots \in {\mathcal F}\)</span>, and hence <span class="math inline">\(\cup_{i=1}^\infty A_i\in{\mathcal F}\)</span> and also its complement. By DeMorgan’s Law,
<span class="math display">\[
\left( \cup_{i=1}^\infty A_i^c \right)^c = \cap_{i=1}^\infty A_i.
\]</span></p>
<p>Sets contained in <span class="math inline">\({\mathcal F}\)</span> are called <strong>measurable sets</strong>.</p>
<div class="mynote">
<p>The <span class="math inline">\(\sigma\)</span>-algebra is an important condition for measure to not breakdown, because it helps draw a line as to which subsets of the sample space is measurable, and which is not.
Out of interest, condition iii. in Definition <a href="axiomatic-probability.html#def:sigmaalgebra">1.1</a> is the condition that makes <span class="math inline">\({\mathcal F}\)</span> a <span class="math inline">\(\sigma\)</span>-algebra (the <span class="math inline">\(\sigma\)</span> stands for countable <u><strong>s</strong></u>um).
Without this condition, one ends up with just an <em>algebra</em> of sets, one that is most likely <em>too small</em>, failing to contain sets that we would like assign a measure.</p>
</div>
<p>Let’s take a look at some examples of <span class="math inline">\(\sigma\)</span>-algebras.</p>
<div class="example">
<p><span id="exm:unlabeled-div-5" class="example"><strong>Example 1.5  </strong></span><br></p>
<ol style="list-style-type: decimal">
<li><p>The trivial <span class="math inline">\(\sigma\)</span>-algebra: <span class="math display">\[\big\{ \{\}, \Omega \big\}.\]</span> This corresponds the case of no information.</p></li>
<li><p>The power set of the sample space <span class="math inline">\(\Omega\)</span>: <span class="math display">\[\big\{ A \mid A \subseteq \Omega \big\}.\]</span> This corresponds the case of full information.</p></li>
<li><p>The collection <span class="math inline">\(\big\{ \{\}, A, A^c, \Omega \big\}\)</span> is a <span class="math inline">\(\sigma\)</span>-algebra, for any <span class="math inline">\(A\subseteq \Omega\)</span>.</p></li>
<li><p>Let <span class="math inline">\(\Omega = \{a,b,c,d\}\)</span>. A possible<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a> <span class="math inline">\(\sigma\)</span>-algebra is <span class="math display">\[\big\{\{\}, \{a,b,c,d\}, \{a,b\}, \{c,d\} \big\}.\]</span></p></li>
<li><p>Define <span class="math inline">\(B(s)\)</span> to be a square of side length <span class="math inline">\(s\)</span>. Let <span class="math inline">\(\Omega\)</span> be the collection of points in <span class="math inline">\((0,1)\times(0,1)\subset \mathbb{R}^2\)</span> contained within the a unit square <span class="math inline">\(B(1)\)</span>. Then <span class="math display">\[{\mathcal F}=\{ \text{Collection of points contained in the square } B(s) \text{ with } s \in (0,1) \}.\]</span> It should be clear there are uncountably many such squares that can be fit within the unit square.</p></li>
</ol>
</div>
<p>Just as a remark, most introduction to probability measure will deal with finite or countable sets when introducing <span class="math inline">\(\sigma\)</span>-algebras, giving readers an impression that it’s only possible to define <span class="math inline">\(\sigma\)</span>-algebras on such sets. The fifth example above gives an example of a <span class="math inline">\(\sigma\)</span>-algebra which is uncountable.</p>
<p>The twin <span class="math inline">\((\Omega,{\mathcal F})\)</span> is called a <em>measurable space</em>. This sort of defines the “parts” of our problem which are measurable, as per Definition <a href="axiomatic-probability.html#def:sigmaalgebra">1.1</a>. What’s missing is a measure, i.e. the thing that actually tells us ‘how long a piece of string is’, so to speak<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a>. We now define a measure as follows.</p>
<div class="definition">
<p><span id="def:measure" class="definition"><strong>Definition 1.2  (Measure) </strong></span>A <em>measure</em> <span class="math inline">\(\mu\)</span> is a non-negative real valued function defined on a <span class="math inline">\(\sigma\)</span>-algebra, i.e. <span class="math inline">\(\mu:{\mathcal F}\to\mathbb{R}_{\geq 0}\cup\{\infty\}\)</span>, where <span class="math inline">\(\mathbb{R}_{\geq 0}\)</span> are the non-negative real numbers and <span class="math inline">\({\mathcal F}\)</span> a <span class="math inline">\(\sigma\)</span>-algebra of subsets of <span class="math inline">\(\Omega\)</span>. The measure <span class="math inline">\(\mu\)</span> satisfies the following properties:</p>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(\mu(\{\})=0\)</span>.</li>
<li><span class="math inline">\(\mu\)</span> is countably additive, i.e. if <span class="math inline">\(A_1,A_2,\dots\)</span> are disjoint events, then <span class="math display">\[\mu\left( \cup_{i=1}^\infty A_i \right) = \sum_{i=1}^\infty \mu(A_i).\]</span></li>
</ol>
</div>
<p>If, in addition the measure of the entire sample space is normalised (i.e. <span class="math inline">\(\mu(\Omega)=1\)</span>), then <span class="math inline">\(\mu\)</span> is called a <strong>probability measure</strong>. We will see this in the next section.</p>
<p>The triplet <span class="math inline">\((\Omega,{\mathcal F},\mu)\)</span> is called a <em>measure space</em> (note that without the measure it is called a measurable space).
This space simply tells us the parts needed for well-defined measure to take place on the subsets of <span class="math inline">\(\Omega\)</span>.</p>
<!-- ::: {.remark} -->
<!-- See Defn 1.2.1 in C&B and the following examples, as well as §1.9 in Wasserman. -->
<!-- ::: -->
<!-- ::: {.remark} -->
<!-- There are alternative formulations/approaches to defining probabilities, e.g. Cox's Theorem (logical probabilities). -->
<!-- ::: -->
<div class="example">
<p><span id="exm:unlabeled-div-6" class="example"><strong>Example 1.6  </strong></span><br></p>
<ol style="list-style-type: decimal">
<li><p>The counting measure. Let <span class="math inline">\(\Omega\)</span> be a countable set [You may be creative as you like here to make this less abstract, e.g. the books on your shelf or the members of your family, although the set need not be finite]. Let <span class="math inline">\({\mathcal F}={\mathcal P}(\Omega)\)</span> be the power set of <span class="math inline">\(\Omega\)</span>. For all sets <span class="math inline">\(A\in{\mathcal A}\)</span>, define <span class="math display">\[\mu(A) = \begin{cases} |A| &amp; A \text{ has finitely many elements}\\ \infty &amp;\text{otherwise} \end{cases}\]</span> where the operator <span class="math inline">\(|\cdot|\)</span> represents the <em>cardinality</em> of the set, i.e. the number of elements it contains (its size).</p></li>
<li><p>The Lebesgue measure in one dimension. Let <span class="math inline">\(\Omega=\mathbb{R}\)</span>, and define <span class="math inline">\({\mathcal F}\)</span> to contain all sets of the form</p>
<ul>
<li>[a,b], i.e. closed intervals,</li>
<li>(a,b), i.e. open intervals,</li>
<li>(a,b], i.e. open-closed intervals; and</li>
<li>[a,b), i.e. closed-open intervals.</li>
</ul>
<p>for all real numbers <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. We can deduce that the <span class="math inline">\(\sigma\)</span>-algebra <span class="math inline">\({\mathcal F}\)</span> contains all possible “nice” intervals of the real line, including unbounded intervals and even singletons, which means any continuous partition of the real line can be measured (including a point, which should have measure zero). To see this, using the properties of <span class="math inline">\(\sigma\)</span>-algebras,</p>
<ul>
<li><p>unbounded intervals are in <span class="math inline">\({\mathcal F}\)</span>, since, for instance <span class="math display">\[(x,+\infty)=\cup_{i=1}^\infty(x,x+i).\]</span></p></li>
<li><p>singletons are in <span class="math inline">\({\mathcal F}\)</span>, since <span class="math display">\[\{x\}=\cap_{i=1}^\infty (x-1/i,x+1/i).\]</span></p></li>
</ul>
<p>This set <span class="math inline">\({\mathcal F}\)</span> has a special name, called the Borel <span class="math inline">\(\sigma\)</span>-algebra.</p>
<p>All that’s left is to define the measure. The Lebesgue measure <span class="math inline">\(\mu\)</span> assigns the usual concept of length to any continuous interval on <span class="math inline">\(\mathbb{R}\)</span> (to be precise, the Borel <span class="math inline">\(\sigma\)</span>-algebra on <span class="math inline">\(\mathbb{R}\)</span>): <span class="math display">\[\mu\left(A\right)=b-a\]</span>
where <span class="math inline">\(A\)</span> is any interval of <span class="math inline">\(\mathbb{R}\)</span> of the above forms (closed, open, open-closed, closed-open). This measure works even for singleton sets or unbounded intervals.</p></li>
</ol>
</div>
</div>
<div id="axioms-of-probability" class="section level3 hasAnchor" number="1.2.2">
<h3><span class="header-section-number">1.2.2</span> Axioms of probability<a href="axiomatic-probability.html#axioms-of-probability" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the previous section, we defined a measure space as the triplet <span class="math inline">\((\Omega,{\mathcal F},\mu)\)</span>.
This formulation lets us work on the set of interest <span class="math inline">\(\Omega\)</span>, and defines the possible measurable subsets <span class="math inline">\({\mathcal F}\subseteq \Omega\)</span>, as well as the measuring device <span class="math inline">\(\mu\)</span>.
This framework generalises the intutive notions of length, area, and volume to higher dimensions and more abstract notions.</p>
<p>In probability theory, we are interested in making use of measure theory to assign probabilities to events. So again in the context of conducting an “experiment”,</p>
<ul>
<li>The sample space <span class="math inline">\(\Omega\)</span> is the set of possible outcomes <span class="math inline">\(\{\omega_1,\omega_2,\dots\}\)</span> of the experiment.</li>
<li>The <span class="math inline">\(\sigma\)</span>-algebra <span class="math inline">\({\mathcal F}\subseteq\Omega\)</span> would define the set of possible outcomes that are measurable, and are able to be assigned probabilities. <span class="math inline">\({\mathcal F}\)</span> is known as the <em>event space</em>.</li>
</ul>
<p>All that’s left is to define a <em>probability measure</em> on the measurable space <span class="math inline">\((\Omega,{\mathcal F})\)</span>.</p>
<div class="definition">
<p><span id="def:unlabeled-div-7" class="definition"><strong>Definition 1.3  (Axioms of Probability) </strong></span>Given a measurable space <span class="math inline">\((\Omega,{\mathcal F})\)</span>, a <em>probability measure</em> (or <em>probability function</em>) is a function <span class="math inline">\(\mathbb{P}:{\mathcal F}\to[0,1]\)</span> that satisfies the following three conditions:</p>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(\mathbb{P}(E) \geq 0, \forall E \in {\mathcal F}\)</span>.</li>
<li><span class="math inline">\(\mathbb{P}(\Omega) = 1\)</span>.</li>
<li>For pairwise disjoint events <span class="math inline">\(A_1,A_2,\dots\)</span>, <span class="math display">\[\mathbb{P}\bigg( \bigcup_{i=1}^\infty A_i  \bigg) = \sum_{i=1}^\infty A_i.\]</span></li>
</ol>
<p>These three conditions are commonly known as the <em>Axioms of Probability</em>, or <em>Kolmogorov Axioms</em>.</p>
</div>
<p>This is pretty much similar to the definition of the measure <span class="math inline">\(\mu\)</span> for a measure space, except for the unitarity requirement <span class="math inline">\(\mathbb{P}(\Omega)=1\)</span>.
The first and second condition implicitly states that probabilities are always <u>finite</u>, by the results of Theorem <a href="axiomatic-probability.html#thm:derivedprobres1">1.1</a> below.
In contrast, measure theory allows for infinite measure.</p>
<p>The second condition above states that the probability of <em>at least</em> one of the elementary events in the entire space will definitely occur.
One common misunderstanding here is to read the statement as “the probability of all of possible events occurring is 1”, which is a rarer thing in most situations.</p>
<p>As a remark, the above axioms does not tell us anything about what the functional form of <span class="math inline">\(\mathbb{P}\)</span> actually is.
It is pretty abstract, but the good thing is that any such function that satisfies the above three axioms is by definition a probability function.
At this point, there is still no notion of <em>randomness</em> in play.
All we are doing is providing the building blocks to be able to assign a numerical representation of (un)certainty of some particular event happening.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:kolmogorov"></span>
<img src="figure/kolmogorov.jpg" alt="Andrey Nikolaevich Kolmogorov 25 April 1903--20 October 1987. Widely considered to be the father of probability theory." width="50%" />
<p class="caption">
Figure 1.1: Andrey Nikolaevich Kolmogorov 25 April 1903–20 October 1987. Widely considered to be the father of probability theory.
</p>
</div>
<div class="mynote">
<p>As mentioned, any function abstract or concrete satisfying the Probability Axioms is regarded as a probability function.
But what does the probability number represent, and what does it actually mean?
Broadly speaking, there are two main interpretation of probabilities.</p>
<ol style="list-style-type: decimal">
<li><p>The <strong>frequentist</strong> interpretation is one that relies on “long run” frequencies. A probability of heads being 50% in a coin flip is interpreted to mean the following: If we flip the coin many times, then the proportion of heads that is observed will be 50% in the long run.</p></li>
<li><p>The <strong>subjectivist</strong> or Bayesian interpretation is that the probability measures an observer’s strength of belief that the event is true. Put a different way, it is the measure of ignorance on the observers part on what has happened. When a coin is flipped, it has landed either heads or tails, and this much is certain. What is uncertain is my <em>knowledge about the coin</em>, rather than the outcome of the coin itself. Setting a 50% probability for heads occuring implies that I am willing to bet at a 1:1 odds that the coin landed heads.</p></li>
</ol>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-8" class="example"><strong>Example 1.7  (C\&amp;B 1.2.5) </strong></span>Consider the simple experiment of tossing a coin.
Define the sample space to be <span class="math inline">\(\Omega=\{H,T\}\)</span>, as representing the only two possible outcomes <span class="math inline">\(H=\)</span> heads or <span class="math inline">\(T=\)</span> tails.</p>
<p>What is the probability of heads occurring?
The Axioms of Probability does not help us in this regard! (I mean, it does not give us a functional form for the probabilities)</p>
<p>Perhaps a function that assigns equal probability to either event would be a good place to start, so we require
<span class="math display" id="eq:headtail1">\[\begin{equation}
\mathbb{P}(\{H\})=\mathbb{P}(\{T\}). \tag{1.1}  
\end{equation}\]</span>
At this point, we still don’t know their values–the probabilities could be 0.1, 0.2, 0.3, or any other value.
Or could they?</p>
<p>Since <span class="math inline">\(\Omega = \{H\} \cup \{T\}\)</span>, we know that by the Probability Axioms that
<span class="math display" id="eq:headtail2">\[\begin{align}
1 = \mathbb{P}(\Omega) 
&amp;= \mathbb{P}\left( \{H\} \cup \{T\} \right) \nonumber \\
&amp;= \mathbb{P}(\{H\}) + \mathbb{P}(\{T\}) \tag{1.2}
\end{align}\]</span>
so the only possible value that satisfies both <a href="axiomatic-probability.html#eq:headtail1">(1.1)</a> and <a href="axiomatic-probability.html#eq:headtail2">(1.2)</a> is
<span class="math display">\[
\mathbb{P}(\{H\})=\mathbb{P}(\{T\}) = 0.5.
\]</span></p>
</div>
<p>Of course, without the restriction of equal probability in <a href="axiomatic-probability.html#eq:headtail1">(1.1)</a>, then any two numbers satisfying <a href="axiomatic-probability.html#eq:headtail2">(1.2)</a> and the Probability Axioms would be valid, e.g. <span class="math inline">\(\mathbb{P}(\{H\})= 0.8\)</span> and <span class="math inline">\(\mathbb{P}(\{T\}) = 0.2.\)</span></p>
<div class="example">
<p><span id="exm:unlabeled-div-9" class="example"><strong>Example 1.8  </strong></span>Two six-sided dice are thrown and the outcome for both dice are recorded.</p>
<ul>
<li><p>As there are 36 possible outcomes, the sample space is
<span class="math display">\[
\Omega = \big\{ \omega_{ij}=\{i,j\} \mid i,j =1,\dots,6 \big\}
\]</span></p></li>
<li><p>Suppose we are interested in the event <span class="math inline">\(E\)</span> defined to be <em>‘the sum of the two scores is 6’</em>. These would be the events
<span class="math display">\[
E := \big\{ \{1,5\}, \{2,4\}, \{3,3\}, \{4,2\}, \{5,1\} \big\}.
\]</span>
One may easily construct a <span class="math inline">\(\sigma\)</span>-algebra <span class="math inline">\({\mathcal F}\)</span> (for example, the power set of <span class="math inline">\(\Omega\)</span>) and verify that the event <span class="math inline">\(E\)</span> is contained within it. So this is a measurable event.</p></li>
</ul>
<p>So at this point, we might be thinking about a suitable probability function so that we may assign a probability to the event <span class="math inline">\(E\)</span>.
Especially if the two dice are fair, it seems reasonable to assume that any of the outcome in <span class="math inline">\(\omega_{ij}\in\Omega\)</span> is equally likely to occur, so we set <span class="math inline">\(\mathbb{P}(\omega_{ij}) = 1/36\)</span> for any <span class="math inline">\(i,j =1,\dots,6\)</span>.
In particular, the probability of any event should be proportional to the total number of outcomes in <span class="math inline">\(\Omega\)</span>.
As a quick exercise, you may check that such a probability function satisfies all the Kolmogorov Axioms.</p>
<p><span class="math display">\[\begin{align*}
\mathbb{P}(E) &amp;= 
\mathbb{P}\left( \{1,5\} \cup \{2,4\} \cup \{3,3\} \cup \{4,2\} \cup \{5,1\} \right) \\
&amp;= \mathbb{P}( \{1,5\}) + \mathbb{P}(\{2,4\}) + \mathbb{P}(\{3,3\}) \\
&amp; \hspace{1em} + \mathbb{P}(\{4,2\}) + \mathbb{P}(\{5,1\}) \\
&amp;= \frac{5}{36}
\end{align*}\]</span></p>
<p>Alternatively, we could have also easily argued that
<span class="math display">\[
\mathbb{P}(E) = \frac{|E|}{|\Omega|} = \frac{5}{36}.
\]</span></p>
</div>
<p>As a remark, it would be very cumbersome to have to check the Kolmogorov Axioms every time we encounter a probability function.
For problems like the above, we won’t run into any technical issues because the sample space is finite and/or countable<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a>.
In general, most of the problems we will come across will satisfy the axioms automatically, especially with “nice” sample space and events, so we usually don’t check axioms all the time.</p>
<div class="mynote">
<p>At this point, most textbooks go into a section about <em>counting</em>, namely using methods like combinations and permutations.
I’m sure you’ve encountered this previously in your statistics classes, and appreciate how useful they are when trying to calculate probabilities as being “the number of outcomes in the event space” divided by “the number of outcomes in the sample space”.
However, our focus for this course is to get to the inference section, and the topic of counting does not contribute much to that understanding, so I shall skip it.</p>
</div>
</div>
<div id="derived-probability-results" class="section level3 hasAnchor" number="1.2.3">
<h3><span class="header-section-number">1.2.3</span> Derived probability results<a href="axiomatic-probability.html#derived-probability-results" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let us now look at some useful probability results that can be derived from the probability axioms.</p>
<div class="theorem">
<p><span id="thm:derivedprobres1" class="theorem"><strong>Theorem 1.1  </strong></span>Let <span class="math inline">\((\Omega,{\mathcal F},\mathbb{P})\)</span> be a probability space. For any <span class="math inline">\(E\in{\mathcal F}\)</span>,</p>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(\mathbb{P}(\{\}) = 0\)</span>;</li>
<li><span class="math inline">\(\mathbb{P}(E) \leq 1\)</span>; and</li>
<li><span class="math inline">\(\mathbb{P}(E^c)=1-\mathbb{P}(E).\)</span></li>
</ol>
</div>
<p>An important thing that we learn here is that probabilities are always finite and bounded within <span class="math inline">\([0,1]\)</span>, i.e. for any event <span class="math inline">\(E\)</span>, <span class="math inline">\(0 \leq \mathbb{P}(E) \leq 1\)</span>.
So please, do not make the mistake of reporting <em>negative probabilities</em> or probabilities greater than one–they are mathematically impossible<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a>!</p>
<div class="mycheck">
<p>The proof of Theorem <a href="axiomatic-probability.html#thm:derivedprobres1">1.1</a> is left an exercise. Try this out for yourself!</p>
</div>
<p>Further results regarding two events in the sample space based on the Probability Axioms:</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-10" class="theorem"><strong>Theorem 1.2  </strong></span>Let <span class="math inline">\((\Omega,{\mathcal F},\mathbb{P})\)</span> be a probability space. For any <span class="math inline">\(A,B \in{\mathcal F}\)</span>,</p>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(\mathbb{P}(B \cap A^c) = \mathbb{P}(B) - \mathbb{P}(A\cap B)\)</span>;</li>
<li><span class="math inline">\(\mathbb{P}(A \cup B) = \mathbb{P}(A) + \mathbb{P}(B) - \mathbb{P}(A \cap B)\)</span>; and</li>
<li>If <span class="math inline">\(A \subseteq B\)</span>, then <span class="math inline">\(\mathbb{P}(A) \leq \mathbb{P}(B)\)</span>.</li>
</ol>
</div>
<p>While these results are not so self-evident from the Probability Axioms, it may be useful to employ Venn diagrams to visualise the above statements.</p>
<p><img src="bookdown-adv-stats_files/figure-html/venn1-1.png" width="32%" style="display: block; margin: auto;" />
<img src="bookdown-adv-stats_files/figure-html/venn2-1.png" width="32%" style="display: block; margin: auto;" />
<img src="bookdown-adv-stats_files/figure-html/venn3-1.png" width="32%" style="display: block; margin: auto;" /></p>
<div class="proof">
<p><span id="unlabeled-div-11" class="proof"><em>Proof</em>. </span><br></p>
<ol style="list-style-type: lower-roman">
<li><p>Note that <span class="math inline">\(B\)</span> is composed of the two disjoint sets <span class="math inline">\(B= \{B\cap A\} \cup \{ B \cap A^c\}\)</span>, so we have <span class="math display">\[\mathbb{P}(B) = \mathbb{P}(B\cap A) + \mathbb{P}(B \cap A^c),\]</span> and the desired results is obtained after rearranging.</p></li>
<li><p>Using the identity <span class="math display">\[A \cup B = (A \cup B) \cap \overbrace{(A \cup A^c)}^\Omega=A \cup \{B \cap A^c \},\]</span> we have that (since the two events are disjoint)
<span class="math display">\[\begin{align*}
\mathbb{P}(A \cup B) &amp;= \mathbb{P}(A) + \mathbb{P}(B \cap A^c) \\
&amp;= \mathbb{P}(A) + \mathbb{P}(B \cap A^c) \\
&amp;= \mathbb{P}(A) + \mathbb{P}(B) - \mathbb{P}(B\cap A).
\end{align*}\]</span></p></li>
<li><p>Since <span class="math inline">\(A \subseteq B\)</span>, <span class="math inline">\(A \cap B = A\)</span>, using i. we get <span class="math display">\[0 \leq \mathbb{P}(B \cap A^c) = \mathbb{P}(B) - \overbrace{\mathbb{P}(A\cap B)}^{\mathbb{P}(A)},\]</span> thus obtaining the desired result.</p></li>
</ol>
</div>
<div class="theorem">
<p><span id="thm:lawoftotalprob" class="theorem"><strong>Theorem 1.3  (Law of Total Probability) </strong></span>Let <span class="math inline">\((\Omega,{\mathcal F},\mathbb{P})\)</span> be a probability space. Let <span class="math inline">\(A\in{\mathcal F}\)</span> and consider a (countably infinite) partition of the sample space <span class="math inline">\(C_1,C_2,\dots\)</span> such that <span class="math inline">\(C_i\cap C_j = \{\}\)</span> for any <span class="math inline">\(i,j\)</span> and <span class="math inline">\(\bigcup_{i=1}^\infty C_i = \Omega\)</span>. Then
<span class="math display">\[
\mathbb{P}(A) = \sum_{i=1}^\infty \mathbb{P}(A \cap C_i).
\]</span></p>
</div>
<p>We may visualise the partitions of the sample space <span class="math inline">\(C_i\)</span> as well as the event <span class="math inline">\(A\)</span> of interest as follows:</p>
<p><img src="figure/lawoftotalprob.jpeg" width="80%" style="display: block; margin: auto;" /></p>
<p>Of course, we can only show a finite number of partitions for illustration, but this works for infinitely many countable partitions as well. We can see that the set <span class="math inline">\(A\)</span> is simply made up of the intersections of <span class="math inline">\(A\)</span> and the partitions.
Some of these intersections will be empty, but that’s OK.</p>
<div class="proof">
<p><span id="unlabeled-div-12" class="proof"><em>Proof</em>. </span>Write
<span class="math display">\[
A = A \cap \Omega = A \cap \left(\bigcup_{i=1}^\infty C_i\right) = \bigcup_{i=1}^\infty(A \cap C_i)
\]</span>
Evidently the events in the union on the right hand side of the equality are disjoint, since <span class="math inline">\(C_i\)</span> themselves are disjoint.
Therefore,
<span class="math display">\[\begin{align*}
\mathbb{P}(A) 
&amp;= \mathbb{P}\left(\bigcup_{i=1}^\infty(A \cap C_i) \right) \\
&amp;= \sum_{i=1}^\infty \mathbb{P}(A \cap C_i)
\end{align*}\]</span>
as required.</p>
</div>
</div>
<div id="why-measure-theory" class="section level3 hasAnchor" number="1.2.4">
<h3><span class="header-section-number">1.2.4</span> Why measure theory?<a href="axiomatic-probability.html#why-measure-theory" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><em>You may treat this section as optional, but it would deepen your understanding of probability theory.</em></p>
<p>Consider the uniform distribution on a random variable <span class="math inline">\(X\)</span> on the unit interval, denoted <span class="math inline">\(X\sim\mathop{\mathrm{Unif}}(0,1)\)</span>. You may have come across this before, and know that the probability that <span class="math inline">\(X\)</span> lies in any interval contained in <span class="math inline">\([0,1]\)</span> is simply the length of the interval, i.e.
<span class="math display" id="eq:meas1">\[\begin{equation}
\mathbb{P}\big([a,b]\big) = \mathbb{P}\big([a,b)\big) = \mathbb{P}\big((a,b]\big) = \mathbb{P}\big((a,b)\big) = b-a, \tag{1.3}
\end{equation}\]</span>
for <span class="math inline">\(0 \leq a \leq b \leq 1\)</span>. This definition works fine for the degenerate case <span class="math inline">\(\mathbb{P}(\{a\})=0\)</span> for the singleton set <span class="math inline">\(\{a|a\in(0,1)\}\)</span>.
In general, if <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are disjoint subsets of <span class="math inline">\([0,1]\)</span> then
<span class="math display">\[\begin{equation}
\mathbb{P}(A \cup B) = \mathbb{P}(A) + \mathbb{P}(B),
\end{equation}\]</span>
and we can even extend this notion to that of <em>countable additivity</em>
<span class="math display" id="eq:meas2">\[\begin{equation}
\mathbb{P}\left( \cup_{i=1}^\infty A_i \right) = \sum_{i=1}^\infty \mathbb{P}(A_i), \tag{1.4}
\end{equation}\]</span>
for disjoint sets <span class="math inline">\(\{A_1,A_2,\dots\}\)</span><a href="#fn12" class="footnote-ref" id="fnref12"><sup>12</sup></a>.</p>
<p>For a uniform measure on <span class="math inline">\([0,1]\)</span>, one expects that the measure of some subset <span class="math inline">\(A \subseteq [0,1]\)</span> to be unaffected by “shifting” (with wrap-around) of that subset by some fixed amount <span class="math inline">\(r\in[0,1]\)</span>.
Define the <em><span class="math inline">\(r\)</span>-shift</em> of <span class="math inline">\(A\subseteq [0,1]\)</span> by
<span class="math display">\[
A \oplus r := \left\{ a + r \mid a \in A, a+r \leq 1 \right\} \cup \left\{ a + r - 1 \mid a \in A, a+r &gt; 1 \right\}.
\]</span>
Then we should have
<span class="math display" id="eq:meas3">\[\begin{equation}
\mathbb{P}(A \oplus r) = \mathbb{P}(A). \tag{1.5}
\end{equation}\]</span></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:wraparoundmeasureanim"></span>
<img src="bookdown-adv-stats_files/figure-html/wraparoundmeasureanim-1.gif" alt="An interval in [0,1] shifted by some fixed amount, with wrap-around, should have consistent length." width="100%" />
<p class="caption">
Figure 1.2: An interval in [0,1] shifted by some fixed amount, with wrap-around, should have consistent length.
</p>
</div>
<p>At this point you might notice that all of this resonates with the previous example on the Lebesgue measure, except perhaps the shifting part, and indeed that is the case.
Suppose that we dispense with measure theory and do not define things like the <span class="math inline">\(\sigma\)</span>-algebra on the <span class="math inline">\([0,1]\)</span> or the triplet <span class="math inline">\((\Omega,{\mathcal F},\mathbb{P})\)</span>, and only use the above probability definitions given in <a href="axiomatic-probability.html#eq:meas1">(1.3)</a>, <a href="axiomatic-probability.html#eq:meas2">(1.4)</a>, and <a href="axiomatic-probability.html#eq:meas3">(1.5)</a>.
How far can we push the boundaries of such probability definitions before things start to breakdown?</p>
<p>Consider these questions:</p>
<ul>
<li>What is the probability that <span class="math inline">\(X\)</span> is rational?</li>
<li>What is the probability that <span class="math inline">\(X^n\)</span> is rational for some positive integer <span class="math inline">\(n\)</span>?</li>
<li>What is the probability that <span class="math inline">\(X\)</span> is <em>algebraic</em><a href="#fn13" class="footnote-ref" id="fnref13"><sup>13</sup></a>?</li>
</ul>
<p>All seemingly fair and interesting questions, but are they well defined? Can we actually measure them and assign probabilities to such events? Taking a step back further, we ask:</p>
<blockquote>
<p>Are all possible subsets <span class="math inline">\(A\subseteq [0,1]\)</span> measurable? Does <span class="math inline">\(\mathbb{P}(A)\)</span> even make <em>sense</em> for any event <span class="math inline">\(A\)</span> we can think of?</p>
</blockquote>
<p>It turns out the answer is no, and can be proven by contradiction with the help of equivalence relations.
This shows the need for the heavy machinery that is measure theory for assigning probabilities to events<a href="#fn14" class="footnote-ref" id="fnref14"><sup>14</sup></a>.</p>
<!-- Before we go there, there's one helpful property of measures that we should expect it to have (at least, when considering lengths of intervals such as in the uniform distribution above): *translation invariance*. -->
<!-- What we mean by this is that the measure of a set should remain unchanged even if it is subjected to fixed amount of "shift". -->
<!-- In particular, this is reflected in a more general form of the uniform distribution on an interval $(c,d)$, where -->
<!-- \begin{equation} -->
<!-- \bbP([a,b]) = \frac{b-a}{d-c} = \Pr\big(Y\in[a,b] \big\mid Y\sim\Unif(c,d)\big), (\#eq:uniformshift) -->
<!-- \end{equation} -->
<!-- for $c\leq a\leq b\leq d$.  -->
<!-- If our random variable $X\sim\Unif(0,1)$ is shifted by some amount $k\in\bbR$, then the values of $c$ and $d$ in \@ref(eq:uniformshift) are $c=r$ and $d=1+r$, whence  -->
<!-- $$ -->
<!-- \Pr\big(Y\in[a,b]  \big\mid Y\sim\Unif(r,1+r)\big) = b-a = \Pr\big(X\in[a,b]  \big\mid X\sim\Unif(0,1)\big). -->
<!-- $$ -->
<div class="proposition">
<p><span id="prp:unlabeled-div-13" class="proposition"><strong>Proposition 1.1  </strong></span>There does not exist a definition of <span class="math inline">\(\mathbb{P}(A)\)</span>, defined for all subsets <span class="math inline">\(A\subseteq[0,1]\)</span>, satisfying <a href="axiomatic-probability.html#eq:meas1">(1.3)</a>, <a href="axiomatic-probability.html#eq:meas2">(1.4)</a>, and <a href="axiomatic-probability.html#eq:meas3">(1.5)</a>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-14" class="proof"><em>Proof</em>. </span>All we need to show is the existence of one such subset of <span class="math inline">\([0,1]\)</span> whose measure is undefined. The set we are about to construct is called the Vitali set<a href="#fn15" class="footnote-ref" id="fnref15"><sup>15</sup></a>, after Giuseppe Vitali who described it in 1905.</p>
<p>Define an equivalence relation on <span class="math inline">\([0,1]\)</span> by the following:
<span class="math display">\[x\sim y \Rightarrow y-x \in \mathbb{Q}\]</span>
That is, two real numbers <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are deemed to be the same if their difference is a rational number. We would like to separate all the real numbers <span class="math inline">\(x\in[0,1]\)</span> by this equivalence relation, and collect them into groups called equivalence classes, denoted by <span class="math inline">\([x]\)</span>. Here, <span class="math inline">\([x]\)</span> is the set <span class="math inline">\(\{y \in [0,1] \mid x \sim y\}.\)</span> For instance,</p>
<ul>
<li><p>The equivalence class of <span class="math inline">\(0\)</span> is the set of real numbers <span class="math inline">\(x\)</span> such that <span class="math inline">\(x \sim 0\)</span>, i.e. <span class="math inline">\([0] = \{y \in [0,1] \mid y-0\in\mathbb{Q}\}\)</span>, which is the set of all rational numbers in <span class="math inline">\([0,1]\)</span>.</p></li>
<li><p>The equivalence class of an irrational number <span class="math inline">\(z_1\in[0,1]\)</span> is clearly not in <span class="math inline">\([0]\)</span>, thus would represent a different equivalent class <span class="math inline">\([z_1]=\{y \in [0,1] \mid y-z_1 \in \mathbb{Q}\}\)</span>.</p></li>
<li><p>Yet another irrational number <span class="math inline">\(z_2\not\in [z_1]\)</span> would exist, i.e. a number <span class="math inline">\(z_2\in[0,1]\)</span> such that <span class="math inline">\(z_2-z_1 \not\in\mathbb{Q}\)</span>, and thus would represent another equivalence class <span class="math inline">\([z_2]\)</span>.</p></li>
<li><p>And so on… The equivalence classes may be represented by <span class="math inline">\([0],[z_1],[z_2],\dots\)</span> where <span class="math inline">\(z_i\)</span> are all irrational numbers that differ by an irrational number, and there are uncountably many such numbers and therefore classes.</p></li>
</ul>
<p>Construct the Vitali set <span class="math inline">\(V\)</span> as follows: Take precisely one element from each equivalent class, and put it in <span class="math inline">\(V\)</span>. As a remark, such a <span class="math inline">\(V\)</span> must surely exist by the Axiom of Choice<a href="#fn16" class="footnote-ref" id="fnref16"><sup>16</sup></a>.</p>
<p>Consider now the union of shifted Vitali sets by some rational value <span class="math inline">\(r\in[0,1]\)</span>,
<span class="math display">\[
\bigcup_{r} (V \oplus r)
\]</span>
As a reminder, the set of rational numbers is countably infinite<a href="#fn17" class="footnote-ref" id="fnref17"><sup>17</sup></a>. We make a few observations:</p>
<ul>
<li><p>The equivalence relation partitions the interval <span class="math inline">\([0,1]\)</span> into a disjoint union of equivalence classes. In other words, the sets <span class="math inline">\((V \oplus r)\)</span> and <span class="math inline">\((V \oplus s)\)</span> are disjoint for any rationals <span class="math inline">\(r\neq s\)</span>, such that <span class="math inline">\(r,s\in[0,1]\)</span>. If they were not disjoint, this would mean that there exists some <span class="math inline">\(x,y\in[0,1]\)</span> with <span class="math inline">\(x+r\in(V \oplus r)\)</span> and <span class="math inline">\(y+s\in (V \oplus s)\)</span> such that <span class="math inline">\(x+r=y+s\)</span>. But then this means that <span class="math inline">\(x-y=s-r\in\mathbb{Q}\)</span> so <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are in the same equivalent class, and this is a contradiction.</p></li>
<li><p>Every point in <span class="math inline">\([0,1]\)</span> is contained in the union <span class="math inline">\(\bigcup_{r} (V \oplus r)\)</span>. To see this, fix a point <span class="math inline">\(x\)</span> in <span class="math inline">\([0,1]\)</span>. Note that this point belongs to some equivalent class of <span class="math inline">\(x\)</span>, and in this equivalence class there exists some point <span class="math inline">\(\alpha\)</span> which belongs to <span class="math inline">\(V\)</span> as well by construction. Hence, <span class="math inline">\(\alpha \sim x\)</span>, and thus <span class="math inline">\(x-\alpha=r\in\mathbb{Q}\)</span>, implying that <span class="math inline">\(x\)</span> is a point in the Vitali set <span class="math inline">\(V\)</span> shifted by <span class="math inline">\(r\)</span>. Therefore, <span class="math display">\[[0,1] \subseteq  \bigcup_{r} (V \oplus r).\]</span> and we may write <span class="math display">\[1 = \mathbb{P}([0,1]) \leq \mathbb{P}\left(\bigcup_{r} (V \oplus r)\right),\]</span> since the measure of any set contained in another must have smaller or equal measure. This relation is in fact implied by <a href="axiomatic-probability.html#eq:meas2">(1.4)</a>. Let <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> be such that <span class="math inline">\(A \subseteq B\)</span>. Then we may write <span class="math inline">\(B = A \cup (B-A)\)</span> where the sets <span class="math inline">\(A\)</span> and <span class="math inline">\(B-A\)</span> are disjoint. Hence, <span class="math inline">\(\mathbb{P}(B)=\mathbb{P}(A)+\mathbb{P}(B-A)\)</span>, and since measures are non-negative and in particular <span class="math inline">\(\mathbb{P}(B-A)\in[0,1]\)</span>, we have that <span class="math inline">\(\mathbb{P}(B)\geq \mathbb{P}(A)\)</span>. However since the probability measure cannot be greater than 1, it must be equal to 1.</p></li>
<li><p>The disjoint union <span class="math inline">\(\bigcup_{r} (V \oplus r)\)</span> has probability measure (according to our definitions in <a href="axiomatic-probability.html#eq:meas1">(1.3)</a>, <a href="axiomatic-probability.html#eq:meas2">(1.4)</a>, and <a href="axiomatic-probability.html#eq:meas3">(1.5)</a>)
<span class="math display">\[\begin{align*}
 \mathbb{P}\left(\bigcup_{r} (V \oplus r)\right) 
 &amp;= \sum_r \mathbb{P}(V \oplus r) \\
 &amp;= \sum_r \mathbb{P}(V) 
 \end{align*}\]</span></p></li>
</ul>
<p>Putting these three observations together gives us
<span class="math display">\[
1 = \mathbb{P}\left(\bigcup_{r} (V \oplus r)\right)  = \sum_r \mathbb{P}(V).
\]</span>
This leads to the desired contradiction: A countably infinite sum of the same quantity repeated can only equal 0, <span class="math inline">\(+\infty\)</span>, or <span class="math inline">\(-\infty\)</span>, but it can never equal 1.</p>
</div>
<p>In summary,</p>
<ul>
<li>Not all subsets of uncountable sets are measurable. Admitting all subsets of uncountable sets will break mathematics.</li>
<li><span class="math inline">\(\sigma\)</span>-algebras are the patch that fixes mathematics. It gatekeeps the subsets of uncountable sets and disregards those which are not measurable.</li>
<li>Actually, if you have been following along, you might realise that we are at risk of breaking mathematics when dealing with uncountable sets. Strictly speaking, we only need <span class="math inline">\(\sigma\)</span>-algebras when working in a set with uncountable cardinality.</li>
</ul>
<p>Finally, what on earth is an “unmeasurable” set? Wouldn’t it be (even arbitrarily) possible to just define a measure for whatever set we can think of? If the above example hasn’t convinced you enough, some other mathematicians have tried to resolve this but it seems it is not possible to do so without encountering paradoxes, such as the one below.</p>
<blockquote>
<p>The Banach–Tarski paradox states that a ball in the ordinary Euclidean space can be doubled using only the operations of partitioning into subsets, replacing a set with a congruent set, and reassembly.</p>
</blockquote>
<p><img src="figure/banach_tarski.png" /></p>
<p>To be clear, no rule of mathematics are broken in the Banach-Tarski paradox, but the result defies intuition. Another statement of this paradox is that <em>we can chop up a pea into finitely many pieces and reassemble it into the sun</em> (pea-sun paradox). If we don’t lay out the foundations for measuring probabilities rigorously, we can end up with nonsensical answers!</p>
<p>This section was highly inspired by the following references:</p>
<ul>
<li>Rosenthal, J. (2006). A first look at rigorous probability.</li>
<li>The discussion here: <a href="https://stats.stackexchange.com/q/199280" class="uri">https://stats.stackexchange.com/q/199280</a></li>
<li>This YouTube video on Vitali Sets: <a href="https://youtu.be/ameugr-wjeI" class="uri">https://youtu.be/ameugr-wjeI</a></li>
</ul>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="7">
<li id="fn7"><p>For the example at hand, the power set is <span class="math inline">\({\mathcal P}(\Omega)=\{ \{\}, \{1\}, \{2\}, \{3\}, \{1,2\}, \{1,3\}, \{2,3\}, \{1,2,3\} \}\)</span><a href="axiomatic-probability.html#fnref7" class="footnote-back">↩︎</a></p></li>
<li id="fn8"><p>You may notice that other <span class="math inline">\(\sigma\)</span>-algebras are indeed possible, e.g. the power set of <span class="math inline">\(\Omega\)</span> in this case. There is a notion of the <em>smallest</em> <span class="math inline">\(\sigma\)</span>-algebra containing the collection of “basic events”. Luckily for us, the event space that we will usually be working with will be the smallest <span class="math inline">\(\sigma\)</span>-algebra without much technicalities, so we shall not explore this concept any further.<a href="axiomatic-probability.html#fnref8" class="footnote-back">↩︎</a></p></li>
<li id="fn9"><p><a href="https://idioms.thefreedictionary.com/How+long+is+a+piece+of+string%3F" class="uri">https://idioms.thefreedictionary.com/How+long+is+a+piece+of+string%3F</a><a href="axiomatic-probability.html#fnref9" class="footnote-back">↩︎</a></p></li>
<li id="fn10"><p>See Theorem 1.2.6 in C&amp;B.<a href="axiomatic-probability.html#fnref10" class="footnote-back">↩︎</a></p></li>
<li id="fn11"><p>At least within the framework of the Kolmogorov Axioms. See: <a href="https://en.wikipedia.org/wiki/Negative_probability" class="uri">https://en.wikipedia.org/wiki/Negative_probability</a><a href="axiomatic-probability.html#fnref11" class="footnote-back">↩︎</a></p></li>
<li id="fn12"><p>A concrete example of this is for the sets <span class="math inline">\(A_1=(0,1/2)\)</span>, <span class="math inline">\(A_2=(1/2, 3/4)\)</span>, <span class="math inline">\(A_3=(3/4,7/8)\)</span>, and so on (adding half the interval at each iteration). One finds that the measure of the countable union is <span class="math inline">\(\sum_{i=1}^\infty (1/2)^i=1\)</span>.<a href="axiomatic-probability.html#fnref12" class="footnote-back">↩︎</a></p></li>
<li id="fn13"><p>An algebraic number is a number that is a root of a non-zero polynomial in one variable with integer coefficients.<a href="axiomatic-probability.html#fnref13" class="footnote-back">↩︎</a></p></li>
<li id="fn14"><p>Or at least, for cases where “not so nice” events need to be measured.<a href="axiomatic-probability.html#fnref14" class="footnote-back">↩︎</a></p></li>
<li id="fn15"><p><a href="https://en.wikipedia.org/wiki/Vitali_set" class="uri">https://en.wikipedia.org/wiki/Vitali_set</a><a href="axiomatic-probability.html#fnref15" class="footnote-back">↩︎</a></p></li>
<li id="fn16"><p>Given a collection of non-empty sets, it is always possible to construct a new set by taking one element from each set in the original collection. See <a href="https://brilliant.org/wiki/axiom-of-choice/" class="uri">https://brilliant.org/wiki/axiom-of-choice/</a><a href="axiomatic-probability.html#fnref16" class="footnote-back">↩︎</a></p></li>
<li id="fn17"><p><a href="https://www.homeschoolmath.net/teaching/rational-numbers-countable.php" class="uri">https://www.homeschoolmath.net/teaching/rational-numbers-countable.php</a><a href="axiomatic-probability.html#fnref17" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="elementary-set-theory.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="conditioning-and-independence.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/haziqj/adv-stats/edit/main/02-prob_theory.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-adv-stats.pdf", "bookdown-adv-stats.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
