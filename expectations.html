<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>1.8 Expectations | SM-4331 Advanced Statistics</title>
  <meta name="description" content="Course notes for SM-4331 Advanced Statistics (UBD)." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="1.8 Expectations | SM-4331 Advanced Statistics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Course notes for SM-4331 Advanced Statistics (UBD)." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1.8 Expectations | SM-4331 Advanced Statistics" />
  
  <meta name="twitter:description" content="Course notes for SM-4331 Advanced Statistics (UBD)." />
  

<meta name="author" content="Dr Haziq Jamil" />


<meta name="date" content="2021-11-29" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="multiple-random-variables.html"/>
<link rel="next" href="moment-generating-functions.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="mystyle.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">SM-4331 Advanced Statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About</a>
<ul>
<li class="chapter" data-level="" data-path="contents.html"><a href="contents.html"><i class="fa fa-check"></i>Contents</a>
<ul>
<li class="chapter" data-level="" data-path="contents.html"><a href="contents.html#goals"><i class="fa fa-check"></i>Goals</a></li>
<li class="chapter" data-level="" data-path="contents.html"><a href="contents.html#incidental-learning-outcomes"><i class="fa fa-check"></i>Incidental learning outcomes</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="module-information.html"><a href="module-information.html"><i class="fa fa-check"></i>Module information</a>
<ul>
<li class="chapter" data-level="" data-path="module-information.html"><a href="module-information.html#class-format"><i class="fa fa-check"></i>Class format</a></li>
<li class="chapter" data-level="" data-path="module-information.html"><a href="module-information.html#assessment"><i class="fa fa-check"></i>Assessment</a></li>
<li class="chapter" data-level="" data-path="module-information.html"><a href="module-information.html#key-data"><i class="fa fa-check"></i>Key data</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="course-policy.html"><a href="course-policy.html"><i class="fa fa-check"></i>Course policy</a>
<ul>
<li class="chapter" data-level="" data-path="course-policy.html"><a href="course-policy.html#communication-policy"><i class="fa fa-check"></i>Communication policy</a></li>
<li class="chapter" data-level="" data-path="course-policy.html"><a href="course-policy.html#attendance-policy"><i class="fa fa-check"></i>Attendance policy</a></li>
<li class="chapter" data-level="" data-path="course-policy.html"><a href="course-policy.html#conduct"><i class="fa fa-check"></i>Conduct</a></li>
<li class="chapter" data-level="" data-path="course-policy.html"><a href="course-policy.html#learning-management-system"><i class="fa fa-check"></i>Learning Management System</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="resources.html"><a href="resources.html"><i class="fa fa-check"></i>Resources</a>
<ul>
<li class="chapter" data-level="" data-path="resources.html"><a href="resources.html#this-course"><i class="fa fa-check"></i>This course</a></li>
<li class="chapter" data-level="" data-path="resources.html"><a href="resources.html#textbooks"><i class="fa fa-check"></i>Textbooks</a></li>
<li class="chapter" data-level="" data-path="resources.html"><a href="resources.html#miscellaneous"><i class="fa fa-check"></i>Miscellaneous</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>I Introduction</b></span></li>
<li class="chapter" data-level="" data-path="what-is-statistics.html"><a href="what-is-statistics.html"><i class="fa fa-check"></i>What is statistics?</a>
<ul>
<li class="chapter" data-level="" data-path="learning-statistics.html"><a href="learning-statistics.html"><i class="fa fa-check"></i>Learning statistics</a></li>
<li class="chapter" data-level="" data-path="population-sample-and-parametric-models.html"><a href="population-sample-and-parametric-models.html"><i class="fa fa-check"></i>Population, sample and parametric models</a>
<ul>
<li class="chapter" data-level="" data-path="population-sample-and-parametric-models.html"><a href="population-sample-and-parametric-models.html#population-vs-sample"><i class="fa fa-check"></i>Population vs sample</a></li>
<li class="chapter" data-level="" data-path="population-sample-and-parametric-models.html"><a href="population-sample-and-parametric-models.html#parametric-models"><i class="fa fa-check"></i>Parametric models</a></li>
<li class="chapter" data-level="" data-path="population-sample-and-parametric-models.html"><a href="population-sample-and-parametric-models.html#a-sample-a-set-of-data-or-random-variablesa-duality"><i class="fa fa-check"></i>A sample: a set of data or random variables?–A duality</a></li>
<li class="chapter" data-level="" data-path="population-sample-and-parametric-models.html"><a href="population-sample-and-parametric-models.html#variability-of-estimates"><i class="fa fa-check"></i>Variability of estimates</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="probability-and-statistics.html"><a href="probability-and-statistics.html"><i class="fa fa-check"></i>Probability and statistics</a></li>
</ul></li>
<li class="part"><span><b>II Prepare</b></span></li>
<li class="chapter" data-level="1" data-path="probability-theory-primer.html"><a href="probability-theory-primer.html"><i class="fa fa-check"></i><b>1</b> Probability theory primer</a>
<ul>
<li class="chapter" data-level="" data-path="probability-theory-primer.html"><a href="probability-theory-primer.html#learning-objectives"><i class="fa fa-check"></i>Learning objectives</a></li>
<li class="chapter" data-level="" data-path="probability-theory-primer.html"><a href="probability-theory-primer.html#readings"><i class="fa fa-check"></i>Readings</a></li>
<li class="chapter" data-level="1.1" data-path="elementary-set-theory.html"><a href="elementary-set-theory.html"><i class="fa fa-check"></i><b>1.1</b> Elementary set theory</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="elementary-set-theory.html"><a href="elementary-set-theory.html#set-operations"><i class="fa fa-check"></i><b>1.1.1</b> Set operations</a></li>
<li class="chapter" data-level="1.1.2" data-path="elementary-set-theory.html"><a href="elementary-set-theory.html#partitions"><i class="fa fa-check"></i><b>1.1.2</b> Partitions</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="axiomatic-probability.html"><a href="axiomatic-probability.html"><i class="fa fa-check"></i><b>1.2</b> Axiomatic probability</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="axiomatic-probability.html"><a href="axiomatic-probability.html#probability-as-a-measure"><i class="fa fa-check"></i><b>1.2.1</b> Probability as a measure</a></li>
<li class="chapter" data-level="1.2.2" data-path="axiomatic-probability.html"><a href="axiomatic-probability.html#axioms-of-probability"><i class="fa fa-check"></i><b>1.2.2</b> Axioms of probability</a></li>
<li class="chapter" data-level="1.2.3" data-path="axiomatic-probability.html"><a href="axiomatic-probability.html#derived-probability-results"><i class="fa fa-check"></i><b>1.2.3</b> Derived probability results</a></li>
<li class="chapter" data-level="1.2.4" data-path="axiomatic-probability.html"><a href="axiomatic-probability.html#why-measure-theory"><i class="fa fa-check"></i><b>1.2.4</b> Why measure theory?</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="conditioning-and-independence.html"><a href="conditioning-and-independence.html"><i class="fa fa-check"></i><b>1.3</b> Conditioning and independence</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="conditioning-and-independence.html"><a href="conditioning-and-independence.html#bayes-theorem"><i class="fa fa-check"></i><b>1.3.1</b> Bayes Theorem</a></li>
<li class="chapter" data-level="1.3.2" data-path="conditioning-and-independence.html"><a href="conditioning-and-independence.html#independence"><i class="fa fa-check"></i><b>1.3.2</b> Independence</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="random-variables.html"><a href="random-variables.html"><i class="fa fa-check"></i><b>1.4</b> Random variables</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="random-variables.html"><a href="random-variables.html#distribution-functions"><i class="fa fa-check"></i><b>1.4.1</b> Distribution functions</a></li>
<li class="chapter" data-level="1.4.2" data-path="random-variables.html"><a href="random-variables.html#identically-distributed-r.v."><i class="fa fa-check"></i><b>1.4.2</b> Identically distributed r.v.</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="probability-functions.html"><a href="probability-functions.html"><i class="fa fa-check"></i><b>1.5</b> Probability functions</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="probability-functions.html"><a href="probability-functions.html#probability-mass-function"><i class="fa fa-check"></i><b>1.5.1</b> Probability mass function</a></li>
<li class="chapter" data-level="1.5.2" data-path="probability-functions.html"><a href="probability-functions.html#probability-density-functions"><i class="fa fa-check"></i><b>1.5.2</b> Probability density functions</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="transformations.html"><a href="transformations.html"><i class="fa fa-check"></i><b>1.6</b> Transformations</a></li>
<li class="chapter" data-level="1.7" data-path="multiple-random-variables.html"><a href="multiple-random-variables.html"><i class="fa fa-check"></i><b>1.7</b> Multiple random variables</a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="multiple-random-variables.html"><a href="multiple-random-variables.html#bivariate-distributions"><i class="fa fa-check"></i><b>1.7.1</b> Bivariate distributions</a></li>
<li class="chapter" data-level="1.7.2" data-path="multiple-random-variables.html"><a href="multiple-random-variables.html#marginal-distributions"><i class="fa fa-check"></i><b>1.7.2</b> Marginal distributions</a></li>
<li class="chapter" data-level="1.7.3" data-path="multiple-random-variables.html"><a href="multiple-random-variables.html#conditional-distributions"><i class="fa fa-check"></i><b>1.7.3</b> Conditional distributions</a></li>
<li class="chapter" data-level="1.7.4" data-path="multiple-random-variables.html"><a href="multiple-random-variables.html#independent-random-variables"><i class="fa fa-check"></i><b>1.7.4</b> Independent random variables</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="expectations.html"><a href="expectations.html"><i class="fa fa-check"></i><b>1.8</b> Expectations</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="expectations.html"><a href="expectations.html#expectations-of-functions-of-r.v."><i class="fa fa-check"></i><b>1.8.1</b> Expectations of functions of r.v.</a></li>
<li class="chapter" data-level="1.8.2" data-path="expectations.html"><a href="expectations.html#properties-of-expectations"><i class="fa fa-check"></i><b>1.8.2</b> Properties of expectations</a></li>
<li class="chapter" data-level="1.8.3" data-path="expectations.html"><a href="expectations.html#variance"><i class="fa fa-check"></i><b>1.8.3</b> Variance</a></li>
<li class="chapter" data-level="1.8.4" data-path="expectations.html"><a href="expectations.html#covariance-and-correlation"><i class="fa fa-check"></i><b>1.8.4</b> Covariance and correlation</a></li>
<li class="chapter" data-level="1.8.5" data-path="expectations.html"><a href="expectations.html#properties-of-variances-and-covariances"><i class="fa fa-check"></i><b>1.8.5</b> Properties of variances and covariances</a></li>
<li class="chapter" data-level="1.8.6" data-path="expectations.html"><a href="expectations.html#variance-covariance-matrix"><i class="fa fa-check"></i><b>1.8.6</b> Variance-covariance matrix</a></li>
<li class="chapter" data-level="1.8.7" data-path="expectations.html"><a href="expectations.html#conditional-expectations"><i class="fa fa-check"></i><b>1.8.7</b> Conditional expectations</a></li>
<li class="chapter" data-level="1.8.8" data-path="expectations.html"><a href="expectations.html#conditional-variance"><i class="fa fa-check"></i><b>1.8.8</b> Conditional variance</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="moment-generating-functions.html"><a href="moment-generating-functions.html"><i class="fa fa-check"></i><b>1.9</b> Moment generating functions</a>
<ul>
<li class="chapter" data-level="1.9.1" data-path="moment-generating-functions.html"><a href="moment-generating-functions.html#moment-generating-functions-1"><i class="fa fa-check"></i><b>1.9.1</b> Moment generating functions</a></li>
<li class="chapter" data-level="1.9.2" data-path="moment-generating-functions.html"><a href="moment-generating-functions.html#generating-moments"><i class="fa fa-check"></i><b>1.9.2</b> Generating moments</a></li>
<li class="chapter" data-level="1.9.3" data-path="moment-generating-functions.html"><a href="moment-generating-functions.html#properties-of-mgf"><i class="fa fa-check"></i><b>1.9.3</b> Properties of mgf</a></li>
</ul></li>
<li class="chapter" data-level="1.10" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>1.10</b> Exercises</a>
<ul>
<li class="chapter" data-level="" data-path="exercises.html"><a href="exercises.html#hand-in-questions"><i class="fa fa-check"></i>Hand-in questions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="commonly-used-probability-models.html"><a href="commonly-used-probability-models.html"><i class="fa fa-check"></i><b>2</b> Commonly-used probability models</a>
<ul>
<li class="chapter" data-level="" data-path="commonly-used-probability-models.html"><a href="commonly-used-probability-models.html#learning-objectives-1"><i class="fa fa-check"></i>Learning objectives</a></li>
<li class="chapter" data-level="" data-path="commonly-used-probability-models.html"><a href="commonly-used-probability-models.html#readings-1"><i class="fa fa-check"></i>Readings</a></li>
<li class="chapter" data-level="2.1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="discrete-models.html"><a href="discrete-models.html"><i class="fa fa-check"></i><b>2.2</b> Discrete models</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="discrete-models.html"><a href="discrete-models.html#point-mass-distribution"><i class="fa fa-check"></i><b>2.2.1</b> Point mass distribution</a></li>
<li class="chapter" data-level="2.2.2" data-path="discrete-models.html"><a href="discrete-models.html#uniform-distribution"><i class="fa fa-check"></i><b>2.2.2</b> Uniform distribution</a></li>
<li class="chapter" data-level="2.2.3" data-path="discrete-models.html"><a href="discrete-models.html#bernoulli-distribution"><i class="fa fa-check"></i><b>2.2.3</b> Bernoulli distribution</a></li>
<li class="chapter" data-level="2.2.4" data-path="discrete-models.html"><a href="discrete-models.html#binomial-distribution"><i class="fa fa-check"></i><b>2.2.4</b> Binomial distribution</a></li>
<li class="chapter" data-level="2.2.5" data-path="discrete-models.html"><a href="discrete-models.html#geometric-distribution"><i class="fa fa-check"></i><b>2.2.5</b> Geometric distribution</a></li>
<li class="chapter" data-level="2.2.6" data-path="discrete-models.html"><a href="discrete-models.html#negative-binomial"><i class="fa fa-check"></i><b>2.2.6</b> Negative binomial</a></li>
<li class="chapter" data-level="2.2.7" data-path="discrete-models.html"><a href="discrete-models.html#poisson-distribution"><i class="fa fa-check"></i><b>2.2.7</b> Poisson distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="continuous-models.html"><a href="continuous-models.html"><i class="fa fa-check"></i><b>2.3</b> Continuous models</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="continuous-models.html"><a href="continuous-models.html#continuous-uniform-distribution"><i class="fa fa-check"></i><b>2.3.1</b> Continuous uniform distribution</a></li>
<li class="chapter" data-level="2.3.2" data-path="continuous-models.html"><a href="continuous-models.html#exponential-distribution"><i class="fa fa-check"></i><b>2.3.2</b> Exponential distribution</a></li>
<li class="chapter" data-level="2.3.3" data-path="continuous-models.html"><a href="continuous-models.html#gamma-distribution"><i class="fa fa-check"></i><b>2.3.3</b> Gamma distribution</a></li>
<li class="chapter" data-level="2.3.4" data-path="continuous-models.html"><a href="continuous-models.html#beta-distribution"><i class="fa fa-check"></i><b>2.3.4</b> Beta distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="normal-distribution.html"><a href="normal-distribution.html"><i class="fa fa-check"></i><b>2.4</b> Normal distribution</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="normal-distribution.html"><a href="normal-distribution.html#location-parameter"><i class="fa fa-check"></i><b>2.4.1</b> Location parameter</a></li>
<li class="chapter" data-level="2.4.2" data-path="normal-distribution.html"><a href="normal-distribution.html#scale-parameter"><i class="fa fa-check"></i><b>2.4.2</b> Scale parameter</a></li>
<li class="chapter" data-level="2.4.3" data-path="normal-distribution.html"><a href="normal-distribution.html#linear-transformations-of-normal-random-variables"><i class="fa fa-check"></i><b>2.4.3</b> Linear transformations of normal random variables</a></li>
<li class="chapter" data-level="2.4.4" data-path="normal-distribution.html"><a href="normal-distribution.html#the-normal-cdf"><i class="fa fa-check"></i><b>2.4.4</b> The normal cdf</a></li>
<li class="chapter" data-level="2.4.5" data-path="normal-distribution.html"><a href="normal-distribution.html#rule"><i class="fa fa-check"></i><b>2.4.5</b> 68–95–99.7 Rule</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="some-relationships.html"><a href="some-relationships.html"><i class="fa fa-check"></i><b>2.5</b> Some relationships</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="some-relationships.html"><a href="some-relationships.html#poisson-binomial-relationship"><i class="fa fa-check"></i><b>2.5.1</b> Poisson-Binomial relationship</a></li>
<li class="chapter" data-level="2.5.2" data-path="some-relationships.html"><a href="some-relationships.html#poisson-exponential"><i class="fa fa-check"></i><b>2.5.2</b> Poisson-Exponential</a></li>
<li class="chapter" data-level="2.5.3" data-path="some-relationships.html"><a href="some-relationships.html#poisson-gamma"><i class="fa fa-check"></i><b>2.5.3</b> Poisson-Gamma</a></li>
<li class="chapter" data-level="2.5.4" data-path="some-relationships.html"><a href="some-relationships.html#normal-approximations"><i class="fa fa-check"></i><b>2.5.4</b> Normal approximations</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="exercises-1.html"><a href="exercises-1.html"><i class="fa fa-check"></i><b>2.6</b> Exercises</a>
<ul>
<li class="chapter" data-level="" data-path="exercises-1.html"><a href="exercises-1.html#hand-in-questions-1"><i class="fa fa-check"></i>Hand-in questions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="inequalities-convergences-and-normal-random-samples.html"><a href="inequalities-convergences-and-normal-random-samples.html"><i class="fa fa-check"></i><b>3</b> Inequalities, convergences, and normal random samples</a>
<ul>
<li class="chapter" data-level="" data-path="inequalities-convergences-and-normal-random-samples.html"><a href="inequalities-convergences-and-normal-random-samples.html#learning-objectives-2"><i class="fa fa-check"></i>Learning objectives</a></li>
<li class="chapter" data-level="" data-path="inequalities-convergences-and-normal-random-samples.html"><a href="inequalities-convergences-and-normal-random-samples.html#readings-2"><i class="fa fa-check"></i>Readings</a></li>
<li class="chapter" data-level="3.1" data-path="introduction-1.html"><a href="introduction-1.html"><i class="fa fa-check"></i><b>3.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="introduction-1.html"><a href="introduction-1.html#random-sampling"><i class="fa fa-check"></i><b>3.1.1</b> Random sampling</a></li>
<li class="chapter" data-level="3.1.2" data-path="introduction-1.html"><a href="introduction-1.html#independent-and-identical-r.v."><i class="fa fa-check"></i><b>3.1.2</b> Independent and identical r.v.</a></li>
<li class="chapter" data-level="3.1.3" data-path="introduction-1.html"><a href="introduction-1.html#statistic"><i class="fa fa-check"></i><b>3.1.3</b> Statistic</a></li>
<li class="chapter" data-level="3.1.4" data-path="introduction-1.html"><a href="introduction-1.html#sampling-distribution"><i class="fa fa-check"></i><b>3.1.4</b> Sampling distribution</a></li>
<li class="chapter" data-level="3.1.5" data-path="introduction-1.html"><a href="introduction-1.html#large-sample-approximation"><i class="fa fa-check"></i><b>3.1.5</b> Large-sample approximation</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="inequalities.html"><a href="inequalities.html"><i class="fa fa-check"></i><b>3.2</b> Inequalities</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="inequalities.html"><a href="inequalities.html#markovs-inequality"><i class="fa fa-check"></i><b>3.2.1</b> Markov’s inequality</a></li>
<li class="chapter" data-level="3.2.2" data-path="inequalities.html"><a href="inequalities.html#chebyshevs-inequality"><i class="fa fa-check"></i><b>3.2.2</b> Chebyshev’s inequality</a></li>
<li class="chapter" data-level="3.2.3" data-path="inequalities.html"><a href="inequalities.html#cauchy-schwartz-inequality"><i class="fa fa-check"></i><b>3.2.3</b> Cauchy-Schwartz inequality</a></li>
<li class="chapter" data-level="3.2.4" data-path="inequalities.html"><a href="inequalities.html#jensens-inequality"><i class="fa fa-check"></i><b>3.2.4</b> Jensen’s inequality</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="convergence-of-random-variables.html"><a href="convergence-of-random-variables.html"><i class="fa fa-check"></i><b>3.3</b> Convergence of random variables</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="convergence-of-random-variables.html"><a href="convergence-of-random-variables.html#convergence-in-probability"><i class="fa fa-check"></i><b>3.3.1</b> Convergence in probability</a></li>
<li class="chapter" data-level="3.3.2" data-path="convergence-of-random-variables.html"><a href="convergence-of-random-variables.html#convergence-in-distribution"><i class="fa fa-check"></i><b>3.3.2</b> Convergence in distribution</a></li>
<li class="chapter" data-level="3.3.3" data-path="convergence-of-random-variables.html"><a href="convergence-of-random-variables.html#mean-square-convergence"><i class="fa fa-check"></i><b>3.3.3</b> Mean-square convergence</a></li>
<li class="chapter" data-level="3.3.4" data-path="convergence-of-random-variables.html"><a href="convergence-of-random-variables.html#relationship-between-convergences"><i class="fa fa-check"></i><b>3.3.4</b> Relationship between convergences</a></li>
<li class="chapter" data-level="3.3.5" data-path="convergence-of-random-variables.html"><a href="convergence-of-random-variables.html#slutzkys-theorem"><i class="fa fa-check"></i><b>3.3.5</b> Slutzky’s Theorem</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="limit-theorems.html"><a href="limit-theorems.html"><i class="fa fa-check"></i><b>3.4</b> Limit theorems</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="limit-theorems.html"><a href="limit-theorems.html#the-weak-law-of-large-numbers"><i class="fa fa-check"></i><b>3.4.1</b> The (weak) Law of Large Numbers</a></li>
<li class="chapter" data-level="3.4.2" data-path="limit-theorems.html"><a href="limit-theorems.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>3.4.2</b> The Central Limit Theorem</a></li>
<li class="chapter" data-level="3.4.3" data-path="limit-theorems.html"><a href="limit-theorems.html#gauging-the-error-of-sample-mean-estimator"><i class="fa fa-check"></i><b>3.4.3</b> Gauging the error of sample mean estimator</a></li>
<li class="chapter" data-level="3.4.4" data-path="limit-theorems.html"><a href="limit-theorems.html#clt-with-sigma2-unknown"><i class="fa fa-check"></i><b>3.4.4</b> CLT with <span class="math inline">\(\sigma^2\)</span> unknown</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="delta-method.html"><a href="delta-method.html"><i class="fa fa-check"></i><b>3.5</b> Delta method</a></li>
<li class="chapter" data-level="3.6" data-path="normal-random-samples.html"><a href="normal-random-samples.html"><i class="fa fa-check"></i><b>3.6</b> Normal random samples</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="normal-random-samples.html"><a href="normal-random-samples.html#chi2-distribution"><i class="fa fa-check"></i><b>3.6.1</b> <span class="math inline">\(\chi^2\)</span>-distribution</a></li>
<li class="chapter" data-level="3.6.2" data-path="normal-random-samples.html"><a href="normal-random-samples.html#students-t-distribution"><i class="fa fa-check"></i><b>3.6.2</b> Student’s <span class="math inline">\(t\)</span>-distribution</a></li>
<li class="chapter" data-level="3.6.3" data-path="normal-random-samples.html"><a href="normal-random-samples.html#proof-of-theorem-refthmpropertynormalsamp"><i class="fa fa-check"></i><b>3.6.3</b> Proof of Theorem @ref(thm:propertynormalsamp)</a></li>
<li class="chapter" data-level="3.6.4" data-path="normal-random-samples.html"><a href="normal-random-samples.html#f-distribution"><i class="fa fa-check"></i><b>3.6.4</b> <span class="math inline">\(F\)</span>-distribution</a></li>
<li class="chapter" data-level="3.6.5" data-path="normal-random-samples.html"><a href="normal-random-samples.html#the-analysis-of-variance"><i class="fa fa-check"></i><b>3.6.5</b> The analysis of variance</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="exercises-2.html"><a href="exercises-2.html"><i class="fa fa-check"></i><b>3.7</b> Exercises</a>
<ul>
<li class="chapter" data-level="" data-path="exercises-2.html"><a href="exercises-2.html#hand-in-questions-2"><i class="fa fa-check"></i>Hand-in questions</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Inference</b></span></li>
<li class="chapter" data-level="4" data-path="point-estimation.html"><a href="point-estimation.html"><i class="fa fa-check"></i><b>4</b> Point estimation</a>
<ul>
<li class="chapter" data-level="" data-path="point-estimation.html"><a href="point-estimation.html#learning-objectives-3"><i class="fa fa-check"></i>Learning objectives</a></li>
<li class="chapter" data-level="" data-path="point-estimation.html"><a href="point-estimation.html#readings-3"><i class="fa fa-check"></i>Readings</a></li>
<li class="chapter" data-level="4.1" data-path="the-likelihood.html"><a href="the-likelihood.html"><i class="fa fa-check"></i><b>4.1</b> The likelihood</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="the-likelihood.html"><a href="the-likelihood.html#calculating-the-likelihood"><i class="fa fa-check"></i><b>4.1.1</b> Calculating the likelihood</a></li>
<li class="chapter" data-level="4.1.2" data-path="the-likelihood.html"><a href="the-likelihood.html#likelihood-ratio"><i class="fa fa-check"></i><b>4.1.2</b> Likelihood ratio</a></li>
<li class="chapter" data-level="4.1.3" data-path="the-likelihood.html"><a href="the-likelihood.html#log-likelihood"><i class="fa fa-check"></i><b>4.1.3</b> Log likelihood</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="sufficiency.html"><a href="sufficiency.html"><i class="fa fa-check"></i><b>4.2</b> Sufficiency</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="sufficiency.html"><a href="sufficiency.html#the-factorisation-theorem"><i class="fa fa-check"></i><b>4.2.1</b> The factorisation theorem</a></li>
<li class="chapter" data-level="4.2.2" data-path="sufficiency.html"><a href="sufficiency.html#minimal-sufficient-statistic"><i class="fa fa-check"></i><b>4.2.2</b> Minimal sufficient statistic</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="point-estimators.html"><a href="point-estimators.html"><i class="fa fa-check"></i><b>4.3</b> Point estimators</a></li>
<li class="chapter" data-level="4.4" data-path="method-of-moments.html"><a href="method-of-moments.html"><i class="fa fa-check"></i><b>4.4</b> Method of moments</a></li>
<li class="chapter" data-level="4.5" data-path="method-of-maximum-likelihood.html"><a href="method-of-maximum-likelihood.html"><i class="fa fa-check"></i><b>4.5</b> Method of maximum likelihood</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="method-of-maximum-likelihood.html"><a href="method-of-maximum-likelihood.html#finding-the-mle"><i class="fa fa-check"></i><b>4.5.1</b> Finding the MLE</a></li>
<li class="chapter" data-level="4.5.2" data-path="method-of-maximum-likelihood.html"><a href="method-of-maximum-likelihood.html#invariance-of-mle"><i class="fa fa-check"></i><b>4.5.2</b> Invariance of MLE</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="evaluating-estimators.html"><a href="evaluating-estimators.html"><i class="fa fa-check"></i><b>4.6</b> Evaluating estimators</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="evaluating-estimators.html"><a href="evaluating-estimators.html#bias"><i class="fa fa-check"></i><b>4.6.1</b> Bias</a></li>
<li class="chapter" data-level="4.6.2" data-path="evaluating-estimators.html"><a href="evaluating-estimators.html#variance-and-standard-error"><i class="fa fa-check"></i><b>4.6.2</b> Variance and standard error</a></li>
<li class="chapter" data-level="4.6.3" data-path="evaluating-estimators.html"><a href="evaluating-estimators.html#mean-squared-error"><i class="fa fa-check"></i><b>4.6.3</b> Mean squared error</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="cramér-rao-lower-bound-crlb.html"><a href="cramér-rao-lower-bound-crlb.html"><i class="fa fa-check"></i><b>4.7</b> Cramér-Rao lower bound (CRLB)</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="cramér-rao-lower-bound-crlb.html"><a href="cramér-rao-lower-bound-crlb.html#fisher-information"><i class="fa fa-check"></i><b>4.7.1</b> Fisher information</a></li>
<li class="chapter" data-level="4.7.2" data-path="cramér-rao-lower-bound-crlb.html"><a href="cramér-rao-lower-bound-crlb.html#variance-reduction-rao-blackwellisation"><i class="fa fa-check"></i><b>4.7.2</b> Variance reduction: Rao-Blackwellisation</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="large-sample-properties-of-estimators.html"><a href="large-sample-properties-of-estimators.html"><i class="fa fa-check"></i><b>4.8</b> Large sample properties of estimators</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="large-sample-properties-of-estimators.html"><a href="large-sample-properties-of-estimators.html#consistency"><i class="fa fa-check"></i><b>4.8.1</b> Consistency</a></li>
<li class="chapter" data-level="4.8.2" data-path="large-sample-properties-of-estimators.html"><a href="large-sample-properties-of-estimators.html#consistency-vs-unbiasedness"><i class="fa fa-check"></i><b>4.8.2</b> Consistency vs unbiasedness</a></li>
<li class="chapter" data-level="4.8.3" data-path="large-sample-properties-of-estimators.html"><a href="large-sample-properties-of-estimators.html#consistency-of-mles"><i class="fa fa-check"></i><b>4.8.3</b> Consistency of MLEs</a></li>
<li class="chapter" data-level="4.8.4" data-path="large-sample-properties-of-estimators.html"><a href="large-sample-properties-of-estimators.html#efficiency"><i class="fa fa-check"></i><b>4.8.4</b> Efficiency</a></li>
<li class="chapter" data-level="4.8.5" data-path="large-sample-properties-of-estimators.html"><a href="large-sample-properties-of-estimators.html#asymptotic-normality-and-consistency"><i class="fa fa-check"></i><b>4.8.5</b> Asymptotic normality and consistency</a></li>
<li class="chapter" data-level="4.8.6" data-path="large-sample-properties-of-estimators.html"><a href="large-sample-properties-of-estimators.html#efficiency-of-mle"><i class="fa fa-check"></i><b>4.8.6</b> Efficiency of MLE</a></li>
<li class="chapter" data-level="4.8.7" data-path="large-sample-properties-of-estimators.html"><a href="large-sample-properties-of-estimators.html#efficiency-of-transformations-of-mle"><i class="fa fa-check"></i><b>4.8.7</b> Efficiency of transformations of MLE</a></li>
<li class="chapter" data-level="4.8.8" data-path="large-sample-properties-of-estimators.html"><a href="large-sample-properties-of-estimators.html#application-of-asymptotic-normality"><i class="fa fa-check"></i><b>4.8.8</b> Application of asymptotic normality</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="exercises-3.html"><a href="exercises-3.html"><i class="fa fa-check"></i><b>4.9</b> Exercises</a>
<ul>
<li class="chapter" data-level="" data-path="exercises-3.html"><a href="exercises-3.html#hand-in-questions-3"><i class="fa fa-check"></i>Hand-in questions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html"><i class="fa fa-check"></i><b>5</b> Hypothesis testing</a>
<ul>
<li class="chapter" data-level="" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#learning-objectives-4"><i class="fa fa-check"></i>Learning objectives</a></li>
<li class="chapter" data-level="" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#readings-4"><i class="fa fa-check"></i>Readings</a></li>
<li class="chapter" data-level="5.1" data-path="introduction-2.html"><a href="introduction-2.html"><i class="fa fa-check"></i><b>5.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="introduction-2.html"><a href="introduction-2.html#a-general-paradigm"><i class="fa fa-check"></i><b>5.1.1</b> A general paradigm</a></li>
<li class="chapter" data-level="5.1.2" data-path="introduction-2.html"><a href="introduction-2.html#p-values"><i class="fa fa-check"></i><b>5.1.2</b> <span class="math inline">\(p\)</span>-values</a></li>
<li class="chapter" data-level="5.1.3" data-path="introduction-2.html"><a href="introduction-2.html#accept-h_0"><i class="fa fa-check"></i><b>5.1.3</b> Accept <span class="math inline">\(H_0\)</span>?</a></li>
<li class="chapter" data-level="5.1.4" data-path="introduction-2.html"><a href="introduction-2.html#uniformity-of-p-values"><i class="fa fa-check"></i><b>5.1.4</b> Uniformity of <span class="math inline">\(p\)</span>-values</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="likelihood-ratio-test.html"><a href="likelihood-ratio-test.html"><i class="fa fa-check"></i><b>5.2</b> Likelihood ratio test</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="likelihood-ratio-test.html"><a href="likelihood-ratio-test.html#log-likelihood-ratio-test-statistic"><i class="fa fa-check"></i><b>5.2.1</b> Log likelihood ratio test statistic</a></li>
<li class="chapter" data-level="5.2.2" data-path="likelihood-ratio-test.html"><a href="likelihood-ratio-test.html#example-normal-with-known-variance"><i class="fa fa-check"></i><b>5.2.2</b> Example: Normal with known variance</a></li>
<li class="chapter" data-level="5.2.3" data-path="likelihood-ratio-test.html"><a href="likelihood-ratio-test.html#example-normal-with-unknown-variance-t-test"><i class="fa fa-check"></i><b>5.2.3</b> Example: Normal with unknown variance (<span class="math inline">\(t\)</span>-test)</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="the-neyman-pearson-approach.html"><a href="the-neyman-pearson-approach.html"><i class="fa fa-check"></i><b>5.3</b> The Neyman-Pearson approach</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="the-neyman-pearson-approach.html"><a href="the-neyman-pearson-approach.html#performance-of-a-test"><i class="fa fa-check"></i><b>5.3.1</b> Performance of a test</a></li>
<li class="chapter" data-level="5.3.2" data-path="the-neyman-pearson-approach.html"><a href="the-neyman-pearson-approach.html#relation-to-p-values"><i class="fa fa-check"></i><b>5.3.2</b> Relation to <span class="math inline">\(p\)</span>-values</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="type-i-and-ii-errors.html"><a href="type-i-and-ii-errors.html"><i class="fa fa-check"></i><b>5.4</b> Type I and II errors</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="type-i-and-ii-errors.html"><a href="type-i-and-ii-errors.html#minimising-errors"><i class="fa fa-check"></i><b>5.4.1</b> Minimising errors</a></li>
<li class="chapter" data-level="5.4.2" data-path="type-i-and-ii-errors.html"><a href="type-i-and-ii-errors.html#optimality-of-the-lr-test"><i class="fa fa-check"></i><b>5.4.2</b> Optimality of the LR test</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="one-sided-tests.html"><a href="one-sided-tests.html"><i class="fa fa-check"></i><b>5.5</b> One-sided tests</a></li>
<li class="chapter" data-level="5.6" data-path="approximate-tests.html"><a href="approximate-tests.html"><i class="fa fa-check"></i><b>5.6</b> Approximate tests</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="approximate-tests.html"><a href="approximate-tests.html#asymptotic-distribution-of-lrts"><i class="fa fa-check"></i><b>5.6.1</b> Asymptotic distribution of LRTs</a></li>
<li class="chapter" data-level="5.6.2" data-path="approximate-tests.html"><a href="approximate-tests.html#wilks-theorem"><i class="fa fa-check"></i><b>5.6.2</b> Wilk’s theorem</a></li>
<li class="chapter" data-level="5.6.3" data-path="approximate-tests.html"><a href="approximate-tests.html#the-wald-test"><i class="fa fa-check"></i><b>5.6.3</b> The Wald test</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="exercises-4.html"><a href="exercises-4.html"><i class="fa fa-check"></i><b>5.7</b> Exercises</a>
<ul>
<li class="chapter" data-level="" data-path="exercises-4.html"><a href="exercises-4.html#hand-in-questions-4"><i class="fa fa-check"></i>Hand-in questions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="interval-estimation.html"><a href="interval-estimation.html"><i class="fa fa-check"></i><b>6</b> Interval estimation</a>
<ul>
<li class="chapter" data-level="" data-path="interval-estimation.html"><a href="interval-estimation.html#learning-objectives-5"><i class="fa fa-check"></i>Learning objectives</a></li>
<li class="chapter" data-level="" data-path="interval-estimation.html"><a href="interval-estimation.html#readings-5"><i class="fa fa-check"></i>Readings</a></li>
<li class="chapter" data-level="6.1" data-path="introduction-3.html"><a href="introduction-3.html"><i class="fa fa-check"></i><b>6.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="introduction-3.html"><a href="introduction-3.html#coverage-probability"><i class="fa fa-check"></i><b>6.1.1</b> Coverage probability</a></li>
<li class="chapter" data-level="6.1.2" data-path="introduction-3.html"><a href="introduction-3.html#confidence-regions"><i class="fa fa-check"></i><b>6.1.2</b> Confidence regions</a></li>
<li class="chapter" data-level="6.1.3" data-path="introduction-3.html"><a href="introduction-3.html#methods-for-obtaining-confidence-regions"><i class="fa fa-check"></i><b>6.1.3</b> Methods for obtaining confidence regions</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="pivots.html"><a href="pivots.html"><i class="fa fa-check"></i><b>6.2</b> Pivots</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="pivots.html"><a href="pivots.html#from-pivot-to-confidence-interval"><i class="fa fa-check"></i><b>6.2.1</b> From pivot to confidence interval</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="inverting-a-test-statistic.html"><a href="inverting-a-test-statistic.html"><i class="fa fa-check"></i><b>6.3</b> Inverting a test statistic</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="inverting-a-test-statistic.html"><a href="inverting-a-test-statistic.html#discrete-distributions"><i class="fa fa-check"></i><b>6.3.1</b> Discrete distributions</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="desirable-confidence-sets.html"><a href="desirable-confidence-sets.html"><i class="fa fa-check"></i><b>6.4</b> Desirable confidence sets</a></li>
<li class="chapter" data-level="6.5" data-path="intervals-based-on-ml-methods.html"><a href="intervals-based-on-ml-methods.html"><i class="fa fa-check"></i><b>6.5</b> Intervals based on ML methods</a></li>
<li class="chapter" data-level="6.6" data-path="the-bootstrap-method.html"><a href="the-bootstrap-method.html"><i class="fa fa-check"></i><b>6.6</b> The bootstrap method</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="the-bootstrap-method.html"><a href="the-bootstrap-method.html#empirical-distribution"><i class="fa fa-check"></i><b>6.6.1</b> Empirical distribution</a></li>
<li class="chapter" data-level="6.6.2" data-path="the-bootstrap-method.html"><a href="the-bootstrap-method.html#bootstrap-variance-estimation"><i class="fa fa-check"></i><b>6.6.2</b> Bootstrap variance estimation</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="bootstrap-confidence-intervals.html"><a href="bootstrap-confidence-intervals.html"><i class="fa fa-check"></i><b>6.7</b> Bootstrap confidence intervals</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="bootstrap-confidence-intervals.html"><a href="bootstrap-confidence-intervals.html#normal-bootstrap-interval"><i class="fa fa-check"></i><b>6.7.1</b> Normal bootstrap interval</a></li>
<li class="chapter" data-level="6.7.2" data-path="bootstrap-confidence-intervals.html"><a href="bootstrap-confidence-intervals.html#bootstrap-percentile-interval"><i class="fa fa-check"></i><b>6.7.2</b> Bootstrap percentile interval</a></li>
<li class="chapter" data-level="6.7.3" data-path="bootstrap-confidence-intervals.html"><a href="bootstrap-confidence-intervals.html#bootstrap-pivotal-interval"><i class="fa fa-check"></i><b>6.7.3</b> Bootstrap pivotal interval</a></li>
<li class="chapter" data-level="6.7.4" data-path="bootstrap-confidence-intervals.html"><a href="bootstrap-confidence-intervals.html#which-one-to-use"><i class="fa fa-check"></i><b>6.7.4</b> Which one to use?</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="exercises-5.html"><a href="exercises-5.html"><i class="fa fa-check"></i><b>6.8</b> Exercises</a>
<ul>
<li class="chapter" data-level="" data-path="exercises-5.html"><a href="exercises-5.html#hand-in-questions-5"><i class="fa fa-check"></i>Hand-in questions</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="exam-tips.html"><a href="exam-tips.html"><i class="fa fa-check"></i><b>A</b> Exam tips</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">SM-4331 Advanced Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="expectations" class="section level2" number="1.8">
<h2><span class="header-section-number">1.8</span> Expectations</h2>
<p>The expected value, or expectation, of a random variable <span class="math inline">\(X\)</span> is its average value <em>weighted</em> according to the probability distribution.
Simply put, it signifies the <em>arithmetic mean</em> of a large number of independent realisations of <span class="math inline">\(X\)</span>.</p>
<div class="definition">
<p><span id="def:unlabeled-div-43" class="definition"><strong>Definition 1.17  (Expectation) </strong></span>The <em>expected value</em> or <em>mean</em> of a random variable <span class="math inline">\(X\)</span>, denoted <span class="math inline">\(\mathop{\mathrm{E}}(X)\)</span>, is defined to be
<span class="math display">\[
\mathop{\mathrm{E}}(X) = \begin{cases}
  \sum_x x f_X(x) = \sum_x x \mathbb{P}(X=x) &amp;\text{if $X$ is discrete} \\
  \int x f_X(x) \mathop{\mathrm{d}}\hspace{0.5pt}\!x &amp;\text{if $X$ is continuous} \\  
\end{cases}
\]</span>
provided that the integral or sum exists (is finite).</p>
</div>
<p>The symbol ‘<span class="math inline">\(\mu\)</span>’ is often used to denote the expected value. It may be represented by <span class="math inline">\(\mathop{\mathrm{E}}X\)</span>, <span class="math inline">\(\mathop{\mathrm{E}}[X]\)</span> or even using <span class="math inline">\(\mathbb{E}\)</span> instead of <span class="math inline">\(\mathop{\mathrm{E}}\)</span>.</p>
<div class="myalert">
<p>The expectation of a random variable is <strong>not to be confused</strong> with the <em>sample mean</em> of a set of observations <span class="math inline">\(\{x_1,\dots,x_n \}\)</span>, i.e. <span class="math inline">\(\bar x_n = \frac{1}{n} \sum_{i=1}^n x_i\)</span>.
The expectation is a purely theoretical value based on probabilities and pdfs.
The sample mean incorporates <em>randomness</em> into the calculations, by virtue of the randomness of the observed set of sample values.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-44" class="example"><strong>Example 1.24  </strong></span>Let <span class="math inline">\(X \in \{0,1\}\)</span> take value 1 with probability <span class="math inline">\(p\)</span>, and 0 with probability <span class="math inline">\(1-p\)</span>.
<span class="math inline">\(X\)</span> is called a Bernoulli random variable, and we write <span class="math inline">\(X \sim \mathop{\mathrm{Bern}}(p)\)</span>.
Then,</p>
<p><span class="math display">\[
\mathop{\mathrm{E}}(X) = \sum_x x\mathbb{P}(X = x) = 1\cdot p + 0 \cdot (1-p) = p.
\]</span></p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-45" class="example"><strong>Example 1.25  </strong></span>Let <span class="math inline">\(X\)</span> be a continuous random variable with pdf <span class="math inline">\(f(x)=\frac{1}{b-a}\)</span>, where <span class="math inline">\(a,b\in\mathbb{R}\)</span> and <span class="math inline">\(a&lt;b\)</span>.
<span class="math inline">\(X\)</span> has what is called a uniform distribution on the interval <span class="math inline">\((a,b)\)</span>, and we write <span class="math inline">\(X\sim\mathop{\mathrm{Unif}}(a,b)\)</span>. The mean of <span class="math inline">\(X\)</span> is</p>
<p><span class="math display">\[
\mathop{\mathrm{E}}(X) = \int_a^b  \frac{x}{b-a} = \frac{a+b}{2},
\]</span></p>
<p>the midpoint of the interval <span class="math inline">\((a,b)\)</span>! This reveals some intuition regarding uniformity of the distribution.</p>
</div>
<p>Do all random variables have expectations?</p>
<div class="example">
<p><span id="exm:unlabeled-div-46" class="example"><strong>Example 1.26  </strong></span><img src="bookdown-adv-stats_files/figure-html/cauchy-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Let <span class="math inline">\(X\)</span> be a continuous random variable with pdf <span class="math inline">\(f(x)=\{ \pi(1+x^2) \}^{-1}\)</span> with support over <span class="math inline">\(\mathbb{R}\)</span>.
This is the Cauchy distribution<a href="#fn21" class="footnote-ref" id="fnref21"><sup>21</sup></a> with location and scale parameter 0 and 1 respectively.
Let’s calculate the mean.</p>
<p>Using the substitution <span class="math inline">\(u = x^2 + 1\)</span> and <span class="math inline">\(\mathop{\mathrm{d}}\hspace{0.5pt}\!u/2 = x \mathop{\mathrm{d}}\hspace{0.5pt}\!x\)</span>, we find that
<span class="math display">\[\begin{align*}
\mathop{\mathrm{E}}(X) 
&amp;= \int_{-\infty}^\infty \frac{x \mathop{\mathrm{d}}\hspace{0.5pt}\!x}{\pi(1+x^2)} \\
&amp;= \int_{-\infty}^0 \frac{x \mathop{\mathrm{d}}\hspace{0.5pt}\!x}{\pi(1+x^2)} + \int_{0}^\infty \frac{x \mathop{\mathrm{d}}\hspace{0.5pt}\!x}{\pi(1+x^2)} \\
&amp;= \frac{1}{2\pi} \int_{u=\infty}^{u=1} \frac{\mathop{\mathrm{d}}\hspace{0.5pt}\!u}{u} + \frac{1}{2\pi} \int_{u=1}^{u=\infty} \frac{\mathop{\mathrm{d}}\hspace{0.5pt}\!u}{u} \\
&amp;= \frac{1}{2\pi} \left[\log u \right]_{\infty}^{1} + \frac{1}{2\pi} \left[\log u \right]^{\infty}_{1} \\
&amp;= \frac{1}{2\pi} (\infty - \infty) = \ ???
\end{align*}\]</span></p>
<p>The mean of the Cauchy distribution is undefined.
This seems a bit weird, since we can see that the pdf is somewhat bell-shaped with its peak at 0, so wouldn’t we expect the mean to be zero?
Not quite.
The highest peak of the bell curve is known as the <em>mode</em> of the distribution, and that indeed is well defined and is zero.
The <em>median</em> is also well-defined, as this is the point at which half the distribution lies below, and half lies above it–the median is zero.
The median exists because the area under the pdf curve must necessarily be equal to 1, a finite value.</p>
<p>On the other hand, if we look at the plot of <span class="math inline">\(xf(x)\)</span> on the positive side of the real line, we see that the tail end does not drop fast enough for the area under the curve to be a finite number.</p>
<p><img src="bookdown-adv-stats_files/figure-html/cauchy2-1.png" width="100%" style="display: block; margin: auto;" /></p>
</div>
<div id="expectations-of-functions-of-r.v." class="section level3" number="1.8.1">
<h3><span class="header-section-number">1.8.1</span> Expectations of functions of r.v.</h3>
<p>Realise that if <span class="math inline">\(X\)</span> is a r.v., then any function of <span class="math inline">\(X\)</span>, <span class="math inline">\(g(X)\)</span>, is also a random variable<a href="#fn22" class="footnote-ref" id="fnref22"><sup>22</sup></a>.
Often time we will want to know the mean of <span class="math inline">\(g(X)\)</span>.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-47" class="theorem"><strong>Theorem 1.6  </strong></span>Let <span class="math inline">\(X\)</span> be a r.v. with pdf <span class="math inline">\(f_X(x)\)</span>, and let <span class="math inline">\(Y=g(X)\)</span>. Then

<span class="math display">\[
\mathop{\mathrm{E}}(Y) = \int g(x)f_X(x)\mathop{\mathrm{d}}\hspace{0.5pt}\!x.
\]</span></p>
</div>
<p>In particular, the <span class="math inline">\(k\)</span>th <strong>moment</strong> of <span class="math inline">\(X\)</span> for <span class="math inline">\(k\in\mathbb{Z}\)</span> is defined to be
<span class="math display">\[
\mathop{\mathrm{E}}(X^k) = \int x^kf_X(x)\mathop{\mathrm{d}}\hspace{0.5pt}\!x.
\]</span>
The <span class="math inline">\(k\)</span>th central moment is defined as <span class="math inline">\(\mathop{\mathrm{E}}((X-\mu)^k)\)</span>, where <span class="math inline">\(\mu:=\mathop{\mathrm{E}}(X)\)</span>.</p>
</div>
<div id="properties-of-expectations" class="section level3" number="1.8.2">
<h3><span class="header-section-number">1.8.2</span> Properties of expectations</h3>
<p>Let <span class="math inline">\(X\)</span> be a r.v., and <span class="math inline">\(a,b,c\in\mathbb{R}\)</span> be constants.
Here are some important properties of expectations [you should really know these!]. They also work for <span class="math inline">\(g(X)\)</span> too.</p>
<ul>
<li><span class="math inline">\(\mathop{\mathrm{E}}(aX +bX +c) = a\mathop{\mathrm{E}}(X) + b\mathop{\mathrm{E}}(X) + c\)</span> (linearity of expectations)</li>
<li>If <span class="math inline">\(Y\)</span> is a r.v. s.t. <span class="math inline">\(X\perp Y\)</span>, then <span class="math inline">\(\mathop{\mathrm{E}}(XY) = \mathop{\mathrm{E}}(X)\mathop{\mathrm{E}}(Y)\)</span></li>
<li>If <span class="math inline">\(X\geq 0\)</span> for all <span class="math inline">\(x\)</span>, then <span class="math inline">\(\mathop{\mathrm{E}}(X)\geq 0\)</span></li>
<li>If <span class="math inline">\(a \leq X \leq b\)</span> for all <span class="math inline">\(x\)</span>, then <span class="math inline">\(a \leq \mathop{\mathrm{E}}(X) \leq b\)</span></li>
<li><span class="math inline">\(\mathop{\mathrm{E}}(X) = \min_b \mathop{\mathrm{E}}((X-b)^2)\)</span> (see Example 2.2.6 C&amp;B)</li>
</ul>
<p>As a corollary, if <span class="math inline">\(X_1,\dots,X_n\)</span> are r.v. and <span class="math inline">\(a_1,\dots,a_n\)</span> are constants, then
<span class="math display">\[
\mathop{\mathrm{E}}\left(\sum_{i=1}^n a_iX_i \right) = \sum_{i=1}^n a_i\mathop{\mathrm{E}}(X_i).
\]</span>
Additionally, if <span class="math inline">\(X_1,\dots,X_n\)</span> are independent,
<span class="math display">\[
\mathop{\mathrm{E}}\left(\prod_{i=1}^n a_iX_i \right) = \prod_{i=1}^n \mathop{\mathrm{E}}(X_i).
\]</span></p>
</div>
<div id="variance" class="section level3" number="1.8.3">
<h3><span class="header-section-number">1.8.3</span> Variance</h3>
<p>Aside from the mean of a r.v., perhaps the most important moment is the second central moment, more commonly known as the variance.</p>
<div class="definition">
<p><span id="def:unlabeled-div-48" class="definition"><strong>Definition 1.18  (Variance) </strong></span>Let <span class="math inline">\(X\)</span> be a r.v. with mean <span class="math inline">\(\mu\)</span>. The variance of <span class="math inline">\(X\)</span> is defined
<span class="math display">\[
\mathop{\mathrm{Var}}(X) = \mathop{\mathrm{E}}\big[(X-\mu)^2\big],
\]</span>
assuming this expectation exists. The standard deviation is <span class="math inline">\(\text{sd}(X) = \sqrt{\mathop{\mathrm{Var}}(X)}\)</span>.</p>
</div>
<ul>
<li>The symbol <span class="math inline">\(\sigma^2\)</span> is often used to denote the variance, and <span class="math inline">\(\sigma\)</span> the standard deviation.</li>
<li>An alternative formula is <span class="math inline">\(\sigma^2 = \mathop{\mathrm{E}}(X^2) - \{\mathop{\mathrm{E}}(X)\}^2\)</span>.</li>
<li>This variance is <strong>not to be confused</strong> with the sample variance of a set of observations <span class="math inline">\(\{x_1,\dots,x_n\}\)</span>, i.e. <span class="math inline">\(s^2 = \frac{1}{n}\sum_{i=1}^n (x_i-\bar x_n)^2\)</span> (although, inspect the two formulae for similarities!).</li>
</ul>
<p>The variance measures the spread of a distribution. That is, how far apart or close together the “mass” of a distribution are.
To illustrate this, have a look at the following <span class="math inline">\(\mathop{\mathrm{N}}(0,\sigma^2)\)</span> pdfs for different values of <span class="math inline">\(\sigma^2\)</span>.</p>
<p><img src="bookdown-adv-stats_files/figure-html/normaldist-1.png" width="100%" style="display: block; margin: auto;" /></p>
</div>
<div id="covariance-and-correlation" class="section level3" number="1.8.4">
<h3><span class="header-section-number">1.8.4</span> Covariance and correlation</h3>
<p>The covariance and correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> measure how strong the linear relationship is between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
<div class="definition">
<p><span id="def:unlabeled-div-49" class="definition"><strong>Definition 1.19  (Covariance and correlation) </strong></span>For two r.v. <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> with finite means <span class="math inline">\(\mu_X\)</span> and <span class="math inline">\(\mu_Y\)</span> resp., and variances <span class="math inline">\(\sigma^2_X\)</span> and <span class="math inline">\(\sigma^2_Y\)</span> resp., the covariance between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is</p>
<p><span class="math display">\[
\mathop{\mathrm{Cov}}(X,Y) = \mathop{\mathrm{E}}\big[(X-\mu_X)(Y-\mu_Y) \big].
\]</span></p>
<p>Their correlation is the number defined by</p>
<p><span class="math display">\[
\rho_{XY} :=  \frac{\mathop{\mathrm{Cov}}(X,Y)}{\sigma_X\sigma_Y}
\]</span></p>
</div>
<ul>
<li>An alternative formula is <span class="math inline">\(\mathop{\mathrm{E}}(XY) -\mathop{\mathrm{E}}(X)\mathop{\mathrm{E}}(Y)\)</span>.</li>
<li>The covariance of <span class="math inline">\(X\)</span> with itself is <span class="math inline">\(\sigma^2\)</span>, while the correlation of <span class="math inline">\(X\)</span> with itself is 1. Try and work this out yourself!</li>
</ul>
<p>The magnitude of the covariance by itself does not reflect how strong the relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is, so this is where the correlation comes in.</p>
<ul>
<li><span class="math inline">\(\rho_{XY}\)</span> takes values between -1 and 1.</li>
<li><span class="math inline">\(\rho_{XY}=0\)</span> implies that there is no linear relationship at all between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</li>
<li>On the other hand, <span class="math inline">\(\rho_{XY}=1\)</span> (<span class="math inline">\(\rho_{XY}=-1\)</span>) implies a perfect positive (negative) linear relationship.</li>
<li>In fact, <span class="math inline">\(|\rho_{XY}=1|\)</span> iff <span class="math inline">\(\exists a\neq 0,b\in\mathbb{R}\)</span> s.t. <span class="math inline">\(\mathbb{P}(Y=aX+b)=1\)</span>. If <span class="math inline">\(a&gt;0\)</span> then <span class="math inline">\(\rho_{XY}=1\)</span>, and if <span class="math inline">\(a&lt;0\)</span> then <span class="math inline">\(\rho_{XY}=-1\)</span>.</li>
<li>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, then <span class="math inline">\(\mathop{\mathrm{Cov}}(X,Y) =\rho_{XY}=0\)</span>. Ttry and prove this!</li>
</ul>
<div class="remark">
<p><span id="unlabeled-div-50" class="remark"><em>Remark</em>. </span>If <span class="math inline">\(\mathop{\mathrm{Cov}}(X,Y)=\rho_{XY}=0\)</span>, then <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are  independent.</p>
</div>
<p>Let <span class="math inline">\(X,Y\,\overset{\text{iid}}{\sim}\,\mathop{\mathrm{N}}(0,1)\)</span>.
We can draw some random values in <code>R</code>, and produce a scatterplot to see the relationship between them.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="expectations.html#cb9-1" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="at">n =</span> <span class="dv">100</span>, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>)</span>
<span id="cb9-2"><a href="expectations.html#cb9-2" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="at">n =</span> <span class="dv">100</span>, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>)</span>
<span id="cb9-3"><a href="expectations.html#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="fu">qplot</span>(X, Y, <span class="at">geom =</span> <span class="st">&quot;point&quot;</span>)</span></code></pre></div>
<p><img src="bookdown-adv-stats_files/figure-html/corr1-1.png" width="100%" height=".55\textheight" style="display: block; margin: auto;" /></p>
<p>Now suppose <span class="math inline">\(Y=2X + Z\)</span>, where <span class="math inline">\(Z\sim\mathop{\mathrm{N}}(0,1)\)</span>.
Now, <span class="math inline">\(\mathop{\mathrm{Cov}}(X,Y)= 2\)</span>, and <span class="math inline">\(\mathop{\mathrm{Var}}(Y)=2\)</span>.
Theoretically, <span class="math inline">\(\rho_{XY}=2/\sqrt{1\cdot 2}\approx 0.71\)</span>.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="expectations.html#cb10-1" aria-hidden="true" tabindex="-1"></a>Z <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="at">n =</span> <span class="dv">100</span>, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>)</span>
<span id="cb10-2"><a href="expectations.html#cb10-2" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">*</span> X <span class="sc">+</span> Z</span>
<span id="cb10-3"><a href="expectations.html#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="fu">qplot</span>(X, Y, <span class="at">geom =</span> <span class="st">&quot;point&quot;</span>)</span></code></pre></div>
<p><img src="bookdown-adv-stats_files/figure-html/corr2-1.png" width="100%" height=".55\textheight" style="display: block; margin: auto;" /></p>
</div>
<div id="properties-of-variances-and-covariances" class="section level3" number="1.8.5">
<h3><span class="header-section-number">1.8.5</span> Properties of variances and covariances</h3>
<p>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be random variables, and <span class="math inline">\(a\neq0,b\in\mathbb{R}\)</span> be constants.</p>
<ul>
<li><span class="math inline">\(\mathop{\mathrm{Var}}(aX + b) = a^2\mathop{\mathrm{Var}}(X)\)</span></li>
<li><span class="math inline">\(\mathop{\mathrm{Var}}(X \pm Y) = \mathop{\mathrm{Var}}(X) + \mathop{\mathrm{Var}}(Y) \pm 2\mathop{\mathrm{Cov}}(X,Y)\)</span></li>
<li>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, then <span class="math inline">\(\mathop{\mathrm{Var}}(X \pm Y) = \mathop{\mathrm{Var}}(X) + \mathop{\mathrm{Var}}(Y)\)</span></li>
</ul>
<p>As a corollary, let <span class="math inline">\(X_1,\dots,X_n\)</span> be r.v. Then,
<span class="math display">\[
\mathop{\mathrm{Var}}\left(\sum_{i=1}^nX_i \right) = \sum_{i=1}^n \mathop{\mathrm{Var}}(X_i) + \sum_{i\neq j}\mathop{\mathrm{Cov}}(X_i,X_j)
\]</span></p>
<p>Let <span class="math inline">\(X,Y,W,V\)</span> be r.v., and <span class="math inline">\(a,b,c,d\in\mathbb{R}\)</span>. Then</p>
<ul>
<li><span class="math inline">\(\mathop{\mathrm{Cov}}(X,Y) = \mathop{\mathrm{Cov}}(Y,X)\)</span></li>
<li><span class="math inline">\(\mathop{\mathrm{Cov}}(X,b) = 0\)</span></li>
<li><span class="math inline">\(\mathop{\mathrm{Cov}}(aX,Y) = a\mathop{\mathrm{Cov}}(X,Y)\)</span></li>
<li><span class="math inline">\(\mathop{\mathrm{Cov}}(aX+b,cY+d)=ac\mathop{\mathrm{Cov}}(X,Y)\)</span></li>
<li><span class="math inline">\(\mathop{\mathrm{Cov}}(X+Y,W+V)=\mathop{\mathrm{Cov}}(X,Y) + \mathop{\mathrm{Cov}}(X, V) + \mathop{\mathrm{Cov}}(Y,W) + \mathop{\mathrm{Cov}}(Y,V)\)</span></li>
</ul>
<div class="example">
<p><span id="exm:unlabeled-div-51" class="example"><strong>Example 1.27  </strong></span>Let <span class="math inline">\(X\sim\mathop{\mathrm{N}}(0,1)\)</span>, and <span class="math inline">\(Y=2X+1\)</span>. Then <span class="math display">\[\mathop{\mathrm{Var}}(Y)=\mathop{\mathrm{Var}}(2X+1)=4\mathop{\mathrm{Var}}(X) = 4\]</span>. Further, <span class="math display">\[\mathop{\mathrm{Cov}}(X,Y)=\mathop{\mathrm{Cov}}(X,2X+1)=2\mathop{\mathrm{Cov}}(X,X)=2\mathop{\mathrm{Var}}(X)=2\]</span>.</p>
</div>
</div>
<div id="variance-covariance-matrix" class="section level3" number="1.8.6">
<h3><span class="header-section-number">1.8.6</span> Variance-covariance matrix</h3>
<p>Consider a random vector <span class="math inline">\((X_1,\dots,X_n)^\top\)</span> whose mean is <span class="math inline">\((\mu_1,\dots,\mu_n)^\top\)</span>.
The variance-covariance matrix, usually denoted <span class="math inline">\({\boldsymbol\Sigma}\in\mathbb{R}^{n\times n}\)</span>, is defined to be</p>
<p><span class="math display">\[
{\boldsymbol\Sigma}= \begin{pmatrix}
\mathop{\mathrm{Var}}(X_1)   &amp;\mathop{\mathrm{Cov}}(X_1,X_2) &amp;\cdots &amp;\mathop{\mathrm{Cov}}(X_1,X_n) \\
\mathop{\mathrm{Cov}}(X_2,X_1)   &amp;\mathop{\mathrm{Var}}(X_2) &amp;\cdots &amp;\mathop{\mathrm{Cov}}(X_2,X_n) \\
\vdots &amp;\vdots &amp;\ddots&amp;\vdots \\
\mathop{\mathrm{Cov}}(X_n,X_1)   &amp;\mathop{\mathrm{Cov}}(X_n,X_2) &amp;\cdots &amp;\mathop{\mathrm{Var}}(X_n) \\
\end{pmatrix}
\]</span></p>
<p>The correlation matrix is similar in structure to the above, except the off-diagonals are filled with <span class="math inline">\(\rho_{X_iX_j}\)</span> and the diagonals are all 1. Can you figure out why this is?</p>
</div>
<div id="conditional-expectations" class="section level3" number="1.8.7">
<h3><span class="header-section-number">1.8.7</span> Conditional expectations</h3>
<p>Conditional pmfs/pdfs are also useful for calculating <em>conditional expectations</em>, i.e. the average value of a random variable <span class="math inline">\(X\)</span> given some information about another r.v. <span class="math inline">\(Y\)</span> which might affect it.</p>
<div class="definition">
<p><span id="def:unlabeled-div-52" class="definition"><strong>Definition 1.20  (Conditional expectation) </strong></span>The conditional expectation of a function of a r.v. <span class="math inline">\(X\)</span>, <span class="math inline">\(g(X)\)</span> say, given a value of a nother r.v. <span class="math inline">\(Y=y\)</span>, is
<span class="math display">\[
\mathop{\mathrm{E}}\left[g(X)|Y=y\right] =
\begin{cases}
\sum_x g(x)\overbrace{\mathbb{P}(X=x|Y=y)}^{f_{X|Y}(x|y)} &amp;\text{if $X$ is discrete}\\
\int g(x)f_{X|Y}(x|y)\mathop{\mathrm{d}}\hspace{0.5pt}\!x &amp;\text{if $X$ is continuous}\\
\end{cases}
\]</span></p>
</div>
<ul>
<li>All of the properties of the usual expectations are applicable.</li>
<li>However, whereas <span class="math inline">\(\mathop{\mathrm{E}}(X)\)</span> is a number (non-random), <span class="math inline">\(\mathop{\mathrm{E}}(X|Y=y)\)</span> is a function of <span class="math inline">\(y\)</span>. If we have not observed <span class="math inline">\(Y\)</span>, then <span class="math inline">\(\mathop{\mathrm{E}}(X|Y)\)</span> is a random variable.</li>
</ul>
<div class="example">
<p><span id="exm:unlabeled-div-53" class="example"><strong>Example 1.28  </strong></span>Suppose we draw <span class="math inline">\(Y\sim\mathop{\mathrm{Unif}}(0,1)\)</span>.
After we observe <span class="math inline">\(Y=y\in[0,1]\)</span>, we draw <span class="math inline">\(X|(Y=y) \sim \mathop{\mathrm{Unif}}(y,1)\)</span>.
Intuitively, we expect that <span class="math inline">\(\mathop{\mathrm{E}}(X|Y=y)\)</span> to be half-way between <span class="math inline">\(y\)</span> and 1, i.e. <span class="math inline">\((1+y)/2\)</span>.</p>
<p>In fact, <span class="math inline">\(f_{X|Y}(x|y) = (1-y)^{-1}\)</span>, so
<span class="math display">\[\begin{align*}
\mathop{\mathrm{E}}(X|Y=y) &amp;= \int_y^1 xf_{X|Y}(x|y) \mathop{\mathrm{d}}\hspace{0.5pt}\!x \\ &amp;= \frac{1}{1-y} \int_y^1  x \mathop{\mathrm{d}}\hspace{0.5pt}\!x  
=\frac{1-y^2}{2(1-y)} = \frac{(1-y)(1+y)}{2(1-y)} =\frac{1+y}{2}.
\end{align*}\]</span></p>
<p>However, if <span class="math inline">\(Y\)</span> has not been observed yet, then <span class="math inline">\(\mathop{\mathrm{E}}(X|Y)=(1+Y)/2\)</span> is a r.v. whose value is <span class="math inline">\(\mathop{\mathrm{E}}(X|Y=y)=(1+y)/2\)</span> once observed.</p>
</div>
<p>If <span class="math inline">\(\mathop{\mathrm{E}}(X|Y)\)</span> is a r.v., what is its mean?</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-54" class="theorem"><strong>Theorem 1.7  (Rule of iterated expectations/Law of total expectations) </strong></span>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are two r.v., then
<span class="math display">\[
{\mathop{\mathrm{E}}}_Y\left[\mathop{\mathrm{E}}(X|Y)\right] = \mathop{\mathrm{E}}(X),
\]</span>
provided the expectation exists.
More generally, <span class="math inline">\(\mathop{\mathrm{E}}(g(X)) = \mathop{\mathrm{E}}\left[\mathop{\mathrm{E}}(g(X)|Y)\right]\)</span> for any function <span class="math inline">\(g\)</span>.</p>
</div>
<p>The total average <span class="math inline">\(\mathop{\mathrm{E}}(X)\)</span> is the average <span class="math inline">\(\mathop{\mathrm{E}}_Y(\cdot)\)</span> of the case-by-case averages <span class="math inline">\(\mathop{\mathrm{E}}(X|Y)\)</span> over <span class="math inline">\(Y\)</span>.</p>
<div class="proof">
<p><span id="unlabeled-div-55" class="proof"><em>Proof</em>. </span><span class="math display">\[\begin{align*}
{\mathop{\mathrm{E}}}_Y\left[\mathop{\mathrm{E}}(X|Y)\right] 
&amp;= \int \left( \int x f_{X|Y}(x|y)\mathop{\mathrm{d}}\hspace{0.5pt}\!x \right) f_Y(y)\mathop{\mathrm{d}}\hspace{0.5pt}\!y \\
&amp;= \int \int x \cdot \overbrace{f_{X|Y}(x|y) f_Y(y)}^{f_{X,Y}(x,y)}  \mathop{\mathrm{d}}\hspace{0.5pt}\!y \mathop{\mathrm{d}}\hspace{0.5pt}\!x  \\
&amp;= \int  x \cdot \overbrace{\int f_{X,Y}(x,y) \mathop{\mathrm{d}}\hspace{0.5pt}\!y}^{f_X(x)}   \mathop{\mathrm{d}}\hspace{0.5pt}\!x  \\
&amp;= \mathop{\mathrm{E}}(X)
\end{align*}\]</span></p>
</div>
</div>
<div id="conditional-variance" class="section level3" number="1.8.8">
<h3><span class="header-section-number">1.8.8</span> Conditional variance</h3>
<div class="definition">
<p><span id="def:unlabeled-div-56" class="definition"><strong>Definition 1.21  (Conditional variance) </strong></span>The conditional variance of a r.v. <span class="math inline">\(X\)</span> given <span class="math inline">\(Y=y\)</span> is
<span class="math display">\[
\mathop{\mathrm{Var}}(X|Y=y) = \mathop{\mathrm{E}}\left[ \left(X - \mathop{\mathrm{E}}(X|Y=y)\right)^2 \,\Big|\, Y=y\right].
\]</span></p>
</div>
<ul>
<li>An alternative formula:
<span class="math display">\[
\mathop{\mathrm{Var}}(X|Y=y) = \mathop{\mathrm{E}}\left(X^2 | Y=y\right) - \left\{ \mathop{\mathrm{E}}(X|Y=y) \right\}^2.
\]</span></li>
</ul>
<p>The law of total variance states that
<span class="math display">\[
\mathop{\mathrm{Var}}(X) = {\mathop{\mathrm{E}}}_Y\left[\mathop{\mathrm{Var}}(X|Y) \right] + {\mathop{\mathrm{Var}}}_Y\left[\mathop{\mathrm{E}}(X|Y) \right].
\]</span></p>
<p>Note that, in this context, both <span class="math inline">\(\mathop{\mathrm{Var}}(X|Y)\)</span> and <span class="math inline">\(\mathop{\mathrm{E}}(X|Y)\)</span> are random variables.
The variance of <span class="math inline">\(X\)</span> is the sum of two parts:</p>
<ol style="list-style-type: decimal">
<li>The average of the variance of <span class="math inline">\(X\)</span> over all possible values of the r.v. <span class="math inline">\(Y\)</span>. This is called the average <em>within-sample variance</em>.</li>
<li>The variance of the conditional expectation of <span class="math inline">\(X\)</span> given <span class="math inline">\(Y\)</span>. This is called the <em>between-sample variance</em> (of the conditional averages).</li>
</ol>
<p>See also: <a href="https://math.stackexchange.com/a/3377007" class="uri">https://math.stackexchange.com/a/3377007</a></p>
<p><img src="figure/lawoftotalvariance.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="21">
<li id="fn21"><p>Named after the French mathematician Augustin Cauchy, although in physics, it is often known by the Lorentz distribution after the Dutch Nobel Laureate Hendrik Lorentz.<a href="expectations.html#fnref21" class="footnote-back">↩︎</a></p></li>
<li id="fn22"><p>We can even describe the distribution for any transformation of <span class="math inline">\(X\)</span>, see C&amp;B Sec 2.1.<a href="expectations.html#fnref22" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="multiple-random-variables.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="moment-generating-functions.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/haziqj/adv-stats/edit/main/02-prob_theory.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-adv-stats.pdf", "bookdown-adv-stats.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
